{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4bad238-8c8a-484f-a496-f12baaadecc8",
   "metadata": {},
   "source": [
    "Which speakers or speaker groups share non-standard linguistic forms shown to be distinguish-\n",
    "ing of n-word referenced speakers, according to LLM surprisal?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c95912b-e6d4-4f67-afcd-161c0bdf8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint as pp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter, defaultdict\n",
    "import pathlib\n",
    "import copy\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0751acf-177e-4ecb-a3ea-e6f260369c56",
   "metadata": {},
   "source": [
    "# functions for determining phonetic shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6667f1e-fff2-4864-9d66-09d6d7b774f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[W][AA][Z]']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "lines = requests.get(\"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\").text.splitlines()\n",
    "\n",
    "# ignore eveything up the 'A  AH0'\n",
    "lines = lines[lines.index('A  AH0'):]\n",
    "\n",
    "# ignore stresses as deep phenomizer doesnt have these\n",
    "word2arpa = defaultdict(list)\n",
    "for line in lines:\n",
    "    splits = [split for split in line.split(\" \") if split != '']\n",
    "    word = splits[0].split('(')[0]\n",
    "    arpa = \"\".join(['[' + split.translate(str.maketrans('', '', '0123456789')) + ']' for split in splits[1:]])\n",
    "    if word not in word2arpa: # just get the main pronunciaion\n",
    "        word2arpa[word].append(arpa)\n",
    "\n",
    "word2arpa['WAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0c4aee-a7b8-409a-9e7d-67eb57983a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigone/lib/python3.11/site-packages/dp/model/model.py:306: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "/opt/miniconda3/envs/bigone/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[W][AA][Z]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dp\n",
    "from dp.phonemizer import Phonemizer\n",
    "phonemizer = Phonemizer.from_checkpoint('./en_us_cmudict_forward.pt')\n",
    "phonemizer('WAS', lang='en_us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6e5764-99a8-4f17-9820-ade366506090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[HH][UW]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def arpaguess(s:str):\n",
    "    if s.upper() not in word2arpa:\n",
    "        return [phonemizer(s, lang='en_us')]\n",
    "    else:\n",
    "        return word2arpa[s.upper()]\n",
    "\n",
    "arpaguess(\"who\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb1327d-834a-4f09-9d44-ff6f0b8953b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['b', 'c'], ['d']]]\n",
      "[[['oughta'], ['ought', 'to']]]\n",
      "[[['oughta'], ['ought', 'to']]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['  it', '- oughta', '?      -\\n', '+ ought', '+ to', '  happen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "def get_changes(from_:list[str], to_:list[str], return_indices=False)->bool:\n",
    "\n",
    "    # accumulator\n",
    "    changes = []\n",
    "\n",
    "    # state\n",
    "    in_change = False\n",
    "\n",
    "    for x in difflib.ndiff(from_, to_): #iterate over diff\n",
    "        # print(x, in_change)\n",
    "        \n",
    "        if x[0] == \"+\" or x[0] == \"-\":  # change detected\n",
    "            sign, entry = x.split(\" \")\n",
    "\n",
    "            # new change? then init addition to changes\n",
    "            if in_change == False:\n",
    "                changes.append([[], []])\n",
    "            \n",
    "            # regardless, record state\n",
    "            in_change = True\n",
    "\n",
    "            # regardless, capture changes\n",
    "            if sign == '-':\n",
    "                changes[-1][0].append(entry)\n",
    "            elif sign == '+':\n",
    "                changes[-1][1].append(entry)\n",
    "\n",
    "        elif x[0] == \"?\":\n",
    "            pass\n",
    "            \n",
    "        else:  # match detected\n",
    "                \n",
    "            # regardless, record state    \n",
    "            in_change = False\n",
    "\n",
    "    return changes\n",
    "            \n",
    "print(get_changes(['a','b','b','c','e'], ['a','b','d','e']))\n",
    "print(get_changes([\"it\", \"oughta\", \"happen\"], [\"it\", \"ought\", \"to\", \"happen\"]))\n",
    "print(get_changes(\"it oughta happen\".split(), \"it ought to happen\".split()))\n",
    "list(difflib.ndiff([\"it\", \"oughta\", \"happen\"], [\"it\", \"ought\", \"to\", \"happen\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf13d81-583f-48d9-8e82-913639a7bc09",
   "metadata": {},
   "source": [
    "# get variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdca6f5-a5b6-4b91-adfb-90f39977512c",
   "metadata": {},
   "source": [
    "## manually assigned phonological variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25028717-7393-486c-8d1d-de92057fcaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DH_D = {\n",
    "    \"de\":[\"the\"], \n",
    "    \"dey\":[\"they\"],\n",
    "    \"dey's\":[\"they's\"],\n",
    "    \"dey'll\":[\"they'll\"],\n",
    "    \"dey'd\":[\"they'd\"],\n",
    "    \"dis\":[\"this\"],\n",
    "    \"dis'll\":[\"this'll\"],\n",
    "    \"dis'd\":[\"this'd\"],\n",
    "    \"dat\":[\"that\"],\n",
    "    \"dat's\":[\"that\"],\n",
    "    \"dat'll\":[\"that'll\"],\n",
    "    \"dat'd\":[\"that'd\"],\n",
    "    \"De\":[\"The\"], \n",
    "    \"Dey\":[\"They\"],\n",
    "    \"Dey's\":[\"They's\"],\n",
    "    \"Dey'll\":[\"They'll\"],\n",
    "    \"Dey'd\":[\"They'd\"],\n",
    "    \"Dis\":[\"This\"],\n",
    "    \"Dis'll\":[\"This'll\"],\n",
    "    \"Dis'd\":[\"This'd\"],\n",
    "    \"Dat\":[\"That\"],\n",
    "    \"Dat's\":[\"That\"],\n",
    "    \"Dat'll\":[\"That'll\"],\n",
    "    \"Dat'd\":[\"That'd\"]\n",
    "}\n",
    "\n",
    "phonological_variations = {}\n",
    "phonological_variations[\"['DH']->['D']\"] = set(list(DH_D.keys()))\n",
    "manually_assigned = set(list(DH_D.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be0fb1-52ae-49f8-9401-42648206e0ce",
   "metadata": {},
   "source": [
    "## get phonological variations from american literature corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a53060-d55d-452d-a7ba-f747e35d8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build annotations set\n",
    "annotations = dict()\n",
    "for speakers in [[\"man\"], [\"woman\"], [\"gentleman\"], [\"lady\"],  [\"child\"], [\"Negro\", \"negro\"]]:\n",
    "\n",
    "    s = \",\".join(speakers)\n",
    "\n",
    "    with open(f\"RQ1_ood_words/RQ1_ood_words_{s}.json\", \"r\") as f:\n",
    "        annotations.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbacdee4-6681-4882-8549-baecb0670faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get phological variations implied by non_canonical:canonical pairs\n",
    "for non_canonical, corrections in annotations.items():\n",
    "    if len(corrections) == 1 and non_canonical not in manually_assigned:\n",
    "        canonical = corrections[0]\n",
    "\n",
    "        for from_, to_ in get_changes(re.findall(r\"\\w+\", arpaguess(canonical)[0]), re.findall(r\"\\w+\", arpaguess(non_canonical)[0])):\n",
    "\n",
    "            # capture instances\n",
    "            if f\"{from_}->{to_}\" not in phonological_variations:\n",
    "                phonological_variations[f\"{from_}->{to_}\"] = set()\n",
    "            phonological_variations[f\"{from_}->{to_}\"].add(non_canonical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6031606-2877-4efd-a268-412475575aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Callatin'\",\n",
       " \"Comin'\",\n",
       " \"Doin'\",\n",
       " \"Evenin'\",\n",
       " \"Gittin'\",\n",
       " \"Good-evenin'\",\n",
       " \"Good-mornin'\",\n",
       " \"He-said-nothin'\",\n",
       " \"Layin'\",\n",
       " \"Lookin'\",\n",
       " \"Lynchin'\",\n",
       " \"Marryin'\",\n",
       " \"Mawnin'\",\n",
       " \"Meanin'\",\n",
       " \"Mornin'\",\n",
       " \"Noffin'\",\n",
       " \"Nothin'\",\n",
       " \"Overpowerin'\",\n",
       " \"Partin'\",\n",
       " \"Peddlin'\",\n",
       " \"Playin'\",\n",
       " \"S'posin'\",\n",
       " \"Seein'\",\n",
       " \"Shirkin'\",\n",
       " \"Somefin'\",\n",
       " \"Sompin's\",\n",
       " \"Suthin'\",\n",
       " \"Waitin'\",\n",
       " \"Waitin's\",\n",
       " \"Wool-gatherin'\",\n",
       " \"a-bawlin'\",\n",
       " \"a-blamin'\",\n",
       " \"a-burnin'\",\n",
       " \"a-carryin'\",\n",
       " \"a-changin'\",\n",
       " \"a-comin'\",\n",
       " \"a-crawlin'\",\n",
       " \"a-cuttin'\",\n",
       " \"a-dealin'\",\n",
       " \"a-disappearin'\",\n",
       " \"a-dyin'\",\n",
       " \"a-feedin'\",\n",
       " \"a-fetchin'\",\n",
       " \"a-fishin'\",\n",
       " \"a-gittin'\",\n",
       " \"a-hearin'\",\n",
       " \"a-holdin'\",\n",
       " \"a-hurtin'\",\n",
       " \"a-kickin'\",\n",
       " \"a-lookin'\",\n",
       " \"a-prayin'\",\n",
       " \"a-ridin'\",\n",
       " \"a-runnin'\",\n",
       " \"a-rushin'\",\n",
       " \"a-savin'\",\n",
       " \"a-settin'\",\n",
       " \"a-sittin'\",\n",
       " \"a-sleepin'\",\n",
       " \"a-smokin'\",\n",
       " \"a-standin'\",\n",
       " \"a-tormentin'\",\n",
       " \"a-wantin'\",\n",
       " \"a-watchin'\",\n",
       " \"a-wearin'\",\n",
       " \"a-whippin'\",\n",
       " \"a-worryin'\",\n",
       " \"a-worshippin'\",\n",
       " \"a-writin'\",\n",
       " \"achin'\",\n",
       " \"agettin'\",\n",
       " \"allowin'\",\n",
       " \"alookin'\",\n",
       " \"amazin'\",\n",
       " \"anythin'\",\n",
       " \"askin'\",\n",
       " \"astonishin'\",\n",
       " \"awaitin'\",\n",
       " \"b'ilin'\",\n",
       " \"bad-lookin'\",\n",
       " \"bearin'\",\n",
       " \"beatin'\",\n",
       " \"beggin'\",\n",
       " \"beginnin'\",\n",
       " \"bein'\",\n",
       " \"belongin'\",\n",
       " \"betrayin'\",\n",
       " \"bettin'\",\n",
       " \"bitin'\",\n",
       " \"blacksmit'in'\",\n",
       " \"blamin'\",\n",
       " \"blatherin'\",\n",
       " \"blessin'\",\n",
       " \"blockin'\",\n",
       " \"bloomin'\",\n",
       " \"blowin'\",\n",
       " \"boardin'-houses\",\n",
       " \"book-learnin'\",\n",
       " \"boostin'\",\n",
       " \"bossin'\",\n",
       " \"breakin'\",\n",
       " \"breavin'\",\n",
       " \"buckin'-bronco\",\n",
       " \"bumpin'\",\n",
       " \"buryin'\",\n",
       " \"bustin'\",\n",
       " \"buyin'\",\n",
       " \"calculatin'\",\n",
       " \"callin'\",\n",
       " \"camp-meetin'\",\n",
       " \"carryin's\",\n",
       " \"cavin'\",\n",
       " \"celebratin'\",\n",
       " 'ceptin',\n",
       " \"changin'\",\n",
       " \"chasin'\",\n",
       " \"chewin'\",\n",
       " \"clearin'\",\n",
       " \"cloggin'\",\n",
       " \"collectin'\",\n",
       " \"comin'\",\n",
       " \"comin's\",\n",
       " \"complainin'\",\n",
       " \"considerin'\",\n",
       " \"contendin'\",\n",
       " \"cookin'\",\n",
       " \"courtin'\",\n",
       " \"crackin'\",\n",
       " \"crawlin'\",\n",
       " 'creepin',\n",
       " \"cringin'\",\n",
       " \"crowdin'\",\n",
       " \"cryin'\",\n",
       " \"cuis-lookin'\",\n",
       " 'cummin',\n",
       " \"cursin'\",\n",
       " \"cussin'\",\n",
       " \"cuttin'\",\n",
       " \"dancin'\",\n",
       " \"darlin'\",\n",
       " \"declarin'\",\n",
       " \"defendin'\",\n",
       " \"dinin'-room\",\n",
       " \"doctorin'\",\n",
       " \"doin'\",\n",
       " \"dootin'\",\n",
       " \"drag-gin'\",\n",
       " \"dreamin'\",\n",
       " \"dressin'-room\",\n",
       " \"drinkin'\",\n",
       " \"duckin'\",\n",
       " \"durin'\",\n",
       " \"dyin'\",\n",
       " \"eatin'\",\n",
       " 'ebenin',\n",
       " \"endurin'\",\n",
       " \"enjoyin'\",\n",
       " \"entertainin'\",\n",
       " \"ev'nin'\",\n",
       " \"ev'rythin'\",\n",
       " \"evenin'\",\n",
       " \"everythin'\",\n",
       " \"excusin'\",\n",
       " \"fallin'\",\n",
       " \"feedin'\",\n",
       " \"feelin'\",\n",
       " \"feelin's\",\n",
       " \"fellow-feelin'\",\n",
       " \"fiddlin'\",\n",
       " \"figgerin'\",\n",
       " \"fightin'\",\n",
       " \"findin'\",\n",
       " \"fishin'\",\n",
       " \"flutterin'\",\n",
       " \"foolin'\",\n",
       " \"fryin'\",\n",
       " \"furrin'\",\n",
       " \"gallopin'\",\n",
       " \"gittin'\",\n",
       " \"givin'\",\n",
       " \"good-mornin'\",\n",
       " \"grindin'\",\n",
       " 'groanin',\n",
       " \"groanin'\",\n",
       " \"gropin'\",\n",
       " \"guidin'\",\n",
       " 'gwyn',\n",
       " \"habin'\",\n",
       " \"handlin'\",\n",
       " \"hangin'\",\n",
       " \"hankerin'\",\n",
       " \"happenin'\",\n",
       " \"hard-stridin'\",\n",
       " \"havin'\",\n",
       " \"headin'\",\n",
       " \"hearin'\",\n",
       " \"helpin'\",\n",
       " \"hittin'\",\n",
       " \"holdin'\",\n",
       " \"hollerin'\",\n",
       " \"hopin'\",\n",
       " \"howlin'\",\n",
       " \"huntin'\",\n",
       " \"hurtin'\",\n",
       " \"int'restin'\",\n",
       " \"int'ruptin'\",\n",
       " \"ironin'\",\n",
       " \"jokin'\",\n",
       " 'keepin',\n",
       " \"keepin'\",\n",
       " \"killin'\",\n",
       " \"knittin'\",\n",
       " \"knowin'\",\n",
       " \"kommin'\",\n",
       " \"laffin'\",\n",
       " \"larfin'\",\n",
       " \"lashin'\",\n",
       " \"laughin'\",\n",
       " \"laughin'-stock\",\n",
       " \"layin'\",\n",
       " \"learnin'\",\n",
       " \"leavin'\",\n",
       " \"lettin'\",\n",
       " \"libbin'\",\n",
       " \"lifin'\",\n",
       " \"lightnin'\",\n",
       " \"likin'\",\n",
       " \"limpin'\",\n",
       " \"lissenin'\",\n",
       " 'listenin',\n",
       " \"livin'\",\n",
       " \"loafin'\",\n",
       " 'lookin',\n",
       " \"lookin'\",\n",
       " \"losin'\",\n",
       " \"loungin'\",\n",
       " \"lovin'\",\n",
       " \"lyin'\",\n",
       " \"makin'\",\n",
       " \"marchin'\",\n",
       " \"marryin'\",\n",
       " \"mawnin'\",\n",
       " \"meanin'\",\n",
       " \"meddlin'\",\n",
       " \"meetin'\",\n",
       " \"mekin'\",\n",
       " \"mendin'\",\n",
       " \"mentionin'\",\n",
       " \"mettlin'\",\n",
       " \"middlin'\",\n",
       " \"moanin'\",\n",
       " 'mornin',\n",
       " \"mornin'\",\n",
       " \"mornin's\",\n",
       " \"movin'\",\n",
       " \"naethin'\",\n",
       " \"namin'\",\n",
       " \"nigger-lovin'\",\n",
       " 'noffin',\n",
       " 'nofin',\n",
       " \"nothin'\",\n",
       " 'nuffin',\n",
       " \"nuffin'\",\n",
       " \"nursin'\",\n",
       " \"nussin'\",\n",
       " \"nut'in'\",\n",
       " \"nuthin'\",\n",
       " 'nuttin',\n",
       " \"nuttin'\",\n",
       " \"onythin'\",\n",
       " \"orderin'\",\n",
       " \"outgrowin'\",\n",
       " \"ownin'\",\n",
       " \"p'intin'\",\n",
       " \"paddlin'\",\n",
       " \"partin'\",\n",
       " \"passin'\",\n",
       " \"payin'\",\n",
       " 'perishin',\n",
       " \"persuadin'\",\n",
       " \"pesterin'\",\n",
       " \"philanderin'\",\n",
       " \"pickin'\",\n",
       " \"picklin'\",\n",
       " \"pizenin'\",\n",
       " \"playin'\",\n",
       " \"poachin'\",\n",
       " \"pokin'\",\n",
       " \"prayer-meetin'\",\n",
       " \"prayin'\",\n",
       " \"pretendin'\",\n",
       " \"prevailin'\",\n",
       " \"punchin'\",\n",
       " \"puttin'\",\n",
       " \"rakin'\",\n",
       " \"readin'\",\n",
       " \"reconnoitrin'\",\n",
       " \"restin'\",\n",
       " \"revivin'\",\n",
       " \"riddin'\",\n",
       " \"ridin'\",\n",
       " \"risin'\",\n",
       " \"roarin'\",\n",
       " \"roastin'\",\n",
       " \"rockin'\",\n",
       " \"rollin'\",\n",
       " \"rottin'\",\n",
       " \"runnin'\",\n",
       " \"savin'\",\n",
       " \"sayin'\",\n",
       " \"scattarin'\",\n",
       " \"scoffin'\",\n",
       " \"seein'\",\n",
       " \"sendin'\",\n",
       " \"settin'\",\n",
       " \"shinin'\",\n",
       " \"shippin'-master\",\n",
       " \"shippin'-office\",\n",
       " \"shootin'\",\n",
       " \"singin'\",\n",
       " \"sinkin'\",\n",
       " \"sleepin'\",\n",
       " \"slippin'\",\n",
       " \"smellin'\",\n",
       " \"smilin'\",\n",
       " \"smitin'\",\n",
       " \"smokin'\",\n",
       " \"smotherin'\",\n",
       " \"snappin'\",\n",
       " \"sneakin'\",\n",
       " \"snickerin'\",\n",
       " 'somefin',\n",
       " \"somefin'\",\n",
       " 'somepin',\n",
       " \"somethin'\",\n",
       " \"sparkin'\",\n",
       " 'speakin',\n",
       " \"speakin'\",\n",
       " 'speckin',\n",
       " \"spendin'\",\n",
       " \"spittin'\",\n",
       " \"spo'tin'\",\n",
       " \"spoilin'\",\n",
       " \"spreadin'\",\n",
       " \"squallin'\",\n",
       " \"standin'\",\n",
       " \"starchin'\",\n",
       " \"startin'\",\n",
       " \"starvin'\",\n",
       " 'stayin',\n",
       " \"stayin'\",\n",
       " \"stealin'\",\n",
       " \"steppin'\",\n",
       " \"stingin'\",\n",
       " \"stinkin'\",\n",
       " \"stompin'\",\n",
       " \"stoomblin'\",\n",
       " \"stoopin'\",\n",
       " \"stoppin'\",\n",
       " \"stringin'\",\n",
       " \"strivin'\",\n",
       " \"sufferin'\",\n",
       " 'sumfin',\n",
       " \"sumfin'\",\n",
       " 'sumpin',\n",
       " \"sumthin'\",\n",
       " \"sunthin'\",\n",
       " 'suthin',\n",
       " \"suthin'\",\n",
       " \"swarin'\",\n",
       " \"sweetenin'\",\n",
       " \"swellin'\",\n",
       " \"swimmin'-pool\",\n",
       " \"tacklin'\",\n",
       " \"takin'\",\n",
       " \"talkin'\",\n",
       " \"tearin'\",\n",
       " 'tellin',\n",
       " \"tellin'\",\n",
       " 'tendin',\n",
       " \"thankin'\",\n",
       " \"thievin'\",\n",
       " \"thinkin'\",\n",
       " \"throwin'\",\n",
       " 'thunderin',\n",
       " \"thunderin'\",\n",
       " \"tomfoolin'\",\n",
       " \"tossin'\",\n",
       " 'totin',\n",
       " \"totin'\",\n",
       " \"traffickin'\",\n",
       " \"trapesin'\",\n",
       " \"travelin'\",\n",
       " \"travellin'\",\n",
       " \"triflin'\",\n",
       " \"trottin'\",\n",
       " \"troublin'\",\n",
       " 'tryin',\n",
       " \"tryin'\",\n",
       " \"tumblin'\",\n",
       " \"tyin'\",\n",
       " \"visitin'\",\n",
       " 'waggin',\n",
       " \"waitin'\",\n",
       " \"wakin'\",\n",
       " \"walk-comin'-from\",\n",
       " \"walkin'\",\n",
       " \"wallopin'\",\n",
       " \"wantin'\",\n",
       " \"was-carryin'\",\n",
       " \"was-hurryin'-aroun'\",\n",
       " \"was-skeered-awful-skeered-an'-somethin'-tole\",\n",
       " \"wastin'\",\n",
       " 'wavin',\n",
       " \"weak'nin'\",\n",
       " \"weavin'\",\n",
       " \"weddin'\",\n",
       " \"weddin'-dress\",\n",
       " 'whichin',\n",
       " 'whinin',\n",
       " \"whinnerin'\",\n",
       " \"whippin'\",\n",
       " \"whoppin'\",\n",
       " \"willin'\",\n",
       " \"wishin'\",\n",
       " 'wonderin',\n",
       " \"wonderin'\",\n",
       " \"worryin'\",\n",
       " \"wrappin'\",\n",
       " \"writin'\",\n",
       " 'xcitin',\n",
       " \"yappin'\",\n",
       " \"yellin'\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonological_variations[\"['NG']->['N']\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c34b9-887f-4b64-92d6-74a87ec2c2ec",
   "metadata": {},
   "source": [
    "for each get upper and lowercase word variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fd5902-8395-4971-bdd6-a96e442e3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in verbs for a-verb cases\n",
    "for v, wordset in phonological_variations.items():\n",
    "    to_add = set()\n",
    "    for word in wordset:\n",
    "        if word[:2] == \"a-\" or word[:2] == \"A-\":\n",
    "            to_add.add(word[2:])\n",
    "            annotations[word[2:]] = [x[2:] for x in annotations[word]]\n",
    "\n",
    "# create lower and upper case versions of all\n",
    "for v, wordset in phonological_variations.items():\n",
    "    to_add = set()\n",
    "    for word in wordset:       \n",
    "        to_add.add(word.capitalize())\n",
    "        to_add.add(word.lower())\n",
    "        annotations[word.capitalize()] = [x.capitalize() for x in annotations[word]]\n",
    "        annotations[word.lower()] = [x.lower() for x in annotations[word]]\n",
    "    phonological_variations[v] = wordset.union(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c712ef08-dc23-4d93-860b-74297f28d13b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"A-bawlin'\",\n",
       " \"A-blamin'\",\n",
       " \"A-burnin'\",\n",
       " \"A-carryin'\",\n",
       " \"A-changin'\",\n",
       " \"A-comin'\",\n",
       " \"A-crawlin'\",\n",
       " \"A-cuttin'\",\n",
       " \"A-dealin'\",\n",
       " \"A-disappearin'\",\n",
       " \"A-dyin'\",\n",
       " \"A-feedin'\",\n",
       " \"A-fetchin'\",\n",
       " \"A-fishin'\",\n",
       " \"A-gittin'\",\n",
       " \"A-hearin'\",\n",
       " \"A-holdin'\",\n",
       " \"A-hurtin'\",\n",
       " \"A-kickin'\",\n",
       " \"A-lookin'\",\n",
       " \"A-prayin'\",\n",
       " \"A-ridin'\",\n",
       " \"A-runnin'\",\n",
       " \"A-rushin'\",\n",
       " \"A-savin'\",\n",
       " \"A-settin'\",\n",
       " \"A-sittin'\",\n",
       " \"A-sleepin'\",\n",
       " \"A-smokin'\",\n",
       " \"A-standin'\",\n",
       " \"A-tormentin'\",\n",
       " \"A-wantin'\",\n",
       " \"A-watchin'\",\n",
       " \"A-wearin'\",\n",
       " \"A-whippin'\",\n",
       " \"A-worryin'\",\n",
       " \"A-worshippin'\",\n",
       " \"A-writin'\",\n",
       " \"Achin'\",\n",
       " \"Agettin'\",\n",
       " \"Allowin'\",\n",
       " \"Alookin'\",\n",
       " \"Amazin'\",\n",
       " \"Anythin'\",\n",
       " \"Askin'\",\n",
       " \"Astonishin'\",\n",
       " \"Awaitin'\",\n",
       " \"B'ilin'\",\n",
       " \"Bad-lookin'\",\n",
       " \"Bearin'\",\n",
       " \"Beatin'\",\n",
       " \"Beggin'\",\n",
       " \"Beginnin'\",\n",
       " \"Bein'\",\n",
       " \"Belongin'\",\n",
       " \"Betrayin'\",\n",
       " \"Bettin'\",\n",
       " \"Bitin'\",\n",
       " \"Blacksmit'in'\",\n",
       " \"Blamin'\",\n",
       " \"Blatherin'\",\n",
       " \"Blessin'\",\n",
       " \"Blockin'\",\n",
       " \"Bloomin'\",\n",
       " \"Blowin'\",\n",
       " \"Boardin'-houses\",\n",
       " \"Book-learnin'\",\n",
       " \"Boostin'\",\n",
       " \"Bossin'\",\n",
       " \"Breakin'\",\n",
       " \"Breavin'\",\n",
       " \"Buckin'-bronco\",\n",
       " \"Bumpin'\",\n",
       " \"Buryin'\",\n",
       " \"Bustin'\",\n",
       " \"Buyin'\",\n",
       " \"Calculatin'\",\n",
       " \"Callatin'\",\n",
       " \"Callin'\",\n",
       " \"Camp-meetin'\",\n",
       " \"Carryin's\",\n",
       " \"Cavin'\",\n",
       " \"Celebratin'\",\n",
       " 'Ceptin',\n",
       " \"Changin'\",\n",
       " \"Chasin'\",\n",
       " \"Chewin'\",\n",
       " \"Clearin'\",\n",
       " \"Cloggin'\",\n",
       " \"Collectin'\",\n",
       " \"Comin'\",\n",
       " \"Comin's\",\n",
       " \"Complainin'\",\n",
       " \"Considerin'\",\n",
       " \"Contendin'\",\n",
       " \"Cookin'\",\n",
       " \"Courtin'\",\n",
       " \"Crackin'\",\n",
       " \"Crawlin'\",\n",
       " 'Creepin',\n",
       " \"Cringin'\",\n",
       " \"Crowdin'\",\n",
       " \"Cryin'\",\n",
       " \"Cuis-lookin'\",\n",
       " 'Cummin',\n",
       " \"Cursin'\",\n",
       " \"Cussin'\",\n",
       " \"Cuttin'\",\n",
       " \"Dancin'\",\n",
       " \"Darlin'\",\n",
       " \"Declarin'\",\n",
       " \"Defendin'\",\n",
       " \"Dinin'-room\",\n",
       " \"Doctorin'\",\n",
       " \"Doin'\",\n",
       " \"Dootin'\",\n",
       " \"Drag-gin'\",\n",
       " \"Dreamin'\",\n",
       " \"Dressin'-room\",\n",
       " \"Drinkin'\",\n",
       " \"Duckin'\",\n",
       " \"Durin'\",\n",
       " \"Dyin'\",\n",
       " \"Eatin'\",\n",
       " 'Ebenin',\n",
       " \"Endurin'\",\n",
       " \"Enjoyin'\",\n",
       " \"Entertainin'\",\n",
       " \"Ev'nin'\",\n",
       " \"Ev'rythin'\",\n",
       " \"Evenin'\",\n",
       " \"Everythin'\",\n",
       " \"Excusin'\",\n",
       " \"Fallin'\",\n",
       " \"Feedin'\",\n",
       " \"Feelin'\",\n",
       " \"Feelin's\",\n",
       " \"Fellow-feelin'\",\n",
       " \"Fiddlin'\",\n",
       " \"Figgerin'\",\n",
       " \"Fightin'\",\n",
       " \"Findin'\",\n",
       " \"Fishin'\",\n",
       " \"Flutterin'\",\n",
       " \"Foolin'\",\n",
       " \"Fryin'\",\n",
       " \"Furrin'\",\n",
       " \"Gallopin'\",\n",
       " \"Gittin'\",\n",
       " \"Givin'\",\n",
       " \"Good-evenin'\",\n",
       " \"Good-mornin'\",\n",
       " \"Grindin'\",\n",
       " 'Groanin',\n",
       " \"Groanin'\",\n",
       " \"Gropin'\",\n",
       " \"Guidin'\",\n",
       " 'Gwyn',\n",
       " \"Habin'\",\n",
       " \"Handlin'\",\n",
       " \"Hangin'\",\n",
       " \"Hankerin'\",\n",
       " \"Happenin'\",\n",
       " \"Hard-stridin'\",\n",
       " \"Havin'\",\n",
       " \"He-said-nothin'\",\n",
       " \"Headin'\",\n",
       " \"Hearin'\",\n",
       " \"Helpin'\",\n",
       " \"Hittin'\",\n",
       " \"Holdin'\",\n",
       " \"Hollerin'\",\n",
       " \"Hopin'\",\n",
       " \"Howlin'\",\n",
       " \"Huntin'\",\n",
       " \"Hurtin'\",\n",
       " \"Int'restin'\",\n",
       " \"Int'ruptin'\",\n",
       " \"Ironin'\",\n",
       " \"Jokin'\",\n",
       " 'Keepin',\n",
       " \"Keepin'\",\n",
       " \"Killin'\",\n",
       " \"Knittin'\",\n",
       " \"Knowin'\",\n",
       " \"Kommin'\",\n",
       " \"Laffin'\",\n",
       " \"Larfin'\",\n",
       " \"Lashin'\",\n",
       " \"Laughin'\",\n",
       " \"Laughin'-stock\",\n",
       " \"Layin'\",\n",
       " \"Learnin'\",\n",
       " \"Leavin'\",\n",
       " \"Lettin'\",\n",
       " \"Libbin'\",\n",
       " \"Lifin'\",\n",
       " \"Lightnin'\",\n",
       " \"Likin'\",\n",
       " \"Limpin'\",\n",
       " \"Lissenin'\",\n",
       " 'Listenin',\n",
       " \"Livin'\",\n",
       " \"Loafin'\",\n",
       " 'Lookin',\n",
       " \"Lookin'\",\n",
       " \"Losin'\",\n",
       " \"Loungin'\",\n",
       " \"Lovin'\",\n",
       " \"Lyin'\",\n",
       " \"Lynchin'\",\n",
       " \"Makin'\",\n",
       " \"Marchin'\",\n",
       " \"Marryin'\",\n",
       " \"Mawnin'\",\n",
       " \"Meanin'\",\n",
       " \"Meddlin'\",\n",
       " \"Meetin'\",\n",
       " \"Mekin'\",\n",
       " \"Mendin'\",\n",
       " \"Mentionin'\",\n",
       " \"Mettlin'\",\n",
       " \"Middlin'\",\n",
       " \"Moanin'\",\n",
       " 'Mornin',\n",
       " \"Mornin'\",\n",
       " \"Mornin's\",\n",
       " \"Movin'\",\n",
       " \"Naethin'\",\n",
       " \"Namin'\",\n",
       " \"Nigger-lovin'\",\n",
       " 'Noffin',\n",
       " \"Noffin'\",\n",
       " 'Nofin',\n",
       " \"Nothin'\",\n",
       " 'Nuffin',\n",
       " \"Nuffin'\",\n",
       " \"Nursin'\",\n",
       " \"Nussin'\",\n",
       " \"Nut'in'\",\n",
       " \"Nuthin'\",\n",
       " 'Nuttin',\n",
       " \"Nuttin'\",\n",
       " \"Onythin'\",\n",
       " \"Orderin'\",\n",
       " \"Outgrowin'\",\n",
       " \"Overpowerin'\",\n",
       " \"Ownin'\",\n",
       " \"P'intin'\",\n",
       " \"Paddlin'\",\n",
       " \"Partin'\",\n",
       " \"Passin'\",\n",
       " \"Payin'\",\n",
       " \"Peddlin'\",\n",
       " 'Perishin',\n",
       " \"Persuadin'\",\n",
       " \"Pesterin'\",\n",
       " \"Philanderin'\",\n",
       " \"Pickin'\",\n",
       " \"Picklin'\",\n",
       " \"Pizenin'\",\n",
       " \"Playin'\",\n",
       " \"Poachin'\",\n",
       " \"Pokin'\",\n",
       " \"Prayer-meetin'\",\n",
       " \"Prayin'\",\n",
       " \"Pretendin'\",\n",
       " \"Prevailin'\",\n",
       " \"Punchin'\",\n",
       " \"Puttin'\",\n",
       " \"Rakin'\",\n",
       " \"Readin'\",\n",
       " \"Reconnoitrin'\",\n",
       " \"Restin'\",\n",
       " \"Revivin'\",\n",
       " \"Riddin'\",\n",
       " \"Ridin'\",\n",
       " \"Risin'\",\n",
       " \"Roarin'\",\n",
       " \"Roastin'\",\n",
       " \"Rockin'\",\n",
       " \"Rollin'\",\n",
       " \"Rottin'\",\n",
       " \"Runnin'\",\n",
       " \"S'posin'\",\n",
       " \"Savin'\",\n",
       " \"Sayin'\",\n",
       " \"Scattarin'\",\n",
       " \"Scoffin'\",\n",
       " \"Seein'\",\n",
       " \"Sendin'\",\n",
       " \"Settin'\",\n",
       " \"Shinin'\",\n",
       " \"Shippin'-master\",\n",
       " \"Shippin'-office\",\n",
       " \"Shirkin'\",\n",
       " \"Shootin'\",\n",
       " \"Singin'\",\n",
       " \"Sinkin'\",\n",
       " \"Sleepin'\",\n",
       " \"Slippin'\",\n",
       " \"Smellin'\",\n",
       " \"Smilin'\",\n",
       " \"Smitin'\",\n",
       " \"Smokin'\",\n",
       " \"Smotherin'\",\n",
       " \"Snappin'\",\n",
       " \"Sneakin'\",\n",
       " \"Snickerin'\",\n",
       " 'Somefin',\n",
       " \"Somefin'\",\n",
       " 'Somepin',\n",
       " \"Somethin'\",\n",
       " \"Sompin's\",\n",
       " \"Sparkin'\",\n",
       " 'Speakin',\n",
       " \"Speakin'\",\n",
       " 'Speckin',\n",
       " \"Spendin'\",\n",
       " \"Spittin'\",\n",
       " \"Spo'tin'\",\n",
       " \"Spoilin'\",\n",
       " \"Spreadin'\",\n",
       " \"Squallin'\",\n",
       " \"Standin'\",\n",
       " \"Starchin'\",\n",
       " \"Startin'\",\n",
       " \"Starvin'\",\n",
       " 'Stayin',\n",
       " \"Stayin'\",\n",
       " \"Stealin'\",\n",
       " \"Steppin'\",\n",
       " \"Stingin'\",\n",
       " \"Stinkin'\",\n",
       " \"Stompin'\",\n",
       " \"Stoomblin'\",\n",
       " \"Stoopin'\",\n",
       " \"Stoppin'\",\n",
       " \"Stringin'\",\n",
       " \"Strivin'\",\n",
       " \"Sufferin'\",\n",
       " 'Sumfin',\n",
       " \"Sumfin'\",\n",
       " 'Sumpin',\n",
       " \"Sumthin'\",\n",
       " \"Sunthin'\",\n",
       " 'Suthin',\n",
       " \"Suthin'\",\n",
       " \"Swarin'\",\n",
       " \"Sweetenin'\",\n",
       " \"Swellin'\",\n",
       " \"Swimmin'-pool\",\n",
       " \"Tacklin'\",\n",
       " \"Takin'\",\n",
       " \"Talkin'\",\n",
       " \"Tearin'\",\n",
       " 'Tellin',\n",
       " \"Tellin'\",\n",
       " 'Tendin',\n",
       " \"Thankin'\",\n",
       " \"Thievin'\",\n",
       " \"Thinkin'\",\n",
       " \"Throwin'\",\n",
       " 'Thunderin',\n",
       " \"Thunderin'\",\n",
       " \"Tomfoolin'\",\n",
       " \"Tossin'\",\n",
       " 'Totin',\n",
       " \"Totin'\",\n",
       " \"Traffickin'\",\n",
       " \"Trapesin'\",\n",
       " \"Travelin'\",\n",
       " \"Travellin'\",\n",
       " \"Triflin'\",\n",
       " \"Trottin'\",\n",
       " \"Troublin'\",\n",
       " 'Tryin',\n",
       " \"Tryin'\",\n",
       " \"Tumblin'\",\n",
       " \"Tyin'\",\n",
       " \"Visitin'\",\n",
       " 'Waggin',\n",
       " \"Waitin'\",\n",
       " \"Waitin's\",\n",
       " \"Wakin'\",\n",
       " \"Walk-comin'-from\",\n",
       " \"Walkin'\",\n",
       " \"Wallopin'\",\n",
       " \"Wantin'\",\n",
       " \"Was-carryin'\",\n",
       " \"Was-hurryin'-aroun'\",\n",
       " \"Was-skeered-awful-skeered-an'-somethin'-tole\",\n",
       " \"Wastin'\",\n",
       " 'Wavin',\n",
       " \"Weak'nin'\",\n",
       " \"Weavin'\",\n",
       " \"Weddin'\",\n",
       " \"Weddin'-dress\",\n",
       " 'Whichin',\n",
       " 'Whinin',\n",
       " \"Whinnerin'\",\n",
       " \"Whippin'\",\n",
       " \"Whoppin'\",\n",
       " \"Willin'\",\n",
       " \"Wishin'\",\n",
       " 'Wonderin',\n",
       " \"Wonderin'\",\n",
       " \"Wool-gatherin'\",\n",
       " \"Worryin'\",\n",
       " \"Wrappin'\",\n",
       " \"Writin'\",\n",
       " 'Xcitin',\n",
       " \"Yappin'\",\n",
       " \"Yellin'\",\n",
       " \"a-bawlin'\",\n",
       " \"a-blamin'\",\n",
       " \"a-burnin'\",\n",
       " \"a-carryin'\",\n",
       " \"a-changin'\",\n",
       " \"a-comin'\",\n",
       " \"a-crawlin'\",\n",
       " \"a-cuttin'\",\n",
       " \"a-dealin'\",\n",
       " \"a-disappearin'\",\n",
       " \"a-dyin'\",\n",
       " \"a-feedin'\",\n",
       " \"a-fetchin'\",\n",
       " \"a-fishin'\",\n",
       " \"a-gittin'\",\n",
       " \"a-hearin'\",\n",
       " \"a-holdin'\",\n",
       " \"a-hurtin'\",\n",
       " \"a-kickin'\",\n",
       " \"a-lookin'\",\n",
       " \"a-prayin'\",\n",
       " \"a-ridin'\",\n",
       " \"a-runnin'\",\n",
       " \"a-rushin'\",\n",
       " \"a-savin'\",\n",
       " \"a-settin'\",\n",
       " \"a-sittin'\",\n",
       " \"a-sleepin'\",\n",
       " \"a-smokin'\",\n",
       " \"a-standin'\",\n",
       " \"a-tormentin'\",\n",
       " \"a-wantin'\",\n",
       " \"a-watchin'\",\n",
       " \"a-wearin'\",\n",
       " \"a-whippin'\",\n",
       " \"a-worryin'\",\n",
       " \"a-worshippin'\",\n",
       " \"a-writin'\",\n",
       " \"achin'\",\n",
       " \"agettin'\",\n",
       " \"allowin'\",\n",
       " \"alookin'\",\n",
       " \"amazin'\",\n",
       " \"anythin'\",\n",
       " \"askin'\",\n",
       " \"astonishin'\",\n",
       " \"awaitin'\",\n",
       " \"b'ilin'\",\n",
       " \"bad-lookin'\",\n",
       " \"bearin'\",\n",
       " \"beatin'\",\n",
       " \"beggin'\",\n",
       " \"beginnin'\",\n",
       " \"bein'\",\n",
       " \"belongin'\",\n",
       " \"betrayin'\",\n",
       " \"bettin'\",\n",
       " \"bitin'\",\n",
       " \"blacksmit'in'\",\n",
       " \"blamin'\",\n",
       " \"blatherin'\",\n",
       " \"blessin'\",\n",
       " \"blockin'\",\n",
       " \"bloomin'\",\n",
       " \"blowin'\",\n",
       " \"boardin'-houses\",\n",
       " \"book-learnin'\",\n",
       " \"boostin'\",\n",
       " \"bossin'\",\n",
       " \"breakin'\",\n",
       " \"breavin'\",\n",
       " \"buckin'-bronco\",\n",
       " \"bumpin'\",\n",
       " \"buryin'\",\n",
       " \"bustin'\",\n",
       " \"buyin'\",\n",
       " \"calculatin'\",\n",
       " \"callatin'\",\n",
       " \"callin'\",\n",
       " \"camp-meetin'\",\n",
       " \"carryin's\",\n",
       " \"cavin'\",\n",
       " \"celebratin'\",\n",
       " 'ceptin',\n",
       " \"changin'\",\n",
       " \"chasin'\",\n",
       " \"chewin'\",\n",
       " \"clearin'\",\n",
       " \"cloggin'\",\n",
       " \"collectin'\",\n",
       " \"comin'\",\n",
       " \"comin's\",\n",
       " \"complainin'\",\n",
       " \"considerin'\",\n",
       " \"contendin'\",\n",
       " \"cookin'\",\n",
       " \"courtin'\",\n",
       " \"crackin'\",\n",
       " \"crawlin'\",\n",
       " 'creepin',\n",
       " \"cringin'\",\n",
       " \"crowdin'\",\n",
       " \"cryin'\",\n",
       " \"cuis-lookin'\",\n",
       " 'cummin',\n",
       " \"cursin'\",\n",
       " \"cussin'\",\n",
       " \"cuttin'\",\n",
       " \"dancin'\",\n",
       " \"darlin'\",\n",
       " \"declarin'\",\n",
       " \"defendin'\",\n",
       " \"dinin'-room\",\n",
       " \"doctorin'\",\n",
       " \"doin'\",\n",
       " \"dootin'\",\n",
       " \"drag-gin'\",\n",
       " \"dreamin'\",\n",
       " \"dressin'-room\",\n",
       " \"drinkin'\",\n",
       " \"duckin'\",\n",
       " \"durin'\",\n",
       " \"dyin'\",\n",
       " \"eatin'\",\n",
       " 'ebenin',\n",
       " \"endurin'\",\n",
       " \"enjoyin'\",\n",
       " \"entertainin'\",\n",
       " \"ev'nin'\",\n",
       " \"ev'rythin'\",\n",
       " \"evenin'\",\n",
       " \"everythin'\",\n",
       " \"excusin'\",\n",
       " \"fallin'\",\n",
       " \"feedin'\",\n",
       " \"feelin'\",\n",
       " \"feelin's\",\n",
       " \"fellow-feelin'\",\n",
       " \"fiddlin'\",\n",
       " \"figgerin'\",\n",
       " \"fightin'\",\n",
       " \"findin'\",\n",
       " \"fishin'\",\n",
       " \"flutterin'\",\n",
       " \"foolin'\",\n",
       " \"fryin'\",\n",
       " \"furrin'\",\n",
       " \"gallopin'\",\n",
       " \"gittin'\",\n",
       " \"givin'\",\n",
       " \"good-evenin'\",\n",
       " \"good-mornin'\",\n",
       " \"grindin'\",\n",
       " 'groanin',\n",
       " \"groanin'\",\n",
       " \"gropin'\",\n",
       " \"guidin'\",\n",
       " 'gwyn',\n",
       " \"habin'\",\n",
       " \"handlin'\",\n",
       " \"hangin'\",\n",
       " \"hankerin'\",\n",
       " \"happenin'\",\n",
       " \"hard-stridin'\",\n",
       " \"havin'\",\n",
       " \"he-said-nothin'\",\n",
       " \"headin'\",\n",
       " \"hearin'\",\n",
       " \"helpin'\",\n",
       " \"hittin'\",\n",
       " \"holdin'\",\n",
       " \"hollerin'\",\n",
       " \"hopin'\",\n",
       " \"howlin'\",\n",
       " \"huntin'\",\n",
       " \"hurtin'\",\n",
       " \"int'restin'\",\n",
       " \"int'ruptin'\",\n",
       " \"ironin'\",\n",
       " \"jokin'\",\n",
       " 'keepin',\n",
       " \"keepin'\",\n",
       " \"killin'\",\n",
       " \"knittin'\",\n",
       " \"knowin'\",\n",
       " \"kommin'\",\n",
       " \"laffin'\",\n",
       " \"larfin'\",\n",
       " \"lashin'\",\n",
       " \"laughin'\",\n",
       " \"laughin'-stock\",\n",
       " \"layin'\",\n",
       " \"learnin'\",\n",
       " \"leavin'\",\n",
       " \"lettin'\",\n",
       " \"libbin'\",\n",
       " \"lifin'\",\n",
       " \"lightnin'\",\n",
       " \"likin'\",\n",
       " \"limpin'\",\n",
       " \"lissenin'\",\n",
       " 'listenin',\n",
       " \"livin'\",\n",
       " \"loafin'\",\n",
       " 'lookin',\n",
       " \"lookin'\",\n",
       " \"losin'\",\n",
       " \"loungin'\",\n",
       " \"lovin'\",\n",
       " \"lyin'\",\n",
       " \"lynchin'\",\n",
       " \"makin'\",\n",
       " \"marchin'\",\n",
       " \"marryin'\",\n",
       " \"mawnin'\",\n",
       " \"meanin'\",\n",
       " \"meddlin'\",\n",
       " \"meetin'\",\n",
       " \"mekin'\",\n",
       " \"mendin'\",\n",
       " \"mentionin'\",\n",
       " \"mettlin'\",\n",
       " \"middlin'\",\n",
       " \"moanin'\",\n",
       " 'mornin',\n",
       " \"mornin'\",\n",
       " \"mornin's\",\n",
       " \"movin'\",\n",
       " \"naethin'\",\n",
       " \"namin'\",\n",
       " \"nigger-lovin'\",\n",
       " 'noffin',\n",
       " \"noffin'\",\n",
       " 'nofin',\n",
       " \"nothin'\",\n",
       " 'nuffin',\n",
       " \"nuffin'\",\n",
       " \"nursin'\",\n",
       " \"nussin'\",\n",
       " \"nut'in'\",\n",
       " \"nuthin'\",\n",
       " 'nuttin',\n",
       " \"nuttin'\",\n",
       " \"onythin'\",\n",
       " \"orderin'\",\n",
       " \"outgrowin'\",\n",
       " \"overpowerin'\",\n",
       " \"ownin'\",\n",
       " \"p'intin'\",\n",
       " \"paddlin'\",\n",
       " \"partin'\",\n",
       " \"passin'\",\n",
       " \"payin'\",\n",
       " \"peddlin'\",\n",
       " 'perishin',\n",
       " \"persuadin'\",\n",
       " \"pesterin'\",\n",
       " \"philanderin'\",\n",
       " \"pickin'\",\n",
       " \"picklin'\",\n",
       " \"pizenin'\",\n",
       " \"playin'\",\n",
       " \"poachin'\",\n",
       " \"pokin'\",\n",
       " \"prayer-meetin'\",\n",
       " \"prayin'\",\n",
       " \"pretendin'\",\n",
       " \"prevailin'\",\n",
       " \"punchin'\",\n",
       " \"puttin'\",\n",
       " \"rakin'\",\n",
       " \"readin'\",\n",
       " \"reconnoitrin'\",\n",
       " \"restin'\",\n",
       " \"revivin'\",\n",
       " \"riddin'\",\n",
       " \"ridin'\",\n",
       " \"risin'\",\n",
       " \"roarin'\",\n",
       " \"roastin'\",\n",
       " \"rockin'\",\n",
       " \"rollin'\",\n",
       " \"rottin'\",\n",
       " \"runnin'\",\n",
       " \"s'posin'\",\n",
       " \"savin'\",\n",
       " \"sayin'\",\n",
       " \"scattarin'\",\n",
       " \"scoffin'\",\n",
       " \"seein'\",\n",
       " \"sendin'\",\n",
       " \"settin'\",\n",
       " \"shinin'\",\n",
       " \"shippin'-master\",\n",
       " \"shippin'-office\",\n",
       " \"shirkin'\",\n",
       " \"shootin'\",\n",
       " \"singin'\",\n",
       " \"sinkin'\",\n",
       " \"sleepin'\",\n",
       " \"slippin'\",\n",
       " \"smellin'\",\n",
       " \"smilin'\",\n",
       " \"smitin'\",\n",
       " \"smokin'\",\n",
       " \"smotherin'\",\n",
       " \"snappin'\",\n",
       " \"sneakin'\",\n",
       " \"snickerin'\",\n",
       " 'somefin',\n",
       " \"somefin'\",\n",
       " 'somepin',\n",
       " \"somethin'\",\n",
       " \"sompin's\",\n",
       " \"sparkin'\",\n",
       " 'speakin',\n",
       " \"speakin'\",\n",
       " 'speckin',\n",
       " \"spendin'\",\n",
       " \"spittin'\",\n",
       " \"spo'tin'\",\n",
       " \"spoilin'\",\n",
       " \"spreadin'\",\n",
       " \"squallin'\",\n",
       " \"standin'\",\n",
       " \"starchin'\",\n",
       " \"startin'\",\n",
       " \"starvin'\",\n",
       " 'stayin',\n",
       " \"stayin'\",\n",
       " \"stealin'\",\n",
       " \"steppin'\",\n",
       " \"stingin'\",\n",
       " \"stinkin'\",\n",
       " \"stompin'\",\n",
       " \"stoomblin'\",\n",
       " \"stoopin'\",\n",
       " \"stoppin'\",\n",
       " \"stringin'\",\n",
       " \"strivin'\",\n",
       " \"sufferin'\",\n",
       " 'sumfin',\n",
       " \"sumfin'\",\n",
       " 'sumpin',\n",
       " \"sumthin'\",\n",
       " \"sunthin'\",\n",
       " 'suthin',\n",
       " \"suthin'\",\n",
       " \"swarin'\",\n",
       " \"sweetenin'\",\n",
       " \"swellin'\",\n",
       " \"swimmin'-pool\",\n",
       " \"tacklin'\",\n",
       " \"takin'\",\n",
       " \"talkin'\",\n",
       " \"tearin'\",\n",
       " 'tellin',\n",
       " \"tellin'\",\n",
       " 'tendin',\n",
       " \"thankin'\",\n",
       " \"thievin'\",\n",
       " \"thinkin'\",\n",
       " \"throwin'\",\n",
       " 'thunderin',\n",
       " \"thunderin'\",\n",
       " \"tomfoolin'\",\n",
       " \"tossin'\",\n",
       " 'totin',\n",
       " \"totin'\",\n",
       " \"traffickin'\",\n",
       " \"trapesin'\",\n",
       " \"travelin'\",\n",
       " \"travellin'\",\n",
       " \"triflin'\",\n",
       " \"trottin'\",\n",
       " \"troublin'\",\n",
       " 'tryin',\n",
       " \"tryin'\",\n",
       " \"tumblin'\",\n",
       " \"tyin'\",\n",
       " \"visitin'\",\n",
       " 'waggin',\n",
       " \"waitin'\",\n",
       " \"waitin's\",\n",
       " \"wakin'\",\n",
       " \"walk-comin'-from\",\n",
       " \"walkin'\",\n",
       " \"wallopin'\",\n",
       " \"wantin'\",\n",
       " \"was-carryin'\",\n",
       " \"was-hurryin'-aroun'\",\n",
       " \"was-skeered-awful-skeered-an'-somethin'-tole\",\n",
       " \"wastin'\",\n",
       " 'wavin',\n",
       " \"weak'nin'\",\n",
       " \"weavin'\",\n",
       " \"weddin'\",\n",
       " \"weddin'-dress\",\n",
       " 'whichin',\n",
       " 'whinin',\n",
       " \"whinnerin'\",\n",
       " \"whippin'\",\n",
       " \"whoppin'\",\n",
       " \"willin'\",\n",
       " \"wishin'\",\n",
       " 'wonderin',\n",
       " \"wonderin'\",\n",
       " \"wool-gatherin'\",\n",
       " \"worryin'\",\n",
       " \"wrappin'\",\n",
       " \"writin'\",\n",
       " 'xcitin',\n",
       " \"yappin'\",\n",
       " \"yellin'\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonological_variations[\"['NG']->['N']\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23e407-7768-49ba-879d-101c94e0ca46",
   "metadata": {},
   "source": [
    "## combine phonological variations with dialectical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e1ac9c6-78e0-4a7e-8099-629e553f821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([\"I'se\", \"i'se\", \"we'se\", \"we's\", \"they'se\", \"they's\", \"dey'se\", \"dey's\", \"We'se\", \"We's\", \"They'se\", \"They's\", \"Dey'se\", \"Dey's\"])\n",
      "dict_keys(['a-gwine', 'agwine', 'ergwine', 'gwine', 'A-gwine', 'Agwine', 'Ergwine', 'Gwine'])\n"
     ]
    }
   ],
   "source": [
    "dialect_words1 = {\n",
    " \"I'se\": 'I am',\n",
    " \"i'se\": 'i am',\n",
    " \"we'se\": \"we are\",\n",
    " \"we's\": \"we are\",\n",
    " \"they'se\": \"they are\",\n",
    " \"they's\": \"they are\",\n",
    " \"dey'se\": \"dey are\",\n",
    " \"dey's\": \"dey are\",\n",
    "}\n",
    "dialect_words1.update({k.capitalize():v.capitalize() for k, v in dialect_words1.items()})\n",
    "print(dialect_words1.keys())\n",
    "\n",
    "dialect_words2 = {\n",
    " 'a-gwine': 'going',\n",
    " 'agwine': 'going',\n",
    " 'ergwine': 'going',\n",
    " 'gwine': 'going',\n",
    "}\n",
    "dialect_words2.update({k.capitalize():v.capitalize() for k, v in dialect_words2.items()})\n",
    "print(dialect_words2.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11492c3f-688a-4c81-a202-4940b6d8cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = copy.deepcopy(phonological_variations)\n",
    "variations[\"dialect_words1\"] = set(dialect_words1.keys())\n",
    "variations[\"dialect_words2\"] = set(dialect_words2.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f60d5-4eee-4374-adea-bc66e9b93ec6",
   "metadata": {},
   "source": [
    "# which speaker descriptors in the American Literature corpus share this vocabulary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55a062-64b8-44b9-b02f-90961b264c5e",
   "metadata": {},
   "source": [
    "## load PG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d6b29b4-1106-4e97-8b83-0fbbde39c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('../llama3.1_70B/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c7c84cb-5698-46b2-a2f9-fcb9a50ece40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer, word_tokenize\n",
    "tk = WhitespaceTokenizer()\n",
    "whitespace_tokenize = tk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34200c69-ef74-48e4-b433-b3254f3abe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'the', \"parents'\", ',', 'not', 'the', \"children's\"]\n",
      "['it', 'Ġis', 'Ġthe', 'Ġparents', \"',\", 'Ġnot', 'Ġthe', 'Ġchildren', \"'s\"]\n",
      "[[0], [1], [2], [3, 4.5], [4.5], [5], [6], [7, 8]]\n"
     ]
    }
   ],
   "source": [
    "def get_words(s:str)->list[str]:\n",
    "    \"\"\" split into words, ensure that contractions aren't split\n",
    "        Note: This is my N wrt., \n",
    "        Note: why not word_tokenize() ? ... because it splits contractions\n",
    "    \"\"\"\n",
    "\n",
    "    # split on whitespace\n",
    "    words = whitespace_tokenize(s)\n",
    "\n",
    "    # split off cases where word is enclosed by '' or `'\n",
    "    words_ = []\n",
    "    for word in words:\n",
    "        if (word[0] == \"'\" and word[-1] == \"'\") or (word[0] == \"`\" and word[-1] == \"'\") or (word[0] == \"\\\"\" and word[-1] == \"\\\"\"):\n",
    "            words_.append(word[0])\n",
    "            words_.append(word[1:-1])\n",
    "            words_.append(word[-1])\n",
    "        else:\n",
    "            words_.append(word)\n",
    "    words = words_\n",
    "    \n",
    "\n",
    "    # split of multiple hyphens, elipses, honorifics, initial (followed by dot), non alphanumeric/hyphen/apostrophe\n",
    "    words_ = []\n",
    "    for word in words:\n",
    "        words_ += re.split(r\"(-{2,}|\\.\\.\\.|Mr\\.|Mrs\\.|Dr\\.|Prof\\.|[A-Z]\\.|[^A-Za-z0-9_'-])\", word)\n",
    "    words = words_\n",
    "\n",
    "    # finally, remove empty strings\n",
    "    words = [word for word in words if word != \"\"]\n",
    "\n",
    "    return words\n",
    "                \n",
    "    \n",
    "def get_words_indices(words, tokens):\n",
    "\n",
    "    words_indices = []\n",
    "\n",
    "    tokens_ = [token[1:] if token[0] == \"Ġ\" else token for token in tokens]\n",
    "\n",
    "    w = 0\n",
    "    state = [\"\", [], True]  # accum of tokens, assum token indices, add to state?\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokens_):\n",
    "\n",
    "        token = tokens_[i]\n",
    "\n",
    "        if tokens[i][-1] == \"Ġ\":  # we have \\s+ that's been mapped to a token ... skip it ... it's rare\n",
    "            i+=1\n",
    "            continue            \n",
    "        \n",
    "        # add more tokens to state and record the previous state\n",
    "        if state[2] == True:\n",
    "            state[0] += token\n",
    "            state[1].append(i)\n",
    "\n",
    "        # we've completed a word!\n",
    "        if state[0] == words[w]:\n",
    "            words_indices.append(state[1])\n",
    "            w += 1\n",
    "            state = [\"\", [], True]\n",
    "\n",
    "        # state exceeds current word ... we have tokens what span words\n",
    "        elif state[0][:len(words[w])]==words[w]:\n",
    "            \n",
    "            # add a decimal to indicate the root to take from token common between words\n",
    "            excess = len(state[0]) - len(words[w])\n",
    "            wanted = len(token) - excess\n",
    "            state[1][-1] = state[1][-1] + wanted/len(token)\n",
    "\n",
    "            words_indices.append(state[1])\n",
    "            w+=1\n",
    "\n",
    "            # init state for next word, incorporating excess of common token\n",
    "            state = [token[-excess:], [i + (excess / len(token))], False]\n",
    "\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return words_indices\n",
    "\n",
    "import math\n",
    "\n",
    "def get_surprisals(words, tokens, chain):\n",
    "\n",
    "    variation_surprisal = []\n",
    "    \n",
    "    for word, word_indices in zip(words, get_words_indices(words, tokens)):\n",
    "        \n",
    "        # are word_indices a decimal? then we need to apply a power to the probability, to split a token that spans words\n",
    "        powers, indices = zip(*[math.modf(i) for i in word_indices])\n",
    "        powers = np.array([1 if p == 0 else p for p in powers])\n",
    "        indices = [int(i) for i in indices]\n",
    "\n",
    "        # log(x^p) = p*log(x)\n",
    "        word_surprisal = (-np.log(chain[indices])*powers).sum()\n",
    "        variation_surprisal.append(word_surprisal)\n",
    "\n",
    "    return variation_surprisal\n",
    "            \n",
    "        \n",
    "# test\n",
    "# s = \"what is this lif', if fulling!...\"s\n",
    "s = \"it is the parents', not the children's\"\n",
    "words = get_words(s)\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(words)\n",
    "print(tokens)\n",
    "words_indices = get_words_indices(words, tokens)\n",
    "print(words_indices)\n",
    "\n",
    "# yields [[0], [1], [2], [3, 4.5], [4.5], [6, 7]], where 4.2 means we take square root wrt., token 4 probability as estimated contribution to prob chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e9d2ac-662b-44c1-9207-56fb961dd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../PG/extract_quotes_via_spaCy/quotes_blacklist.json', 'r') as f:\n",
    "    PG_blacklist = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93e4b49e-7673-4fe3-a957-38b5be3240a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../PG/extract_quotes_via_spaCy/quotes_5Jul.json', 'r') as f:\n",
    "    PG_df = pd.DataFrame([t for i,t in enumerate(json.load(f))], columns = [\"id\", \"p\", \"quote\", \"manner\", \"speaker\"])\n",
    "PG_df['i'] = list(range(len(PG_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bac0de6-48be-4640-844b-5685c33f5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore (as done for chains) ... those quotes in blacklist or not of speakers of interest\n",
    "PG_df = PG_df.loc[(PG_df.loc[:,'i'].isin(PG_blacklist)==False),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd5f3670-d2cd-43ac-aa18-a3a02ccdfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset indices, reset 'i'\n",
    "PG_df.reset_index(drop=True, inplace=True)\n",
    "PG_df['i'] = list(range(len(PG_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4f7cec5-f5c9-4599-96f7-9fa46b9255a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0/2379076 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████████████████████████| 2379076/2379076 [02:01<00:00, 19656.48it/s]\n"
     ]
    }
   ],
   "source": [
    "bad_encoding_i = []\n",
    "bad_encoding_speakers = Counter()\n",
    "for i, (q, speaker) in tqdm(enumerate(zip(PG_df.loc[:,'quote'], PG_df.loc[:,'speaker'])), total=len(PG_df)):\n",
    "    try:\n",
    "        s = q[1:-1]\n",
    "        words = get_words(s)\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        words_indices = get_words_indices(words, tokens)\n",
    "\n",
    "        assert len(words) == len(words_indices)\n",
    "    except:\n",
    "        bad_encoding_i.append(i)\n",
    "        bad_encoding_speakers[speaker] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4363c241-2d7b-4ac2-8cb0-0995b8d2e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop from quotes\n",
    "PG_df.drop(bad_encoding_i, inplace=True)\n",
    "PG_df.reset_index(drop=True, inplace=True)\n",
    "PG_df['i'] = list(range(len(PG_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5298b320-d948-42fd-974b-af33287130b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      8711\n",
       "p                          4\n",
       "quote      \"The Dodge Club,\"\n",
       "manner                  None\n",
       "speaker                 None\n",
       "i                          0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7204f99-cfc3-4cc5-a9a3-d17cd84da38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 2368938/2368938 [00:28<00:00, 82092.22it/s]\n"
     ]
    }
   ],
   "source": [
    "PG_df[\"words\"] = PG_df[\"quote\"].progress_apply(lambda x: get_words(x[1:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dfccc37-bf4e-44cb-87d7-d9617d265932",
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_df[\"sid\"] = PG_df['speaker'] + \"_\" + PG_df['id']\n",
    "PG_df['count'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843b4c7-fa98-4754-ad54-d5170832a327",
   "metadata": {},
   "source": [
    "init. a containeer for capturing (speaker, book id) tuples for speakers of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f37374-463e-463d-a893-c327a45a2730",
   "metadata": {},
   "source": [
    "# identify words demonstrative of variations (of interest) in each quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da784356-4908-4fa2-bd43-eaadede4c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we specify the variations of interest, i.e., variations which distinguish nword speakers wrt., normative reference speakers in literature\n",
    "V = [\n",
    "    \"['NG']->['N']\", \n",
    "    'dialect_words2', \n",
    "    \"['T']->[]\", \n",
    "    \"['ER']->['AH']\", \n",
    "    \"['AH']->[]\", \n",
    "    \"['R']->[]\", \n",
    "    'dialect_words1', \n",
    "    \"['AE']->['AA', 'R']\", \n",
    "    \"['TH']->['F']\", \n",
    "    \"['TH']->['T']\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fd730e3-4bee-471d-b1a0-5989667d05fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 2368938/2368938 [00:06<00:00, 371314.59it/s]\n",
      "100%|█████████████████████████████| 2368938/2368938 [00:03<00:00, 779800.48it/s]\n",
      "100%|█████████████████████████████| 2368938/2368938 [00:04<00:00, 574921.62it/s]\n",
      "100%|█████████████████████████████| 2368938/2368938 [00:04<00:00, 483182.13it/s]\n",
      "100%|█████████████████████████████| 2368938/2368938 [00:12<00:00, 196215.11it/s]\n",
      "100%|█████████████████████████████| 2368938/2368938 [00:12<00:00, 192681.40it/s]\n",
      "100%|████████████████████████████| 2368938/2368938 [00:02<00:00, 1122588.45it/s]\n",
      "100%|█████████████████████████████| 2368938/2368938 [00:13<00:00, 173120.59it/s]\n",
      "100%|████████████████████████████| 2368938/2368938 [00:01<00:00, 1605835.50it/s]\n",
      "100%|█████████████████████████████| 2368938/2368938 [00:15<00:00, 149025.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for v in V:\n",
    "    # PG_df[\"x\"] = PG_df.loc[:, \"words\"].progress_apply(lambda x: bool(x.intersection(variation[v])))\n",
    "    PG_df[v] = PG_df.loc[:, \"words\"].progress_apply(lambda x: variations[v].intersection(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48e145dc-7cfc-4b53-8fef-71eeef1df999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialect_words2</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16534</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"Marse Richard, you can't reason homesickness ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16890</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"Miss Margaret, honey, that's foolishness! Not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16993</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"I suttingly ain't gwine do it to they offspri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17705</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"Yo' Uncle Richard ain't gwine come back to-ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20230</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"I ain't a gwine to stay here, missis,\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368386</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"Happen so, honey, happen so! De French tombst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368461</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"'Well, Gin'l,' he said, 'I'm glad you is got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368581</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"an' Colonel French, my husban' Bud is done go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368586</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"Thank'y, suh, thank'y, Mars' Colonel, an' Mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368607</th>\n",
       "      <td>{gwine}</td>\n",
       "      <td>\"is Mistah Fetters's plantation. You ain' gwin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dialect_words2                                              quote\n",
       "16534          {gwine}  \"Marse Richard, you can't reason homesickness ...\n",
       "16890          {gwine}  \"Miss Margaret, honey, that's foolishness! Not...\n",
       "16993          {gwine}  \"I suttingly ain't gwine do it to they offspri...\n",
       "17705          {gwine}  \"Yo' Uncle Richard ain't gwine come back to-ni...\n",
       "20230          {gwine}            \"I ain't a gwine to stay here, missis,\"\n",
       "...                ...                                                ...\n",
       "2368386        {gwine}  \"Happen so, honey, happen so! De French tombst...\n",
       "2368461        {gwine}  \"'Well, Gin'l,' he said, 'I'm glad you is got ...\n",
       "2368581        {gwine}  \"an' Colonel French, my husban' Bud is done go...\n",
       "2368586        {gwine}  \"Thank'y, suh, thank'y, Mars' Colonel, an' Mis...\n",
       "2368607        {gwine}  \"is Mistah Fetters's plantation. You ain' gwin...\n",
       "\n",
       "[3189 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_df.loc[PG_df.loc[:, 'dialect_words2']!=set(), ['dialect_words2', 'quote']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b2ef5-03db-4573-bc5c-2c2f3d7aa42b",
   "metadata": {},
   "source": [
    "# get sids which ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fdfceee-3c49-430c-84b8-60309d731ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NG']->['N']\n",
      "dialect_words2\n",
      "['T']->[]\n",
      "['ER']->['AH']\n",
      "['AH']->[]\n",
      "['R']->[]\n",
      "dialect_words1\n",
      "['AE']->['AA', 'R']\n",
      "['TH']->['F']\n",
      "['TH']->['T']\n",
      "No. matching quotes 13977\n",
      "No. matching sids 677\n"
     ]
    }
   ],
   "source": [
    "sids = set()\n",
    "\n",
    "# get sids which are in top 100 for any single variation\n",
    "for v in V:\n",
    "    print(v)\n",
    "    \n",
    "    targets = variations[v]\n",
    "\n",
    "    # speakers to ignore\n",
    "    speakers_ignore = [\"he\", \"He\", \"she\", \"She\", \"I\", \"man\", \"woman\", \"gentleman\", \"lady\", \"child\", \"negro\", \"Negro\"]\n",
    "    \n",
    "    # sum counts by sid\n",
    "    agg_df = PG_df.loc[(PG_df.loc[:,v]!=set()) & (~PG_df.loc[:,'speaker'].isin(speakers_ignore)) & (PG_df.loc[:,'speaker'].str.istitle()), ['sid', 'count']].groupby('sid').sum()\n",
    "    S = sorted(list(zip(agg_df.index, agg_df['count'])), key = lambda x: x[1], reverse=True)\n",
    "    sids = sids.union([s[0] for s in S[:min(100, len(S))]])\n",
    "    # print(len(sids), len(S))\n",
    "\n",
    "# No. quotes matching\n",
    "print(\"No. matching quotes\", len(PG_df.loc[PG_df.loc[:, 'sid'].isin(sids),:]))\n",
    "print(\"No. matching sids\", len(sids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a937ad1-5a5f-4e8c-ab73-d2cfbf6fa716",
   "metadata": {},
   "source": [
    "## build a container of quotes for which to get chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2331190-cc5a-4762-a0a6-04287c61d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get all quotes for sids of interest\n",
    "# selected_quotes = []\n",
    "# for i, row in PG_df.loc[PG_df.loc[:, 'sid'].isin(sids),:].iterrows():\n",
    "#     selected_quotes.append([i, row[\"speaker\"], row[\"id\"], row[\"quote\"][1:-1]])\n",
    "\n",
    "# with open(\"RQ2/RQ2_selected_sids.json\", \"w\") as f:\n",
    "#     json.dump(selected_quotes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7626-ecb1-4e25-ac91-cbc62baddcf2",
   "metadata": {},
   "source": [
    "# get contributions by speaker\n",
    "\n",
    "without correcting for downstream effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f484c4c-4974-4c8a-b52f-a92414969f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13977\n",
      "13977\n"
     ]
    }
   ],
   "source": [
    "with open(\"RQ2/RQ2_selected_sids.json\", \"r\") as f:\n",
    "    RQ2_quotes = json.load(f)\n",
    "print(len(RQ2_quotes))\n",
    "\n",
    "with open(\"RQ2/chains_llama3.1_70B_sids.json\", \"r\") as f:\n",
    "    RQ2_chains = json.load(f)\n",
    "print(len(RQ2_chains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da8e96d2-0b5e-47f5-88dd-7e67630bcd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15904, 'Cely', '57418', \"Well, she wa'n't jes as you might say a baby,\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RQ2_quotes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3cbba08-22ae-417f-a3ea-2b88426eedfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1662288]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'Chad': 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_encoding_i = []\n",
    "bad_encoding_speakers = Counter()\n",
    "for (i, speaker, id_, quote), chain in zip(RQ2_quotes, RQ2_chains):\n",
    "    try:\n",
    "        words = get_words(quote)\n",
    "        tokens = tokenizer.tokenize(quote)\n",
    "        words_indices = get_words_indices(words, tokens)\n",
    "\n",
    "        get_surprisals(words, tokens, np.array(chain[1:]))\n",
    "\n",
    "        assert len(words) == len(words_indices)\n",
    "    except:\n",
    "        bad_encoding_i.append(i)\n",
    "        bad_encoding_speakers[speaker] += 1\n",
    "\n",
    "display(bad_encoding_i)\n",
    "display(bad_encoding_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81739d7e-cef4-49e0-8364-9bfb406c474c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13976\n",
      "id                                                                 57418\n",
      "p                                                                    781\n",
      "quote                    \"Well, she wa'n't jes as you might say a baby,\"\n",
      "manner                                                              said\n",
      "speaker                                                             Cely\n",
      "i                                                                  15904\n",
      "words                  [Well, ,, she, wa'n't, jes, as, you, might, sa...\n",
      "sid                                                           Cely_57418\n",
      "count                                                                  0\n",
      "['NG']->['N']                                                         {}\n",
      "dialect_words2                                                        {}\n",
      "['T']->[]                                                          {jes}\n",
      "['ER']->['AH']                                                        {}\n",
      "['AH']->[]                                                            {}\n",
      "['R']->[]                                                             {}\n",
      "dialect_words1                                                        {}\n",
      "['AE']->['AA', 'R']                                                   {}\n",
      "['TH']->['F']                                                         {}\n",
      "['TH']->['T']                                                         {}\n",
      "Name: 15904, dtype: object\n"
     ]
    }
   ],
   "source": [
    "I = [i for i, speaker, id_, quote in RQ2_quotes if i not in bad_encoding_i]\n",
    "PG_RQ2 = PG_df.loc[I, :]\n",
    "print(len(PG_RQ2))\n",
    "print(PG_RQ2.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64818ffb-23d1-463c-8d8a-1b64bbc8f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2c = {i:c for c, (i, speaker, id_, quote) in enumerate(RQ2_quotes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c180076-3806-4023-b62d-c00c36242094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 13976/13976 [00:00<00:00, 112610.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# get words and surprisals for the selected quotes\n",
    "PG_RQ2[\"words\"] = PG_RQ2[\"quote\"].progress_apply(lambda x: get_words(x[1:-1]))\n",
    "PG_RQ2['surprisals'] = [get_surprisals(get_words(quote), tokenizer.tokenize(quote), np.array(chain[1:])) for (i, speaker, id_, quote), chain in zip(RQ2_quotes, RQ2_chains) if i not in bad_encoding_i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdecbfaf-0f6f-46d7-8041-c3e4651e1414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>p</th>\n",
       "      <th>quote</th>\n",
       "      <th>manner</th>\n",
       "      <th>speaker</th>\n",
       "      <th>i</th>\n",
       "      <th>words</th>\n",
       "      <th>sid</th>\n",
       "      <th>count</th>\n",
       "      <th>['NG']-&gt;['N']</th>\n",
       "      <th>dialect_words2</th>\n",
       "      <th>['T']-&gt;[]</th>\n",
       "      <th>['ER']-&gt;['AH']</th>\n",
       "      <th>['AH']-&gt;[]</th>\n",
       "      <th>['R']-&gt;[]</th>\n",
       "      <th>dialect_words1</th>\n",
       "      <th>['AE']-&gt;['AA', 'R']</th>\n",
       "      <th>['TH']-&gt;['F']</th>\n",
       "      <th>['TH']-&gt;['T']</th>\n",
       "      <th>surprisals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15904</th>\n",
       "      <td>57418</td>\n",
       "      <td>781</td>\n",
       "      <td>\"Well, she wa'n't jes as you might say a baby,\"</td>\n",
       "      <td>said</td>\n",
       "      <td>Cely</td>\n",
       "      <td>15904</td>\n",
       "      <td>[Well, ,, she, wa'n't, jes, as, you, might, sa...</td>\n",
       "      <td>Cely_57418</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{jes}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[10.300479921654476, 24.79325874935989, 10.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15905</th>\n",
       "      <td>57418</td>\n",
       "      <td>781</td>\n",
       "      <td>\"but she was the onlies' one I had, and when a...</td>\n",
       "      <td>said</td>\n",
       "      <td>Cely</td>\n",
       "      <td>15905</td>\n",
       "      <td>[but, she, was, the, onlies', one, I, had, ,, ...</td>\n",
       "      <td>Cely_57418</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'Pears}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[11.430847028240985, 9.08608527794871, 8.40651...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15910</th>\n",
       "      <td>57418</td>\n",
       "      <td>788</td>\n",
       "      <td>\"I haven't always lived in Maryland, Miss Marg...</td>\n",
       "      <td>began</td>\n",
       "      <td>Cely</td>\n",
       "      <td>15910</td>\n",
       "      <td>[I, haven't, always, lived, in, Maryland, ,, M...</td>\n",
       "      <td>Cely_57418</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[12.306202682202313, 31.10923871682436, 8.8106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15935</th>\n",
       "      <td>57418</td>\n",
       "      <td>845</td>\n",
       "      <td>\"take her and put her to bed. This has been ha...</td>\n",
       "      <td>appeared</td>\n",
       "      <td>Cely</td>\n",
       "      <td>15935</td>\n",
       "      <td>[take, her, and, put, her, to, bed, ., This, h...</td>\n",
       "      <td>Cely_57418</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[15.361773905911711, 9.071644365321236, 7.3089...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16552</th>\n",
       "      <td>57418</td>\n",
       "      <td>1510</td>\n",
       "      <td>\"You git a chile in that frame of mind,\"</td>\n",
       "      <td>moralized</td>\n",
       "      <td>Cely</td>\n",
       "      <td>16552</td>\n",
       "      <td>[You, git, a, chile, in, that, frame, of, mind...</td>\n",
       "      <td>Cely_57418</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[13.834049620527484, 5.058872014911746, 7.3611...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363015</th>\n",
       "      <td>13531</td>\n",
       "      <td>384</td>\n",
       "      <td>\"Six dollars, sah,\"</td>\n",
       "      <td>said</td>\n",
       "      <td>Grandison</td>\n",
       "      <td>2363015</td>\n",
       "      <td>[Six, dollars, ,, sah, ,]</td>\n",
       "      <td>Grandison_13531</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[9.321138481481842, 10.102986883647903, 20.862...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363020</th>\n",
       "      <td>13531</td>\n",
       "      <td>388</td>\n",
       "      <td>\"Wot par'ble?\"</td>\n",
       "      <td>said</td>\n",
       "      <td>Grandison</td>\n",
       "      <td>2363020</td>\n",
       "      <td>[Wot, par'ble, ?]</td>\n",
       "      <td>Grandison_13531</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{par'ble}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[26.023117188254314, 31.57235209086705, 12.995...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363023</th>\n",
       "      <td>13531</td>\n",
       "      <td>392</td>\n",
       "      <td>\"Git up,\"</td>\n",
       "      <td>said</td>\n",
       "      <td>Grandison</td>\n",
       "      <td>2363023</td>\n",
       "      <td>[Git, up, ,]</td>\n",
       "      <td>Grandison_13531</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[7.981849275995357, 7.652085454951323, 21.0512...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368152</th>\n",
       "      <td>19746</td>\n",
       "      <td>554</td>\n",
       "      <td>\"Yo' boss is a godsen' ter dis town,\"</td>\n",
       "      <td>declared</td>\n",
       "      <td>Archie</td>\n",
       "      <td>2368152</td>\n",
       "      <td>[Yo', boss, is, a, godsen', ter, dis, town, ,]</td>\n",
       "      <td>Archie_19746</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[17.326783821761197, 9.31851796445723, 9.10037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368153</th>\n",
       "      <td>19746</td>\n",
       "      <td>554</td>\n",
       "      <td>\"he sho' is. De w'ite folks says de young nigg...</td>\n",
       "      <td>declared</td>\n",
       "      <td>Archie</td>\n",
       "      <td>2368153</td>\n",
       "      <td>[he, sho', is, ., De, w'ite, folks, says, de, ...</td>\n",
       "      <td>Archie_19746</td>\n",
       "      <td>0</td>\n",
       "      <td>{triflin', nothin'}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{lef', don', ain'}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'lone}</td>\n",
       "      <td>{Lawd}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{Norf}</td>\n",
       "      <td>{}</td>\n",
       "      <td>[4.729165527839956, 19.668056857565876, 7.5801...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13976 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     p                                              quote  \\\n",
       "15904    57418   781    \"Well, she wa'n't jes as you might say a baby,\"   \n",
       "15905    57418   781  \"but she was the onlies' one I had, and when a...   \n",
       "15910    57418   788  \"I haven't always lived in Maryland, Miss Marg...   \n",
       "15935    57418   845  \"take her and put her to bed. This has been ha...   \n",
       "16552    57418  1510           \"You git a chile in that frame of mind,\"   \n",
       "...        ...   ...                                                ...   \n",
       "2363015  13531   384                                \"Six dollars, sah,\"   \n",
       "2363020  13531   388                                     \"Wot par'ble?\"   \n",
       "2363023  13531   392                                          \"Git up,\"   \n",
       "2368152  19746   554              \"Yo' boss is a godsen' ter dis town,\"   \n",
       "2368153  19746   554  \"he sho' is. De w'ite folks says de young nigg...   \n",
       "\n",
       "            manner    speaker        i  \\\n",
       "15904         said       Cely    15904   \n",
       "15905         said       Cely    15905   \n",
       "15910        began       Cely    15910   \n",
       "15935     appeared       Cely    15935   \n",
       "16552    moralized       Cely    16552   \n",
       "...            ...        ...      ...   \n",
       "2363015       said  Grandison  2363015   \n",
       "2363020       said  Grandison  2363020   \n",
       "2363023       said  Grandison  2363023   \n",
       "2368152   declared     Archie  2368152   \n",
       "2368153   declared     Archie  2368153   \n",
       "\n",
       "                                                     words              sid  \\\n",
       "15904    [Well, ,, she, wa'n't, jes, as, you, might, sa...       Cely_57418   \n",
       "15905    [but, she, was, the, onlies', one, I, had, ,, ...       Cely_57418   \n",
       "15910    [I, haven't, always, lived, in, Maryland, ,, M...       Cely_57418   \n",
       "15935    [take, her, and, put, her, to, bed, ., This, h...       Cely_57418   \n",
       "16552    [You, git, a, chile, in, that, frame, of, mind...       Cely_57418   \n",
       "...                                                    ...              ...   \n",
       "2363015                          [Six, dollars, ,, sah, ,]  Grandison_13531   \n",
       "2363020                                  [Wot, par'ble, ?]  Grandison_13531   \n",
       "2363023                                       [Git, up, ,]  Grandison_13531   \n",
       "2368152     [Yo', boss, is, a, godsen', ter, dis, town, ,]     Archie_19746   \n",
       "2368153  [he, sho', is, ., De, w'ite, folks, says, de, ...     Archie_19746   \n",
       "\n",
       "         count        ['NG']->['N'] dialect_words2           ['T']->[]  \\\n",
       "15904        0                   {}             {}               {jes}   \n",
       "15905        0                   {}             {}                  {}   \n",
       "15910        0                   {}             {}                  {}   \n",
       "15935        0                   {}             {}                  {}   \n",
       "16552        0                   {}             {}                  {}   \n",
       "...        ...                  ...            ...                 ...   \n",
       "2363015      0                   {}             {}                  {}   \n",
       "2363020      0                   {}             {}                  {}   \n",
       "2363023      0                   {}             {}                  {}   \n",
       "2368152      0                   {}             {}                  {}   \n",
       "2368153      0  {triflin', nothin'}             {}  {lef', don', ain'}   \n",
       "\n",
       "        ['ER']->['AH'] ['AH']->[] ['R']->[] dialect_words1  \\\n",
       "15904               {}         {}        {}             {}   \n",
       "15905               {}   {'Pears}        {}             {}   \n",
       "15910               {}         {}        {}             {}   \n",
       "15935               {}         {}        {}             {}   \n",
       "16552               {}         {}        {}             {}   \n",
       "...                ...        ...       ...            ...   \n",
       "2363015             {}         {}        {}             {}   \n",
       "2363020             {}  {par'ble}        {}             {}   \n",
       "2363023             {}         {}        {}             {}   \n",
       "2368152             {}         {}        {}             {}   \n",
       "2368153             {}    {'lone}    {Lawd}             {}   \n",
       "\n",
       "        ['AE']->['AA', 'R'] ['TH']->['F'] ['TH']->['T']  \\\n",
       "15904                    {}            {}            {}   \n",
       "15905                    {}            {}            {}   \n",
       "15910                    {}            {}            {}   \n",
       "15935                    {}            {}            {}   \n",
       "16552                    {}            {}            {}   \n",
       "...                     ...           ...           ...   \n",
       "2363015                  {}            {}            {}   \n",
       "2363020                  {}            {}            {}   \n",
       "2363023                  {}            {}            {}   \n",
       "2368152                  {}            {}            {}   \n",
       "2368153                  {}        {Norf}            {}   \n",
       "\n",
       "                                                surprisals  \n",
       "15904    [10.300479921654476, 24.79325874935989, 10.009...  \n",
       "15905    [11.430847028240985, 9.08608527794871, 8.40651...  \n",
       "15910    [12.306202682202313, 31.10923871682436, 8.8106...  \n",
       "15935    [15.361773905911711, 9.071644365321236, 7.3089...  \n",
       "16552    [13.834049620527484, 5.058872014911746, 7.3611...  \n",
       "...                                                    ...  \n",
       "2363015  [9.321138481481842, 10.102986883647903, 20.862...  \n",
       "2363020  [26.023117188254314, 31.57235209086705, 12.995...  \n",
       "2363023  [7.981849275995357, 7.652085454951323, 21.0512...  \n",
       "2368152  [17.326783821761197, 9.31851796445723, 9.10037...  \n",
       "2368153  [4.729165527839956, 19.668056857565876, 7.5801...  \n",
       "\n",
       "[13976 rows x 20 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PG_RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18ef9b19-45c6-4d09-bdb1-4cec78927736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.300479921654476,\n",
       " 24.79325874935989,\n",
       " 10.009807888948112,\n",
       " 29.223640937683193,\n",
       " 8.621276364842362,\n",
       " 8.466506710226957,\n",
       " 9.343876706343544,\n",
       " 11.368004967266902,\n",
       " 9.567148656004152,\n",
       " 8.09024207962894,\n",
       " 5.450214310618973,\n",
       " 18.80000372577162]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 15904\n",
    "words = PG_RQ2.loc[i, 'words']\n",
    "tokens = tokenizer.tokenize(PG_RQ2.loc[i, 'quote'][1:-1])\n",
    "chain = np.array(RQ2_chains[i2c[i]][1:])\n",
    "get_surprisals(words, tokens, chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5264aa-ff8d-4c0d-acd7-5ec4c1000bbf",
   "metadata": {},
   "source": [
    "get (proportional) contributions of each variation to each ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "694b94e1-a9ec-46cc-b77c-3b99fdc220b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1148.80it/s]\n"
     ]
    }
   ],
   "source": [
    "contributions = {}\n",
    "\n",
    "# consider each speaker in turn\n",
    "for sid in tqdm(set(PG_RQ2.loc[:,'sid'])):\n",
    "    contributions[sid] = {}\n",
    "\n",
    "    # get x_bar ...\n",
    "    mask = (PG_RQ2.loc[:,\"sid\"] == sid)\n",
    "\n",
    "    # get_suprisals\n",
    "    surprisals = []\n",
    "    for x in PG_RQ2.loc[mask, \"surprisals\"]:\n",
    "        surprisals += x\n",
    "    x_bar = np.array(surprisals).mean()\n",
    "\n",
    "    # get words\n",
    "    words = []\n",
    "    for x in PG_RQ2.loc[mask, \"words\"]:\n",
    "        words += x\n",
    "\n",
    "    # consider each variation in turn\n",
    "    for v in V:    \n",
    "\n",
    "        ss = 0\n",
    "        counter = 0\n",
    "\n",
    "        for word, surprisal in zip(words, surprisals):\n",
    "            if word not in variations[v]:\n",
    "                ss += surprisal\n",
    "                counter += 1\n",
    "\n",
    "        contributions[sid][v] = 100*(x_bar - ss / counter) / x_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "036354c1-b63d-4fab-b762-407b75c349a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1433.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NG']->['N']\n",
      "\t [('Burl_27363', 5.0237572883687385), ('Ben_36531', 4.951759365001411), ('Ambrose_33289', 4.539784754388284), ('Linda_12352', 4.219323337624955), ('Prince_31160', 3.9231533777084504), ('Babe_31160', 3.7375233827211245), ('Alf_44879', 3.6213234145547366), ('Amanda_26928', 3.497682815502206), ('Becky_31160', 3.483738944024594), ('Janet_3619', 3.46690663366562), ('Zachariah_6013', 3.3815881393569294), ('Angel_20292', 3.324276720088045), ('Jeff_44222', 3.2946494937382225), ('Anderson_12352', 3.2884609215557106), ('Milly_11057', 3.238522275042771), ('Belindy_23810', 3.2124256253596584), ('Beamish_429', 3.1828198733948163), ('Cudjo_31406', 3.1226514675612336), ('Doggett_36283', 3.078475563429472), ('Claggett_41591', 3.02825965728287)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1439.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect_words2\n",
      "\t [('Bob_18318', 2.4701891701163667), ('Joshua_41857', 2.059180331290361), ('Clarissa_41857', 1.9052400665448301), ('Fanny_33407', 1.8003195503495064), ('Hannah_41857', 1.7770361199403635), ('Dicey_46381', 1.7291073734520865), ('Dan_2059', 1.7186236832935526), ('Eliab_6058', 1.651127453160761), ('Alek_39644', 1.302438442839571), ('Ned_41857', 1.2839947589992733), ('Brad_26934', 1.2785848073832007), ('Basha_15796', 1.2557677761867438), ('Tildy_24430', 1.189013049907974), ('Tildy_26429', 1.1833781725712298), ('Bijah_13531', 1.0562457417125855), ('Jerry_11228', 1.0445837060779983), ('Belindy_23810', 1.0025535039819542), ('Letty_10973', 0.983627187870083), ('Isham_10973', 0.9772770924919062), ('Tom_12352', 0.971969764141541)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1439.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T']->[]\n",
      "\t [('Doret_4082', 2.254186586395587), ('Letty_10973', 2.201963060656129), ('Delilah_36829', 2.104290405118421), ('Demming_31134', 1.7658862169047125), ('Wellington_11057', 1.7492445390611828), ('Pallas_46586', 1.6626509385117156), ('Jazon_4097', 1.5898973202254052), ('Dicey_46381', 1.360936751597923), ('Jerry_11228', 1.3330586099164363), ('Cookie_12639', 1.2642277484760476), ('Ephraim_35423', 1.2619491906758005), ('Isham_10973', 1.251436488189813), ('Hightower_31160', 1.2485266906238108), ('Bud_26112', 1.242183214019435), ('Sandy_11228', 1.0935572319123175), ('Bobby_33234', 1.044625350422049), ('Zachariah_6013', 1.010342772923554), ('Chunk_5309', 0.9877815292235083), ('Alexandre_15881', 0.9694619755892787), ('Cook_35359', 0.9168127962515469)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1440.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ER']->['AH']\n",
      "\t [('Jerry_11228', 5.124436952591369), ('Jeremiah_33058', 3.5097541189142527), ('Joshua_63223', 2.3856905219849254), ('Zachariah_6013', 2.340702278398244), ('Andy_52782', 2.283474676258064), ('Jeff_44222', 2.2302710196175073), ('Claggett_41591', 2.0465139992376846), ('Sandy_11228', 1.7350835224591872), ('Colonel_27741', 1.4327649151666209), ('Captain_23745', 1.3815921129205189), ('Jeems_18817', 1.3667210834908914), ('Fanny_33407', 1.3505951268650902), ('Jim_15886', 1.3218702719990483), ('Jefferson_33963', 1.298444824049504), ('Isham_10973', 1.1418222260078534), ('Alek_39644', 1.0378051284877434), ('Dicey_46381', 0.9990810686537477), ('Bennett_9149', 0.9908614789523023), ('Eradicate_4230', 0.9766336544711471), ('Bob_18318', 0.9666847296663272)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1434.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AH']->[]\n",
      "\t [('Maria_18687', 4.754375078461045), ('B._472', 4.543408278400094), ('Creole_29439', 3.4022494516635224), ('Prince_31160', 2.6109482973364257), ('Cruzatte_42925', 2.2543982728899454), ('Andy_3734', 2.1741950811370128), ('Jupiter_35462', 2.126707056128358), ('Barnes_28439', 2.1071855359397547), ('Matthias_18332', 1.9396432235382728), ('Pallas_46586', 1.9319592913521024), ('Jute_5309', 1.9213133900608785), ('Maria_27949', 1.8595802974667386), ('Birdsall_6719', 1.8007581036062736), ('Cousin_26631', 1.7569124579573492), ('Eradicate_4230', 1.6231048364494995), ('Grandison_13531', 1.6222117140432941), ('Jimmy_5187', 1.5897117591240058), ('Tempy_24430', 1.5528666212499014), ('Tempy_26429', 1.4875933876361727), ('Dicey_46381', 1.4671087954998645)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1436.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R']->[]\n",
      "\t [('Ephraim_35423', 1.96962986489898), ('Jim_15886', 1.63926859483702), ('Delphy_16505', 1.6193266342693942), ('Alf_44879', 1.1959370202402297), ('Bowles_6431', 1.1575914728379826), ('Euonymus_15881', 1.066958564541016), ('Sam_40013', 0.8622749617344542), ('Milly_11057', 0.8464884491285958), ('Jane_11228', 0.7846867358227055), ('Jeff_44222', 0.7845579985257668), ('Jute_5309', 0.7682214299503576), ('Harry_4746', 0.7594165658701767), ('Abel_6872', 0.6781406570415642), ('Sheba_6719', 0.6567212318887231), ('Ann_16138', 0.650656036729779), ('Blensop_9908', 0.6394215138992858), ('Fanny_33407', 0.6363455524686377), ('Gordon_25884', 0.5573727771538056), ('Birdsall_6719', 0.5546085985196278), ('Dicey_46381', 0.5250503158746274)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1438.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect_words1\n",
      "\t [('Hannibal_6090', 2.4940127046218867), ('Tom_12352', 2.2732973558414487), ('Daniel_12352', 1.7798142971852615), ('Hundred_55012', 1.4965679457796939), ('Wool_3792', 1.329159603769922), ('Dicey_46381', 1.30913675067905), ('Joshua_63223', 1.2728611779946524), ('Barbara_36390', 1.2658972685675633), ('Wool_24337', 1.1326599884000423), ('Sheba_6719', 1.1149819081535406), ('Maria_27949', 0.9981341042671477), ('Salters_12352', 0.9130790619049489), ('Maria_18687', 0.876759845648751), ('Abel_6872', 0.8357839425079276), ('Anderson_12352', 0.7869745999967459), ('Emile_4955', 0.7635485778733359), ('Dolf_30111', 0.742360589828714), ('Berry_6058', 0.6950652255807209), ('Chunk_5309', 0.564127487531683), ('Mammy_41598', 0.5467301251225783)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1436.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AE']->['AA', 'R']\n",
      "\t [('Wool_24337', 2.033975185734332), ('Wool_3792', 1.683982376828534), ('Joshua_63223', 1.536891114565755), ('Zachariah_6013', 1.5105027588872986), ('Anderson_12352', 1.393760747512505), ('Joe_32757', 1.3925188944810236), ('Basha_15796', 1.378525569038142), ('Jute_5309', 1.2222785787882213), ('Harbert_50701', 0.9462382035596003), ('Joe_23789', 0.9166743142730934), ('Delilah_36829', 0.8841389653303217), ('Janet_3619', 0.8156937122054391), ('Chunk_5309', 0.8122396545778734), ('Abel_6872', 0.8041307446116265), ('Dicey_46381', 0.797506715061419), ('Katie_6376', 0.7867484606278997), ('Cely_57418', 0.7494841964464356), ('Daniel_12352', 0.6779883031017934), ('Jinkey_5309', 0.5651245891861945), ('Bub_5660', 0.5307405200304011)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1436.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TH']->['F']\n",
      "\t [('Wool_3792', 1.5903951184648466), ('Isham_10973', 1.15733442463221), ('Cudjo_31406', 1.0436377769697767), ('Jeremiah_33058', 1.0113319404012198), ('Letty_10973', 1.002820187278338), ('Hugh_15796', 0.9696525407738117), ('Milly_11057', 0.8884700344694664), ('Zachariah_6013', 0.7884711328569556), ('Tempy_24430', 0.7677996241253745), ('Toby_31406', 0.7315420226436544), ('Wellington_11057', 0.697402610282019), ('Andy_52782', 0.6729406991457646), ('Tempy_26429', 0.6283763788657377), ('Wool_24337', 0.6207960070241343), ('Jute_5309', 0.583442768185019), ('Jim_15402', 0.5785007870096804), ('Jovial_15774', 0.525670025911959), ('Emile_4955', 0.5215454842111956), ('Nimbus_6058', 0.5190237257931488), ('Ned_41857', 0.5044843476757126)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1437.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TH']->['T']\n",
      "\t [('Onondago_8880', 5.600656777386363), ('Koerner_40398', 2.8475917567868034), ('Creoles_42925', 2.581401109167171), ('Maka_12190', 2.389473738771174), ('Ikey_22804', 2.2754514506013233), ('Cruzatte_42925', 2.0469175756651268), ('Flopper_15578', 1.513498406544359), ('Caesar_9845', 1.4920894109695102), ('Doret_4082', 1.3585520792240515), ('Ches_25809', 1.3086256800691978), ('Belindy_23810', 1.0159469058202717), ('Corlaer_70683', 0.9475260308608753), ('Nick_10434', 0.9299336418751593), ('Laguerre_34567', 0.7858776853132002), ('Chainbearer_34916', 0.7437243341793532), ('Dooley_35374', 0.7435202002665775), ('Juan_51987', 0.5914309465587408), ('Fritz_37149', 0.5140027238813977), ('General_15328', 0.46288671189114444), ('Jinkey_5309', 0.4448252019653905)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for v in V:\n",
    "    X = []\n",
    "    for sid, d in tqdm(contributions.items()):\n",
    "        n = sum(PG_RQ2.loc[:, \"sid\"]==sid)\n",
    "        if n >= 10:  # only consider sids with 10 or more quotations\n",
    "            X.append((sid, d[v]))\n",
    "    print(v)\n",
    "    print('\\t', sorted(X, key=lambda x: x[1], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347dc1be-990e-42ec-87b1-6b8f6aedde51",
   "metadata": {},
   "source": [
    "# get contributions by speaker\n",
    "\n",
    "correcting for downstream effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704e05a-d39d-472a-9357-7ea7f8938531",
   "metadata": {},
   "source": [
    "## get corrected quotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a79e8fea-a21a-4121-b01b-b130ece4762c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That water is dealt, Dat's is good\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'because I say so!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"don't do it\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'and why do you think that?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I am going'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def word_swaps(words:list[str], replacements:dict[str,str]):\n",
    "    swaps = [(word, replacements[word]) for word in words if word in replacements.keys()]\n",
    "    return swaps\n",
    "\n",
    "def replace_strings(quote:str, replacements:dict[str,str]):\n",
    "\n",
    "    # identify what is to be swapped in order\n",
    "    words = get_words(quote)\n",
    "    swaps = deque(word_swaps(words, replacements)) # state\n",
    "\n",
    "    # for every word (in quote), associate it with its quote.find index\n",
    "    word_and_in_text_index = []\n",
    "    start = 0\n",
    "    for word in words:\n",
    "        quote_index = quote.find(word, start)\n",
    "        word_and_in_text_index.append((word, quote_index))\n",
    "        start = quote_index + len(word)\n",
    "    word_and_in_text_index = deque(word_and_in_text_index)\n",
    "\n",
    "    # build the new quote\n",
    "    new_quote:str = \"\"\n",
    "    start = 0\n",
    "    while len(swaps) > 0:\n",
    "        old_word, new_word = swaps.popleft()\n",
    "\n",
    "        while True:\n",
    "            word, quote_index = word_and_in_text_index.popleft()\n",
    "\n",
    "            if old_word == word:\n",
    "                new_quote += quote[start:quote_index] + new_word\n",
    "                start = quote_index + len(old_word)\n",
    "                break      \n",
    "\n",
    "    new_quote += quote[start:]\n",
    "                \n",
    "    return new_quote\n",
    "        \n",
    "display(replace_strings(\"Dat water is dealt, Dat's _is_ good\", {\"Dat\":\"That\", \"_is_\":\"is\"}))\n",
    "display(replace_strings(\"'cause I say so!\", {\"'cause\":\"because\"}))\n",
    "display(replace_strings(\"don' do it\", {\"don'\":\"don't\"}))\n",
    "display(replace_strings(\"and _why_ do you think that?\", {\"_why_\":\"why\"}))\n",
    "display(replace_strings(\"I'se going\", {\"I'se\":\"I am\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af86d476-1116-4238-beb8-e674a03bff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13976it [00:00, 34181.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3972\n",
      "13976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for each quote wrt., our sids of interest, for each variation of interest, build an corrected context\n",
    "corrected_quotes = []\n",
    "count_no_variation = 0\n",
    "count_variation = 0\n",
    "\n",
    "# iterate over the \n",
    "for i, row in tqdm(PG_RQ2.loc[PG_RQ2.loc[:,\"sid\"].isin(sids),:].iterrows()):\n",
    "    if i not in bad_encoding_i:\n",
    "        \n",
    "        quote = row['quote'][1:-1]\n",
    "    \n",
    "        some_variation = False\n",
    "        \n",
    "        # handle phonetic variations\n",
    "        for v in [\"['NG']->['N']\", \"['T']->[]\", \"['ER']->['AH']\", \"['AH']->[]\", \"['R']->[]\", \"['AE']->['AA', 'R']\",\"['TH']->['F']\", \"['TH']->['T']\"]:\n",
    "            demonstrative_words:set = row[v]\n",
    "            if len(demonstrative_words) > 0:\n",
    "                replacements = {x: annotations[x][0] for x in demonstrative_words}\n",
    "                corrected_quote = replace_strings(quote, replacements)\n",
    "                corrected_quotes.append([i, replacements, v, corrected_quote])\n",
    "                some_variation = True\n",
    "    \n",
    "        for v in ['dialect_words1']:\n",
    "            demonstrative_words:set = row[v]\n",
    "            if len(demonstrative_words) > 0:\n",
    "                replacements = {x: dialect_words1[x] for x in demonstrative_words}\n",
    "                corrected_quote = replace_strings(quote, replacements)\n",
    "                corrected_quotes.append([i, replacements, v, corrected_quote])\n",
    "                some_variation = True\n",
    "    \n",
    "        for v in ['dialect_words2']:\n",
    "            demonstrative_words:set = row[v]\n",
    "            if len(demonstrative_words) > 0:\n",
    "                replacements = {x: dialect_words2[x] for x in demonstrative_words}\n",
    "                corrected_quote = replace_strings(quote, replacements)\n",
    "                corrected_quotes.append([i, replacements, v, corrected_quote])\n",
    "                some_variation = True\n",
    "    \n",
    "        # let's check that everything adds up!\n",
    "        if some_variation == False:\n",
    "            count_no_variation += 1\n",
    "        else:\n",
    "            count_variation += 1\n",
    "            \n",
    "print(len(corrected_quotes))\n",
    "print(count_no_variation + count_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6477b1d9-7aaa-4c46-b3ab-d01f8a2c4042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"RQ2_downstream/all_corrected.json\", \"w\") as f:\n",
    "#     json.dump(corrected_quotes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403b908-3b69-415c-8c7d-8b7433c239a1",
   "metadata": {},
   "source": [
    "## load corrected and chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3443bf70-b924-40a6-9e77-b79b32e9ddeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3972\n",
      "3972\n"
     ]
    }
   ],
   "source": [
    "with open(\"RQ2_downstream/all_corrected.json\", \"r\") as f:\n",
    "    corrected_quotes = json.load(f)\n",
    "print(len(corrected_quotes))\n",
    "\n",
    "with open(\"RQ2_downstream/chains_llama3.1_70B.json\", \"r\") as f:\n",
    "    chains_corrected = json.load(f)\n",
    "print(len(chains_corrected))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7c79c-4c40-4b82-bce0-ca0ee3b43adb",
   "metadata": {},
   "source": [
    "## let's check ... are we able to calculate surprisals for all corrected quotes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7982261a-9fc7-46e8-8736-b2021f17a59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15904,\n",
       " {'jes': 'just'},\n",
       " \"['T']->[]\",\n",
       " \"Well, she wa'n't just as you might say a baby,\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_quotes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff21636c-49e5-4ec1-91f3-121cf5010065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3972it [00:00, 4137.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for c, (i, r, v, quote) in tqdm(enumerate(corrected_quotes)):\n",
    "    words = get_words(quote)\n",
    "    tokens = tokenizer.tokenize(quote)\n",
    "    chain = np.array(chains_corrected[c][1:])\n",
    "    get_surprisals(words, tokens, chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425af39f-f6aa-4afd-9f5d-98d482240c17",
   "metadata": {},
   "source": [
    "## calculate contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6aae289-27c3-4b8d-ab21-7f87a4bf4bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 677/677 [00:12<00:00, 53.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# consider all variations found in the corrected quotes\n",
    "V = set([v for i, r, v, quote in corrected_quotes])\n",
    "\n",
    "contributions_c = {}\n",
    "\n",
    "for sid in tqdm(set(PG_RQ2.loc[:,'sid'])):\n",
    "    contributions_c[sid] = {}\n",
    "\n",
    "    # for quotation collection corresponding to sid, get \\bar{S}_collection, which we label x_bar\n",
    "    surprisals = []\n",
    "    for s in PG_RQ2.loc[PG_RQ2.loc[:,'sid']==sid, \"surprisals\"]:\n",
    "        surprisals += s\n",
    "    x_bar =  np.mean(surprisals)\n",
    "    \n",
    "    # consider each variation in turn\n",
    "    for v in V:\n",
    "\n",
    "        # where I is the PG_RQ2 indices in the corrections which match sid and variation considered\n",
    "        i2c = {i:c for c, (i, d, variation, quote) in enumerate(corrected_quotes) if v==variation and PG_RQ2.loc[i,'sid']==sid}\n",
    "        I = list(i2c.keys())\n",
    "\n",
    "        # collect surprisal for all non-corrected quotes (wrt., variation considered), for sid\n",
    "        surprisals = []\n",
    "        for s in PG_RQ2.loc[(~PG_RQ2.index.isin(I)) & (PG_RQ2.loc[:,\"sid\"]==sid), \"surprisals\"]:\n",
    "            surprisals += s\n",
    "\n",
    "        # collect surprisals for \n",
    "        for i in I:\n",
    "\n",
    "            # get surprisals (by word) wrt., corrected quotations\n",
    "            quote_c = corrected_quotes[i2c[i]][3]\n",
    "            chain_c = np.array(chains_corrected[i2c[i]][1:])  # ignore standard beginning token given to all chains\n",
    "            words_c = get_words(quote_c)\n",
    "            tokens_c = tokenizer.tokenize(quote_c)\n",
    "            surprisals_c = get_surprisals(words_c, tokens_c, chain_c)\n",
    "\n",
    "\n",
    "            # sum the surprisals where corresponding words are unchanged between correction and original \n",
    "            matcher = difflib.SequenceMatcher(None, PG_RQ2.loc[i, 'words'], words_c)\n",
    "            opcodes = matcher.get_opcodes()\n",
    "            for tag, i1, i2, j1, j2 in opcodes:  # where x1\n",
    "                if tag == \"equal\":\n",
    "                    surprisals += surprisals_c[j1:j2]\n",
    "    \n",
    "        # report\n",
    "        contributions_c[sid][v] = 100*(x_bar - np.array(surprisals).mean())/x_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "397c71b1-f3e6-4ee0-891e-1bd3df66a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1307.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect_words1\n",
      "\t [('Tom_12352', 2.9390584210041886), ('Hannibal_6090', 2.8765395830710205), ('Daniel_12352', 1.7571945091994754), ('Hundred_55012', 1.7363105441098197), ('Wool_3792', 1.7118320125233144), ('Joshua_63223', 1.7110194134608334), ('Dicey_46381', 1.4484662490347922), ('Wool_24337', 1.4386347448116659), ('Sheba_6719', 1.331762845806213), ('Barbara_36390', 1.3195163701594073), ('Abel_6872', 1.2199185289922951), ('Maria_27949', 1.1147815387527567), ('Berry_6058', 1.0157565320395492), ('Salters_12352', 1.0081329249788686), ('Bud_26112', 1.0006277481286676), ('Maria_18687', 0.9882277756710929), ('Dolf_30111', 0.9244306490905603), ('Anderson_12352', 0.8843221392018511), ('Chunk_5309', 0.8413416632258779), ('Emile_4955', 0.8084398733823298)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1414.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T']->[]\n",
      "\t [('Doret_4082', 2.5152303813209005), ('Cookie_12639', 2.4316030346248985), ('Delilah_36829', 2.2090681785638995), ('Letty_10973', 2.1550970149701425), ('Jazon_4097', 1.8889194641496019), ('Wellington_11057', 1.7699896782760387), ('Bobby_33234', 1.6406721586819855), ('Jerry_11228', 1.5402490585732327), ('Ephraim_35423', 1.5317041916152614), ('Pallas_46586', 1.4029264434828412), ('Hightower_31160', 1.3786341060610938), ('Isham_10973', 1.200921503728235), ('Demming_31134', 1.1607504374846436), ('Dicey_46381', 1.1351165726720815), ('Jim_15886', 1.0896395265387953), ('Bud_26112', 0.9889766906199602), ('Alexandre_15881', 0.9433574869578928), ('Jefferson_33963', 0.8273196513713529), ('Bishop_23637', 0.8018186546903455), ('Billy_5187', 0.7895140022097054)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1408.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TH']->['T']\n",
      "\t [('Onondago_8880', 4.823120470558763), ('Koerner_40398', 2.76979678353384), ('Creoles_42925', 2.5598885277259145), ('Ikey_22804', 2.3675608555697076), ('Maka_12190', 2.3625756164924256), ('Cruzatte_42925', 2.0397382209565094), ('Doret_4082', 1.4602869554285065), ('Flopper_15578', 1.3945470362966472), ('Ches_25809', 1.3061893413743704), ('Chainbearer_34916', 1.1989139812889937), ('Caesar_9845', 1.051862365587308), ('Belindy_23810', 1.0237251945244326), ('Corlaer_70683', 1.0097741926082913), ('Nick_10434', 0.9585991112407487), ('Laguerre_34567', 0.8271469806036231), ('Dooley_35374', 0.7567051500888772), ('Fritz_37149', 0.5277453102092591), ('Juan_51987', 0.4386541285003227), ('Jack_26429', 0.43476599925028825), ('Jack_24430', 0.43476599925028825)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1416.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R']->[]\n",
      "\t [('Delphy_16505', 1.9163038674516177), ('Ephraim_35423', 1.6468348059607572), ('Jim_15886', 1.6094660533569665), ('Euonymus_15881', 1.3717100686964163), ('Jane_11228', 1.261987239744906), ('Alf_44879', 1.0434213400539092), ('Bowles_6431', 0.9367745277667101), ('Abel_6872', 0.9285424049269105), ('Ann_16138', 0.8759228224168657), ('Harry_4746', 0.8111702752885832), ('Milly_11057', 0.7881116955287221), ('Jute_5309', 0.7625077571603367), ('Sam_40013', 0.7483206680883507), ('Dicey_46381', 0.7119731632172335), ('Daniel_50494', 0.6212210095730941), ('Tempy_24430', 0.594295848279164), ('Blensop_9908', 0.5937418836584701), ('Sheba_6719', 0.5921101600208288), ('Gordon_25884', 0.5776979530550072), ('Birdsall_6719', 0.5658378828641712)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1417.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NG']->['N']\n",
      "\t [('Burl_27363', 5.189947058678523), ('Ben_36531', 4.931543202476135), ('Ambrose_33289', 4.503479545680077), ('Prince_31160', 4.101124427248771), ('Linda_12352', 3.9652283175278633), ('Milly_11057', 3.828532824341438), ('Becky_31160', 3.6758563915914424), ('Alf_44879', 3.638549012633227), ('Cudjo_31406', 3.4859939188703493), ('Amanda_26928', 3.437100028413665), ('Babe_31160', 3.4326120773926836), ('Zachariah_6013', 3.3585662723361436), ('Isham_10973', 3.333115327390869), ('Anson_21850', 3.3045592161142086), ('Doggett_36283', 3.249514899511497), ('Janet_3619', 3.2003096001130062), ('Mammy_41598', 3.2002365151193093), ('Belindy_23810', 3.1967056641166742), ('Elmer_6908', 3.189593254688915), ('Adam_18286', 3.1012280898964306)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1415.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect_words2\n",
      "\t [('Bob_18318', 2.7043134099076855), ('Fanny_33407', 2.519934855104532), ('Dicey_46381', 2.4721133377249522), ('Clarissa_41857', 2.183391354221218), ('Hannah_41857', 2.1165024198144518), ('Brad_26934', 1.8327371302003264), ('Joshua_41857', 1.801227246891708), ('Dan_2059', 1.793590333777779), ('Ned_41857', 1.6256525219359832), ('Eliab_6058', 1.601874081337017), ('Crissy_50701', 1.5225806301720866), ('Basha_15796', 1.4808506298585142), ('Jerry_11228', 1.471704826729092), ('Tildy_26429', 1.4470826230813558), ('Alek_39644', 1.4348588885738172), ('Tildy_24430', 1.386505079425229), ('Isham_10973', 1.378114423759967), ('Belindy_23810', 1.2591974168433793), ('Ike_34575', 1.1043427201615992), ('Tom_12352', 1.055262424465669)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1403.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AE']->['AA', 'R']\n",
      "\t [('Joshua_63223', 2.0452126231398227), ('Wool_3792', 1.955522627404107), ('Zachariah_6013', 1.637462285799035), ('Anderson_12352', 1.6193297896360848), ('Joe_32757', 1.5967325997593314), ('Wool_24337', 1.454798877938226), ('Jute_5309', 1.317396491105517), ('Tom_12352', 1.08153637853513), ('Dicey_46381', 1.0411117515835295), ('Janet_3619', 0.9947544484150048), ('Katie_6376', 0.984004559248543), ('Ephraim_35423', 0.9672366225375747), ('Chunk_5309', 0.9366949559909402), ('Harbert_50701', 0.9080875425227295), ('Basha_15796', 0.9029097286110728), ('Abel_6872', 0.804503855938033), ('Jinkey_5309', 0.6995238597820032), ('Daniel_12352', 0.6749502635601802), ('Cely_57418', 0.6666512758706676), ('Cleave_22066', 0.6399427185490221)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1344.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AH']->[]\n",
      "\t [('Maria_18687', 5.366696980928583), ('B._472', 5.161058933060685), ('Creole_29439', 3.49950243197036), ('Prince_31160', 2.6734868683704756), ('Matthias_18332', 2.6051050813405436), ('Maria_27949', 2.4834037930474118), ('Cruzatte_42925', 2.3822249491135943), ('Jupiter_35462', 2.1654665447362866), ('Andy_3734', 1.9773327672747163), ('Jute_5309', 1.9476604693048507), ('Jimmy_5187', 1.8647880121891884), ('Benton_18332', 1.8512349143094913), ('Pallas_46586', 1.790331041570259), ('Birdsall_6719', 1.7500202907986002), ('Tempy_24430', 1.7373022444675341), ('Cousin_26631', 1.7308123022612714), ('Grandison_13531', 1.7298466210163927), ('Ann_28439', 1.6737135144721909), ('Eradicate_4230', 1.6545249355146354), ('Tempy_26429', 1.6538865640092455)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1385.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ER']->['AH']\n",
      "\t [('Jerry_11228', 4.917827065427178), ('Jeremiah_33058', 3.6944668660165174), ('Captain_23745', 3.238635428570913), ('Jeff_44222', 2.353986837777089), ('Andy_52782', 2.3533455533941705), ('Colonel_27741', 2.326687126570132), ('Zachariah_6013', 2.193595195693582), ('Claggett_41591', 2.1757300944633826), ('Sandy_11228', 1.7660462381408744), ('Joshua_63223', 1.7623612957699868), ('Jeems_18817', 1.492446905981333), ('Fanny_33407', 1.355208369090468), ('Bob_18318', 1.315079803846893), ('Jim_15886', 1.2036866626914364), ('Jefferson_33963', 1.1327912678388048), ('Isham_10973', 1.0481550029336035), ('Alek_39644', 1.029551807434017), ('Dicey_46381', 0.9683081583943917), ('Anderson_12352', 0.9481247849869717), ('Agnes_41590', 0.810560185583066)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 677/677 [00:00<00:00, 1078.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TH']->['F']\n",
      "\t [('Wool_3792', 1.3662486074351676), ('Isham_10973', 1.3094789735899826), ('Cudjo_31406', 1.1533292337068506), ('Jim_15402', 1.050447066973604), ('Letty_10973', 1.036659753302553), ('Milly_11057', 1.030502800687011), ('Jeremiah_33058', 1.0117688730228742), ('Hugh_15796', 0.9764983906040955), ('Zachariah_6013', 0.8893256203471266), ('Jute_5309', 0.8665683014544661), ('Wellington_11057', 0.8396746756312371), ('Tempy_24430', 0.7604670637940983), ('Toby_31406', 0.7370884682816873), ('Andy_52782', 0.662246796982849), ('Wool_24337', 0.6399077658616067), ('Tempy_26429', 0.6340718719652171), ('Plato_33058', 0.5377648860985372), ('Emile_4955', 0.5328142965962968), ('Nimbus_6058', 0.5261473984842786), ('Ned_41857', 0.5118613547959292)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for v in V:\n",
    "    X = []\n",
    "    for sid, d in tqdm(contributions_c.items()):\n",
    "        n = sum(PG_RQ2.loc[:, \"sid\"]==sid)\n",
    "        if n >= 10:  # only consider sids with 10 or more quotations\n",
    "            X.append((sid, d[v]))\n",
    "    print(v)\n",
    "    print('\\t', sorted(X, key=lambda x: x[1], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c8a38-0816-4466-9789-55d850d4f101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
