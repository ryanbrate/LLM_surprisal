{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde2301f-9d01-480e-87ec-7e3fc7cd3c99",
   "metadata": {},
   "source": [
    "RQ3: According to LLM suprisal, to what extent do quotations attributed to speakers referenced by\n",
    "’n-word’ differ between the American Literate and American News domains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1334d7e9-0d8a-4ab8-906a-7034d4e92b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "import arviz as az\n",
    "import pymc as pm\n",
    "from pprint import pprint as pp\n",
    "import copy\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer, word_tokenize\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05780e2f-486e-4053-93f8-31273c5eb5ce",
   "metadata": {},
   "source": [
    "# Tokenizing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e20869-5e2d-4c93-aeda-8258213880eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../llama3.1_70B/tokenizer/')\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "whitespace_tokenize = tk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ffa074-58df-45e1-b93f-55ac1a7531ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(s:str)->list[str]:\n",
    "    \"\"\" split into words, ensure that contractions aren't split\n",
    "        Note: This is my N wrt., \n",
    "        Note: why not word_tokenize() ? ... because it splits contractions\n",
    "    \"\"\"\n",
    "\n",
    "    # split on whitespace\n",
    "    words = whitespace_tokenize(s)\n",
    "\n",
    "    # split off cases where word is enclosed by '' or `'\n",
    "    words_ = []\n",
    "    for word in words:\n",
    "        if (word[0] == \"'\" and word[-1] == \"'\") or (word[0] == \"`\" and word[-1] == \"'\") or (word[0] == \"\\\"\" and word[-1] == \"\\\"\"):\n",
    "            words_.append(word[0])\n",
    "            words_.append(word[1:-1])\n",
    "            words_.append(word[-1])\n",
    "        else:\n",
    "            words_.append(word)\n",
    "    words = words_\n",
    "    \n",
    "\n",
    "    # split of multiple hyphens, elipses, honorifics, initial (followed by dot), non alphanumeric/hyphen/apostrophe\n",
    "    words_ = []\n",
    "    for word in words:\n",
    "        words_ += re.split(r\"(-{2,}|\\.\\.\\.|Mr\\.|Mrs\\.|Dr\\.|Prof\\.|[A-Z]\\.|[^A-Za-z0-9_'-])\", word)\n",
    "    words = words_\n",
    "\n",
    "    # finally, remove empty strings\n",
    "    words = [word for word in words if word != \"\"]\n",
    "\n",
    "    return words\n",
    "                \n",
    "    \n",
    "def get_words_indices(words, tokens):\n",
    "\n",
    "    words_indices = []\n",
    "\n",
    "    tokens_ = [token[1:] if token[0] == \"Ġ\" else token for token in tokens]\n",
    "\n",
    "    w = 0\n",
    "    state = [\"\", [], True]  # accum of tokens, assum token indices, add to state?\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokens_):\n",
    "\n",
    "        token = tokens_[i]\n",
    "\n",
    "        if tokens[i][-1] == \"Ġ\":  # we have \\s+ that's been mapped to a token ... skip it ... it's rare\n",
    "            i+=1\n",
    "            continue            \n",
    "        \n",
    "        # add more tokens to state and record the previous state\n",
    "        if state[2] == True:\n",
    "            state[0] += token\n",
    "            state[1].append(i)\n",
    "\n",
    "        # we've completed a word!\n",
    "        if state[0] == words[w]:\n",
    "            words_indices.append(state[1])\n",
    "            w += 1\n",
    "            state = [\"\", [], True]\n",
    "\n",
    "        # state exceeds current word ... we have tokens what span words\n",
    "        elif state[0][:len(words[w])]==words[w]:\n",
    "            \n",
    "            # add a decimal to indicate the root to take from token common between words\n",
    "            excess = len(state[0]) - len(words[w])\n",
    "            wanted = len(token) - excess\n",
    "            state[1][-1] = state[1][-1] + wanted/len(token)\n",
    "\n",
    "            words_indices.append(state[1])\n",
    "            w+=1\n",
    "\n",
    "            # init state for next word, incorporating excess of common token\n",
    "            state = [token[-excess:], [i + (excess / len(token))], False]\n",
    "\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return words_indices\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6ab83-33ea-4724-9a30-ce3c83eebc89",
   "metadata": {},
   "source": [
    "# Identify phonological variations in the LOC n-word quotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892fadb-517f-439c-9af9-76e51b64741a",
   "metadata": {},
   "source": [
    "load tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a7d841-21b2-4868-bd5e-503ba888f860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../LOC/tuples_news.json\", \"r\") as f:\n",
    "    tuples_news = json.load(f)\n",
    "len(tuples_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006920b-9441-47ce-98e4-8d6d2ce1401f",
   "metadata": {},
   "source": [
    "identify ood words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5095aa-f58a-4c34-9e49-ca5784a53196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303817"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scowl english words en_us large\n",
    "with open('../../wordlist-en_US-large-2020.12.07/en_US-large.txt', 'r') as f:\n",
    "    en_words = [line.strip('\\n') for line in f.readlines()]\n",
    "en_words += [w.capitalize() for w in en_words]\n",
    "en_words = set(en_words)\n",
    "len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978cc0ad-f445-40b1-9565-8b39ad590546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_standard_contr(word:str)->bool:\n",
    "    # ignore standard contractions 'll, 's, 'd, 've where, the 'stem' is in dict\n",
    "    if (word[-3:] == \"'ll\" and word[:-3] in en_words) or (word[-3:] == \"'ve\" and word[:-3] in en_words) or (word[-2:] == \"'s\" and word[:-2] in en_words) or (word[-3:] == \"n't\" and word[:-3] in en_words) or (word[-3:] == \"'re\" and word[:-3] in en_words)  or (word[-2:] == \"'d\" and word[:-2] in en_words) or (word[-2:] == \"'m\" and word[:-2] in en_words) or (word[-1:] == \"'\" and word[:-1] in en_words):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def is_hyphenated(word:str)->bool:\n",
    "    if all([w in en_words for w in word.split('-')]):\n",
    "        return True\n",
    "    else:\n",
    "        False\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e8df0-61bd-472f-aa01-c5540c40292a",
   "metadata": {},
   "source": [
    "get ood words (commented out, so don't overwrite annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c198759-0f33-4027-835e-9b072493f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words from all quotations\n",
    "words = list(chain(*[get_words(tuple_[1][0]) for tuple_ in tuples_news]))\n",
    "\n",
    "# get ood words\n",
    "ood_words = list(set([word for word in words if not (word.strip('-') in en_words or word in string.punctuation or not is_standard_contr(word.strip('-')) or word.isnumeric() or is_hyphenated(word))]))\n",
    "\n",
    "# save ood words\n",
    "with open(f\"RQ3_ood_words/RQ3_ood_words_pre.json\", 'w') as f:\n",
    "        json.dump({word:[] for word in ood_words}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb74ec-dc14-46be-8727-39e67d77ac94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "annotate ... cell below useful for examining parent quotations of ood words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d15d00-e837-4b0e-a980-c4d5fed96842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup? - <<yah>>, ha! What a looking feller dat is to make soup ob! Heah Caesar, bite him,\n"
     ]
    }
   ],
   "source": [
    "word = \"yah\"\n",
    "for tuple_ in tuples_news:\n",
    "    q = tuple_[1][0]\n",
    "    words = get_words(q)\n",
    "    if word in words:\n",
    "        print(q.replace(word, '<<'+word+'>>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2838d4-4abf-4094-b8f9-441a58851ce7",
   "metadata": {},
   "source": [
    "# Build amended quotations for each targeted variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671202f-e692-44d7-8301-1009867ead1e",
   "metadata": {},
   "source": [
    "## Variations targetted in LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb1171-f3af-4fb3-beeb-96513eab8677",
   "metadata": {},
   "source": [
    "dialect words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cccf6b3-c29e-4b2a-a1ee-80fb3cf425c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([\"i's\", \"i'se\", \"we'se\", \"we's\", \"they'se\", \"they's\", \"dey'se\", \"dey's\", \"I's\", \"I'se\", \"We'se\", \"We's\", \"They'se\", \"They's\", \"Dey'se\", \"Dey's\"])\n",
      "dict_keys(['a-gwine', 'agwine', 'ergwine', 'gwine', 'A-gwine', 'Agwine', 'Ergwine', 'Gwine'])\n"
     ]
    }
   ],
   "source": [
    "dialect_words1 = {\n",
    " \"i's\": 'i am',\n",
    " \"i'se\": 'i am',\n",
    " \"we'se\": \"we are\",\n",
    " \"we's\": \"we are\",\n",
    " \"they'se\": \"they are\",\n",
    " \"they's\": \"they are\",\n",
    " \"dey'se\": \"dey are\",\n",
    " \"dey's\": \"dey are\",\n",
    "}\n",
    "dialect_words1.update({k.capitalize():v.capitalize() for k, v in dialect_words1.items()})\n",
    "print(dialect_words1.keys())\n",
    "\n",
    "dialect_words2 = {\n",
    " 'a-gwine': 'going',\n",
    " 'agwine': 'going',\n",
    " 'ergwine': 'going',\n",
    " 'gwine': 'going',\n",
    "}\n",
    "dialect_words2.update({k.capitalize():v.capitalize() for k, v in dialect_words2.items()})\n",
    "print(dialect_words2.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f5086c-0b7a-42f2-afbc-af505d958b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://chroniclingamerica.loc.gov/lccn/sn83025182/1910-05-22/ed-1/seq-6/#words=negro',\n",
       " [\"Hit only cost me a string er fish ter git married, jedge, but, please God, I'd give a whale ter git rid er her.\",\n",
       "  'said',\n",
       "  'negro']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples_news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319e890-f39b-41ed-a0e2-6eea9fae5d66",
   "metadata": {},
   "source": [
    "italicised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d59bbb1d-c9b2-4ea7-9227-74ec33c3222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of italicised words\n",
    "def is_italicised(word:str)->bool:\n",
    "    if len(word) > 2:\n",
    "        if word[0] == \"_\" and word[-1] == \"_\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "italicised_words = dict()\n",
    "for url, (quote, manner, speaker) in tuples_news:\n",
    "    words = get_words(quote)\n",
    "    for word in words:\n",
    "        if is_italicised(word):\n",
    "            italicised_words[word] = word.strip(\"_\")\n",
    "\n",
    "italicised_words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9add450-7198-4dec-93f0-c1febe788dd1",
   "metadata": {},
   "source": [
    "regularised irregular verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9521b396-32a1-478a-9c9e-b08b9fa094fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularised_irregular = {\n",
    " 'beated': 'beat',\n",
    " 'becomed': 'became',\n",
    " 'beginnned': 'began',\n",
    " 'bended': 'bent',\n",
    " 'bidded': 'bid',\n",
    " 'binded': 'bound', #\n",
    " 'bited': 'bit',\n",
    " 'bleeded': 'bled', #\n",
    " 'breaked': 'broke',\n",
    " 'bringed': 'brought',\n",
    " 'builded': 'built',\n",
    " 'buyed': 'bought',\n",
    " 'catched': 'caught',  # \n",
    " 'choosed': 'chose',\n",
    " 'comed': 'came',\n",
    " 'dealed': 'dealt',\n",
    " 'doesed': 'did',\n",
    " 'drawed': 'drew',\n",
    " 'drinked': 'drunk',\n",
    " 'drived': 'drove', # \n",
    " 'eated': 'ate',\n",
    " 'falled': 'fell',\n",
    " 'feeded': 'fed',\n",
    " 'feeled': 'felt',  # \n",
    " 'fighted': 'fought',\n",
    " 'finded': 'found',\n",
    " 'forgetted': 'forgot',\n",
    " 'getted': 'got',\n",
    " 'gived': 'gave',\n",
    " 'goed': 'went',\n",
    " 'growed': 'grew',\n",
    " 'hased': 'had',\n",
    " 'haved': 'had',\n",
    " 'holded': 'held',\n",
    " 'hurted': 'hurt',\n",
    " 'ised': 'was',\n",
    " 'keeped': 'kept',\n",
    " 'knowed': 'knew',\n",
    " 'leaded': 'led',\n",
    " 'maked': 'made',\n",
    " 'meeted': 'met',\n",
    " 'mistaked': 'mistook',\n",
    " 'readed': 'read',\n",
    " 'rided': 'rode',\n",
    " 'rised': 'rose',\n",
    " 'runned': 'ran',\n",
    " 'sayed': 'said',\n",
    " 'seeked': 'sought',\n",
    " 'sended': 'sent',\n",
    " 'shalled': 'should',\n",
    " 'shooted': 'shot',\n",
    " 'sinked': 'sunk',\n",
    " 'sitted': 'sat',\n",
    " 'sleeped': 'slept',\n",
    " 'speaked': 'spoke',\n",
    " 'spended': 'spent',\n",
    " 'springed': 'sprung',\n",
    " 'standed': 'stood',\n",
    " 'stealed': 'stole',\n",
    " 'striked': 'struck',\n",
    " 'swimmed': 'swum',\n",
    " 'swinged': 'swung',\n",
    " 'taked': 'took',\n",
    " 'thinked': 'thought',\n",
    " 'throwed': 'threw',\n",
    " 'understanded': 'understood',\n",
    " 'winned': 'won',\n",
    " 'writed': 'wrote'}\n",
    "regularised_irregular.update({k.capitalize():v.capitalize() for k, v in regularised_irregular.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632c199-d5f4-4bc7-abdc-99cbd4da1218",
   "metadata": {},
   "source": [
    "### various resources and functions for identifying phonological variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54469f6b-2432-48b4-8710-948b15edc581",
   "metadata": {},
   "source": [
    "load cmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c87036-662b-4e0f-8744-abf2a2bddf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[W][AA][Z]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "import requests\n",
    "lines = requests.get(\"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\").text.splitlines()\n",
    "\n",
    "# ignore eveything up the 'A  AH0'\n",
    "lines = lines[lines.index('A  AH0'):]\n",
    "\n",
    "# ignore stresses as deep phenomizer doesnt have these\n",
    "word2arpa = defaultdict(list)\n",
    "for line in lines:\n",
    "    splits = [split for split in line.split(\" \") if split != '']\n",
    "    word = splits[0].split('(')[0]\n",
    "    arpa = \"\".join(['[' + split.translate(str.maketrans('', '', '0123456789')) + ']' for split in splits[1:]])\n",
    "    if word not in word2arpa: # just get the main pronunciaion\n",
    "        word2arpa[word].append(arpa)\n",
    "\n",
    "word2arpa['WAS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44cfef-be1b-4218-851e-14038b8b9334",
   "metadata": {},
   "source": [
    "load deep phonomizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5765922-48d6-4ba6-ae42-52eefa140d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/bigone/lib/python3.11/site-packages/dp/model/model.py:306: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "/opt/miniconda3/envs/bigone/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[W][AA][Z]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dp\n",
    "from dp.phonemizer import Phonemizer\n",
    "phonemizer = Phonemizer.from_checkpoint('./en_us_cmudict_forward.pt')\n",
    "phonemizer('WAS', lang='en_us')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b11ca8-787a-4489-908d-ca2a82c72809",
   "metadata": {},
   "source": [
    "useful functions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6738f093-acf9-4c9d-a6a9-44449dd73006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[G][OW][IH][NG]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def arpaguess(s:str):\n",
    "    if s.upper() not in word2arpa:\n",
    "        return [phonemizer(s, lang='en_us')]\n",
    "    else:\n",
    "        return word2arpa[s.upper()]\n",
    "\n",
    "arpaguess(\"going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff003c47-5217-49bc-afff-2caf27a4b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['b', 'c'], ['d']]]\n",
      "[[['oughta'], ['ought', 'to']]]\n",
      "[[['oughta'], ['ought', 'to']]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['  it', '- oughta', '?      -\\n', '+ ought', '+ to', '  happen']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "def get_changes(from_:list[str], to_:list[str], return_indices=False)->bool:\n",
    "\n",
    "    # accumulator\n",
    "    changes = []\n",
    "\n",
    "    # state\n",
    "    in_change = False\n",
    "\n",
    "    for x in difflib.ndiff(from_, to_): #iterate over diff\n",
    "        # print(x, in_change)\n",
    "        \n",
    "        if x[0] == \"+\" or x[0] == \"-\":  # change detected\n",
    "            sign, entry = x.split(\" \")\n",
    "\n",
    "            # new change? then init addition to changes\n",
    "            if in_change == False:\n",
    "                changes.append([[], []])\n",
    "            \n",
    "            # regardless, record state\n",
    "            in_change = True\n",
    "\n",
    "            # regardless, capture changes\n",
    "            if sign == '-':\n",
    "                changes[-1][0].append(entry)\n",
    "            elif sign == '+':\n",
    "                changes[-1][1].append(entry)\n",
    "\n",
    "        elif x[0] == \"?\":\n",
    "            pass\n",
    "            \n",
    "        else:  # match detected\n",
    "                \n",
    "            # regardless, record state    \n",
    "            in_change = False\n",
    "\n",
    "    return changes\n",
    "            \n",
    "print(get_changes(['a','b','b','c','e'], ['a','b','d','e']))\n",
    "print(get_changes([\"it\", \"oughta\", \"happen\"], [\"it\", \"ought\", \"to\", \"happen\"]))\n",
    "print(get_changes(\"it oughta happen\".split(), \"it ought to happen\".split()))\n",
    "list(difflib.ndiff([\"it\", \"oughta\", \"happen\"], [\"it\", \"ought\", \"to\", \"happen\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc5ab0-c170-48c3-a7d2-3faa6bf5498e",
   "metadata": {},
   "source": [
    "specify 'DH->D' phonological variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d58cbab-82cb-4898-a727-ed77a919b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonological_variations_LOC = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb0d6e9-9ea6-4721-adfc-588c992a19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DH_D = {\n",
    "    \"de\":[\"the\"], \n",
    "    \"dey\":[\"they\"],\n",
    "    \"dey's\":[\"they's\"],\n",
    "    \"dey'll\":[\"they'll\"],\n",
    "    \"dey'd\":[\"they'd\"],\n",
    "    \"dis\":[\"this\"],\n",
    "    \"dis'll\":[\"this'll\"],\n",
    "    \"dis'd\":[\"this'd\"],\n",
    "    \"dat\":[\"that\"],\n",
    "    \"dat's\":[\"that\"],\n",
    "    \"dat'll\":[\"that'll\"],\n",
    "    \"dat'd\":[\"that'd\"],\n",
    "    \"De\":[\"The\"], \n",
    "    \"Dey\":[\"They\"],\n",
    "    \"Dey's\":[\"They's\"],\n",
    "    \"Dey'll\":[\"They'll\"],\n",
    "    \"Dey'd\":[\"They'd\"],\n",
    "    \"Dis\":[\"This\"],\n",
    "    \"Dis'll\":[\"This'll\"],\n",
    "    \"Dis'd\":[\"This'd\"],\n",
    "    \"Dat\":[\"That\"],\n",
    "    \"Dat's\":[\"That\"],\n",
    "    \"Dat'll\":[\"That'll\"],\n",
    "    \"Dat'd\":[\"That'd\"]\n",
    "}\n",
    "\n",
    "for speakers in [[\"Negro\", \"negro\"]]:\n",
    "\n",
    "    s = \",\".join(speakers)\n",
    "    phonological_variations_LOC = {\"['DH']->['D']\":set(list(DH_D.keys()))}\n",
    "\n",
    "manually_assigned = set(list(DH_D.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a34ae-b094-41de-b67f-0b9a679ffaa0",
   "metadata": {},
   "source": [
    "identify phonological variations from annotations wrt., n-word attributed quotations in LOC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06f5d3e-d54a-451d-a12b-93a260024391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['DH']->['D']\": {\"'dat\",\n",
       "  'Dat',\n",
       "  \"Dat'd\",\n",
       "  \"Dat'll\",\n",
       "  \"Dat's\",\n",
       "  'De',\n",
       "  \"Deh's\",\n",
       "  'Dese',\n",
       "  'Dey',\n",
       "  \"Dey'd\",\n",
       "  \"Dey'll\",\n",
       "  \"Dey's\",\n",
       "  'Dis',\n",
       "  \"Dis'd\",\n",
       "  \"Dis'll\",\n",
       "  'brudder',\n",
       "  'dan',\n",
       "  'dat',\n",
       "  \"dat'd\",\n",
       "  \"dat'll\",\n",
       "  \"dat's\",\n",
       "  'dats',\n",
       "  'de',\n",
       "  'dem',\n",
       "  'dese',\n",
       "  'dey',\n",
       "  \"dey'd\",\n",
       "  \"dey'll\",\n",
       "  \"dey's\",\n",
       "  'deze',\n",
       "  'dis',\n",
       "  \"dis'd\",\n",
       "  \"dis'll\",\n",
       "  'disyeh',\n",
       "  'furder',\n",
       "  'nudder',\n",
       "  'togedder',\n",
       "  'wid'},\n",
       " \"['IH', 'K']->[]\": {\"'cept\",\n",
       "  \"'scuse\",\n",
       "  \"'specs\",\n",
       "  \"'sperience\",\n",
       "  \"'splain\",\n",
       "  \"'spress\",\n",
       "  \"'spression\",\n",
       "  \"s'peck\",\n",
       "  'spected'},\n",
       " \"['T']->[]\": {\"'sackly\",\n",
       "  \"'specs\",\n",
       "  \"Doan'\",\n",
       "  \"Jes'\",\n",
       "  'des',\n",
       "  \"didn'\",\n",
       "  'doan',\n",
       "  \"doan'\",\n",
       "  \"excep'\",\n",
       "  'ezzactly',\n",
       "  \"genalmun's\",\n",
       "  'gennulman',\n",
       "  \"hain'\",\n",
       "  'jes',\n",
       "  \"jes'\",\n",
       "  \"las'\",\n",
       "  \"lef'\",\n",
       "  'lif',\n",
       "  \"mos'ly\",\n",
       "  \"nex'\",\n",
       "  \"s'peck\",\n",
       "  \"spec'able\",\n",
       "  \"trus'\"},\n",
       " \"['G', 'EH']->['JH', 'IY']\": {'giti'},\n",
       " \"[]->['IY']\": {'Jedge', 'giti', 'jedge', 'lawdy', 'useter'},\n",
       " \"['AH']->['EH']\": {'Jedge', \"Jes'\", 'jedge', 'jes', \"jes'\", 'shet'},\n",
       " \"['EH']->['IH']\": {'Jineral',\n",
       "  'aginst',\n",
       "  \"dockymint'll\",\n",
       "  \"exceptin'\",\n",
       "  \"frien's\",\n",
       "  \"gettin'\",\n",
       "  'ginerations',\n",
       "  'keer',\n",
       "  \"keerfu'\",\n",
       "  'keerful',\n",
       "  'skeerd',\n",
       "  'watermillions',\n",
       "  'yistehday',\n",
       "  'yit'},\n",
       " \"['ER']->['IH']\": {'niggehs', 'yistehday'},\n",
       " \"['V']->['M']\": {'gim'},\n",
       " \"['AO', 'R']->['OW']\": {\"'fo\",\n",
       "  \"Afo'\",\n",
       "  \"befo'\",\n",
       "  'befoah',\n",
       "  \"co'ner\",\n",
       "  'foah',\n",
       "  'foh',\n",
       "  'roah',\n",
       "  \"sto'\",\n",
       "  'stoah'},\n",
       " \"['NG']->['N']\": {'Dancin',\n",
       "  \"Dancin'\",\n",
       "  \"Havin'\",\n",
       "  'Mawnin',\n",
       "  \"Meanin'\",\n",
       "  'Mornin',\n",
       "  \"Nuffin'\",\n",
       "  \"Thanksgivin'\",\n",
       "  \"Whoopin'\",\n",
       "  \"a-huntin'\",\n",
       "  \"a-smillin'\",\n",
       "  \"a-tryin'\",\n",
       "  \"accomodatin'\",\n",
       "  \"accusin'\",\n",
       "  \"addin'\",\n",
       "  \"anythin'\",\n",
       "  \"arollin'\",\n",
       "  \"beatin'\",\n",
       "  \"bein'\",\n",
       "  \"blowin'\",\n",
       "  \"bollin'\",\n",
       "  \"botherin'\",\n",
       "  \"catchin'\",\n",
       "  \"climbin'\",\n",
       "  \"comin'\",\n",
       "  \"cookin'\",\n",
       "  \"doin'\",\n",
       "  \"eatin'\",\n",
       "  \"eberythin'\",\n",
       "  \"exceptin'\",\n",
       "  \"feedin'\",\n",
       "  \"feelin's\",\n",
       "  \"fightin'\",\n",
       "  \"gamblin'\",\n",
       "  \"gittin'\",\n",
       "  \"handlin'\",\n",
       "  \"hidin'\",\n",
       "  \"hopin'\",\n",
       "  \"huntin'\",\n",
       "  \"killin'\",\n",
       "  \"layin'\",\n",
       "  \"livin'\",\n",
       "  \"loadin'\",\n",
       "  \"makin'\",\n",
       "  'mawnin',\n",
       "  \"mawnin'\",\n",
       "  \"mohnin'\",\n",
       "  \"mornin'\",\n",
       "  'noffin',\n",
       "  \"noffin'\",\n",
       "  \"nofin'\",\n",
       "  \"nothin'\",\n",
       "  'nuffin',\n",
       "  \"nuffin'\",\n",
       "  'nutfin',\n",
       "  \"nuthin'\",\n",
       "  \"paddlin'\",\n",
       "  \"peelin'\",\n",
       "  'playin',\n",
       "  \"preachin'\",\n",
       "  \"purloinin'\",\n",
       "  \"readin'\",\n",
       "  \"repeatin'\",\n",
       "  \"ridin'\",\n",
       "  \"roarin'\",\n",
       "  'savin',\n",
       "  \"savin'\",\n",
       "  \"sayin'\",\n",
       "  \"settlin'\",\n",
       "  \"shootin'\",\n",
       "  'showin',\n",
       "  \"showin'\",\n",
       "  \"singin'\",\n",
       "  \"skulkin'\",\n",
       "  \"sneakin'\",\n",
       "  \"sportin'\",\n",
       "  \"standin'\",\n",
       "  \"starvin'\",\n",
       "  \"stealin'\",\n",
       "  'sumfin',\n",
       "  \"summin'\",\n",
       "  \"sumpin'\",\n",
       "  \"suth'in\",\n",
       "  \"suthi'n\",\n",
       "  \"swallerin'\",\n",
       "  \"sweatin'\",\n",
       "  \"tappin'\",\n",
       "  \"tellin'\",\n",
       "  \"throwin'\",\n",
       "  \"trappin'\",\n",
       "  \"tryin'\",\n",
       "  \"waitin'\",\n",
       "  \"walkin'\",\n",
       "  \"wantin'\",\n",
       "  \"whinin'\",\n",
       "  \"writin'\",\n",
       "  \"wukkin'\"},\n",
       " \"['AE']->['AA', 'R']\": {'Marse', 'marse', 'marster'},\n",
       " \"['T', 'ER']->[]\": {'Marse', 'marse'},\n",
       " \"['W', 'IH', 'TH']->[]\": {\"'out\"},\n",
       " \"['EY']->['EH']\": {'Mebbe', 'mebby', 'teks'},\n",
       " \"['IY']->[]\": {'Mabbe', 'Mebbe', 'amphibus'},\n",
       " \"[]->['Y']\": {'watermillions'},\n",
       " \"['V']->['B']\": {'Nebber',\n",
       "  'belieb',\n",
       "  'debble',\n",
       "  'ebery',\n",
       "  'eberywhere',\n",
       "  'feber',\n",
       "  'gibs',\n",
       "  'hab',\n",
       "  'hebben',\n",
       "  'libbing',\n",
       "  'libe',\n",
       "  'lub',\n",
       "  'nebber',\n",
       "  'ober',\n",
       "  'obertook',\n",
       "  'seben',\n",
       "  'sebenty',\n",
       "  'silberware'},\n",
       " \"['B', 'IH']->[]\": {\"'cause\", \"'fo\", \"'for\", \"'fore\", 'kaz'},\n",
       " \"['AO']->['AA']\": {\"'cause\", 'kaz', \"sister-'n-law\", 'talkin'},\n",
       " \"['AE', 'F']->['EY']\": {\"a'ter\"},\n",
       " \"['JH']->['G']\": {'gemman', 'gemmens'},\n",
       " \"['N', 'T', 'AH', 'L']->[]\": {'gemman'},\n",
       " \"['OY']->['IH']\": {\"app'inted\"},\n",
       " \"['AH']->['IH']\": {\"'vited\",\n",
       "  'Jist',\n",
       "  \"app'inted\",\n",
       "  'desparation',\n",
       "  'finansully',\n",
       "  'jist',\n",
       "  'magistate',\n",
       "  'minnit',\n",
       "  'pahted',\n",
       "  'peculiah',\n",
       "  'pussented',\n",
       "  'raskil',\n",
       "  'sargint',\n",
       "  'sarpints',\n",
       "  'sartin',\n",
       "  'sich',\n",
       "  'spected',\n",
       "  'stummick',\n",
       "  'wagin',\n",
       "  'wagins'},\n",
       " \"['ER']->[]\": {\"Jeff'son\", 'diffunce', \"pow'ful\"},\n",
       " \"['TH']->['D']\": {'Dere', 'widout'},\n",
       " \"['AH']->[]\": {\"'Bout\",\n",
       "  \"'bandoned\",\n",
       "  \"'bliged\",\n",
       "  \"'bout\",\n",
       "  \"'fraid\",\n",
       "  \"'lone\",\n",
       "  \"'long\",\n",
       "  \"'low\",\n",
       "  \"'mongst\",\n",
       "  \"'pears\",\n",
       "  \"'preciate\",\n",
       "  \"bar'ls\",\n",
       "  \"chick'n\",\n",
       "  \"dar'll\",\n",
       "  \"didn'\",\n",
       "  \"dockymint'll\",\n",
       "  \"fam'ly\",\n",
       "  \"gamblin'\",\n",
       "  'nudder',\n",
       "  'nuther',\n",
       "  \"paddlin'\",\n",
       "  \"s'port\",\n",
       "  \"settlin'\",\n",
       "  \"su'tt'n'ly\",\n",
       "  'wharbouts'},\n",
       " \"['AW']->['OW']\": {\"'low\"},\n",
       " \"['OY']->['AY']\": {'biled', 'parbiled', \"sp'iled\"},\n",
       " \"['ER']->['AH']\": {\"Cunn'l\",\n",
       "  'Honah',\n",
       "  'Kunnel',\n",
       "  'Majah',\n",
       "  'Mistah',\n",
       "  'Suttinly',\n",
       "  'bettah',\n",
       "  'buhned',\n",
       "  'conductah',\n",
       "  \"doctah's\",\n",
       "  'dollah',\n",
       "  'dollahs',\n",
       "  'evah',\n",
       "  'foheveh',\n",
       "  'foreveh',\n",
       "  'fust',\n",
       "  'honah',\n",
       "  \"honah'\",\n",
       "  'kunnel',\n",
       "  'kunnels',\n",
       "  'mastah',\n",
       "  'niggah',\n",
       "  'niggahs',\n",
       "  'ownah',\n",
       "  'peculiah',\n",
       "  'propah',\n",
       "  \"su'tt'n'ly\",\n",
       "  'suh',\n",
       "  'suttenly',\n",
       "  'suttinly',\n",
       "  'teacheh',\n",
       "  \"tu'n\",\n",
       "  'wintah',\n",
       "  'wuk',\n",
       "  \"wukkin'\"},\n",
       " \"['D']->[]\": {\"'roun\",\n",
       "  \"Hol'\",\n",
       "  \"Lor'\",\n",
       "  \"behin'\",\n",
       "  'behine',\n",
       "  \"foun'\",\n",
       "  \"frien's\",\n",
       "  'groun',\n",
       "  \"han'\",\n",
       "  \"hol'\",\n",
       "  \"lan'\",\n",
       "  \"ol'\",\n",
       "  'roun',\n",
       "  \"roun'\",\n",
       "  'skinnenbones',\n",
       "  \"soun's\",\n",
       "  \"stan'\",\n",
       "  \"tol'\",\n",
       "  'un',\n",
       "  \"un'erstan'\"},\n",
       " \"['OW']->['AA']\": {'Naw', 'closter', 'naw', \"railro'd\", \"ro'd\"},\n",
       " \"['R']->[]\": {'Heah',\n",
       "  'Lawd',\n",
       "  \"Lawd's\",\n",
       "  'Lawdy',\n",
       "  'Mawnin',\n",
       "  \"Mist'ess\",\n",
       "  'cigah',\n",
       "  'hahd',\n",
       "  'heah',\n",
       "  \"hund'ed\",\n",
       "  'lawdy',\n",
       "  'magistate',\n",
       "  'mawnin',\n",
       "  \"mawnin'\",\n",
       "  'pahted',\n",
       "  'pahty'},\n",
       " \"['D']->['N']\": {'hearn'},\n",
       " \"['DH', 'EH']->['D', 'IH']\": {'dere'},\n",
       " \"['TH']->['F']\": {'Norf',\n",
       "  \"Nuffin'\",\n",
       "  'frew',\n",
       "  'fru',\n",
       "  'munf',\n",
       "  'nuffin',\n",
       "  \"nuffin'\",\n",
       "  'sumfin',\n",
       "  'trufe',\n",
       "  'wifout',\n",
       "  'worf'},\n",
       " \"['AY']->['IH']\": {\"a-smillin'\",\n",
       "  'chile',\n",
       "  \"climbin'\",\n",
       "  \"hidin'\",\n",
       "  \"ridin'\",\n",
       "  'riz',\n",
       "  \"whinin'\",\n",
       "  \"writin'\"},\n",
       " \"['DH', 'EH']->['D', 'IY']\": {\"Dere's\", \"dere's\"},\n",
       " \"['IH', 'N']->[]\": {\"'Deed\",\n",
       "  \"'kase\",\n",
       "  \"'quire\",\n",
       "  \"'stead\",\n",
       "  \"'sulted\",\n",
       "  \"'vited\",\n",
       "  'kase',\n",
       "  'kaze'},\n",
       " \"[]->['EH']\": {\"las'\"},\n",
       " \"['AE']->['EY', 'EH']\": {\"las'\"},\n",
       " \"[]->['IY', 'HH']\": {\"behin'\"},\n",
       " \"['HH', 'AY']->[]\": {\"behin'\"},\n",
       " \"['ER', 'G', 'EH']->['AA', 'JH', 'IH']\": {\"fo'git\"},\n",
       " \"['UH', 'R']->['OW']\": {\"Sho'\", 'sho', \"sho'\", \"sho'ly\", 'shoah', 'sholy'},\n",
       " \"[]->['HH']\": {'Honah',\n",
       "  \"Whoopin'\",\n",
       "  \"hain'\",\n",
       "  \"hain't\",\n",
       "  'honah',\n",
       "  \"honah'\",\n",
       "  \"whot's\",\n",
       "  'whut'},\n",
       " \"['EH']->['AA']\": {'Whar', \"Ya'ass\", 'Yassah', 'whar', 'wharbouts', 'yassah'},\n",
       " \"['IH', 'R']->['AH']\": {'yassah'},\n",
       " \"['OW']->['W']\": {'gwinei'},\n",
       " \"['NG']->['N', 'AY']\": {'gwinei'},\n",
       " \"['IY']->['Y', 'AH', 'L']\": {'Anfibulous', 'amfibulous'},\n",
       " \"['DH', 'EH']->['D', 'AA']\": {'Dars', 'dar', \"dar'll\", \"dar's\"},\n",
       " \"['HH']->[]\": {\"'er\", \"'im\", 'disyeh'},\n",
       " \"['R']->['EH']\": {'disyeh'},\n",
       " \"['AH']->['ER']\": {'canderdate', 'erway', 'perserverance', 'useter'},\n",
       " \"['OW']->['AH']\": {\"on'y\"},\n",
       " \"['L']->[]\": {'chidren', \"on'y\"},\n",
       " \"['EH', 'K']->[]\": {\"'stravigant\"},\n",
       " \"['ER']->['AA']\": {'ouahselves', 'ovah', 'rubbah', 'sah'},\n",
       " \"['AH', 'TH']->['AA', 'F']\": {'noffin', \"noffin'\"},\n",
       " \"['V']->['F']\": {'uf'},\n",
       " \"['G', 'IH', 'V', 'DH']->['JH']\": {\"gi'um\"},\n",
       " \"['IH', 'NG']->['AH', 'N']\": {\"accommodatin'\",\n",
       "  \"everlastin'\",\n",
       "  \"gettin'\",\n",
       "  \"goin'\",\n",
       "  'talkin'},\n",
       " \"['Z', 'AH']->['S']\": {'doesnt'},\n",
       " \"['ER']->['OW']\": {\"fo'ever\", \"yo'se'f\"},\n",
       " \"['EH', 'L']->['AH']\": {\"myse'f\", \"yo'se'f\"},\n",
       " \"['AE']->['EH']\": {\"arollin'\", 'ez', 'hed'},\n",
       " \"['DH']->['V']\": {'wiv'},\n",
       " \"['AA']->['AO']\": {\"swallerin'\", 'wanter', \"wantin'\"},\n",
       " \"['OW']->['ER']\": {\"swallerin'\", 'yaller'},\n",
       " \"['V', 'ER']->['B', 'AH']\": {'nebbah'},\n",
       " \"['IH']->[]\": {\"'lected\",\n",
       "  \"'leven\",\n",
       "  \"'lustration\",\n",
       "  \"'nough\",\n",
       "  \"'nuff\",\n",
       "  \"'tis\",\n",
       "  \"b'longs\",\n",
       "  'nuff'},\n",
       " \"['F']->[]\": {'ixed'},\n",
       " \"['AH', 'N']->[]\": {'twell'},\n",
       " \"['IH']->['W', 'EH']\": {'ontwell', 'twell'},\n",
       " \"['AA']->['IH']\": {'gitten'},\n",
       " \"['EH']->[]\": {'skuze'},\n",
       " \"['Y']->[]\": {'skuze'},\n",
       " \"['ER']->['AA', 'R']\": {'Sarvant', 'sarpints', 'sartin', 'unsartain'},\n",
       " \"['S']->['Z']\": {\"'kase\", \"'scuse\", 'kase', 'kaze'},\n",
       " \"['EH']->['EY']\": {'againt', 'daid', 'fe-llers'},\n",
       " \"['DH', 'AH']->['D', 'EH']\": {\"dem's\"},\n",
       " \"['V', 'R']->['B', 'ER']\": {\"eberyt'ing\", \"eberythin'\"},\n",
       " \"['TH']->['T']\": {\"T'anks\", \"eberyt'ing\", \"t'ing\", \"t'ings\", 'tink'},\n",
       " \"['HH', 'IY']->['AH', 'Y', 'AA']\": {'Look-a-yar'},\n",
       " \"['AA']->['AH']\": {'drupped', 'wuz'},\n",
       " \"['ER']->['EH']\": {'Misteh', 'oveh', 'powehful'},\n",
       " \"['EH']->['IY']\": {\"'leven\", 'evah', 'generman'},\n",
       " \"['T', 'AH', 'L']->['ER']\": {'generman', 'gennerman'},\n",
       " \"['DH', 'AE']->['EH']\": {\"'n\"},\n",
       " \"['AH']->['AA']\": {\"Afo'\", 'onless', 'ontwell', 'togedder', \"whot's\"},\n",
       " \"['ER', 'AW']->['R', 'UW']\": {\"'roun\"},\n",
       " \"['IH', 'G']->[]\": {\"'zactly\", 'zactly'},\n",
       " \"['IH']->['IY']\": {\"befo'\", 'leetle'},\n",
       " \"[]->['K']\": {'aks'},\n",
       " \"['K']->[]\": {'aks', 'ast', 'rassel'},\n",
       " \"['OW', 'DH', 'Z']->['AA', 'S']\": {\"clo's\"},\n",
       " \"['EH']->['AO']\": {\"w'ar\", 'yaller'},\n",
       " \"['AY']->['AA']\": {'mah'},\n",
       " \"['T']->['D']\": {\"stuf'd\"},\n",
       " \"['JH']->[]\": {'ust'},\n",
       " \"['IH', 'R']->['IY']\": {\"heah's\"},\n",
       " \"['S']->[]\": {'againt', \"licen'\", 'postcripts'},\n",
       " \"['UW']->['AH']\": {'Yuh', 'yuh'},\n",
       " \"['P', 'ER']->[]\": {\"'ticular\"},\n",
       " \"['EY']->['AE']\": {'Mabbe', 'Majah', 'newspapuh', 'savin', \"savin'\"},\n",
       " \"['D', 'IH']->[]\": {\"'scourage\", \"'sides\", \"'sturb\"},\n",
       " \"['ER']->['R']\": {\"'round\", \"Confed'rit\"},\n",
       " \"['L']->['R']\": {\"'Bress\", \"'bress\", 'bress'},\n",
       " \"['AO']->['AA', 'S', 'AH']\": {'ossifer', \"ossifer's\"},\n",
       " \"['IH', 'S']->[]\": {\"ossifer's\"},\n",
       " \"['AA']->['OW']\": {\"accomodatin'\", 'propah'},\n",
       " \"['IH']->['EH']\": {\"'ndeed\", \"'pears\", 'Ef', 'ef', \"sister-'n-law\"},\n",
       " \"['AE']->['AA']\": {\"bar'ls\", 'cotch', \"han'\", 'kotch', \"las'ez\"},\n",
       " \"['T', 'AE']->['EH']\": {\"las'ez\"},\n",
       " \"['OY']->['AH']\": {'disappunted'},\n",
       " \"['AH']->['AE']\": {'aginst'},\n",
       " \"['AY']->['AE']\": {'lak'},\n",
       " \"['AE']->['EY']\": {\"cain't\", 'kaint'},\n",
       " \"['ER', 'TH']->['AH', 'F']\": {'wuff'},\n",
       " \"['DH', 'OW']->['TH', 'AW', 'T']\": {'thouht'},\n",
       " \"['HH', 'IY']->['IH']\": {\"'ere\"},\n",
       " \"['EH', 'S']->['AE', 'Z']\": {'Yas', 'yas'},\n",
       " \"['OW']->[]\": {\"'possum\"},\n",
       " \"['TH']->[]\": {\"summin'\"},\n",
       " \"['EH']->['AE']\": {'Yassir', 'Yassuh', 'yassuh'},\n",
       " \"['S', 'ER']->['AH']\": {'Yassah', 'Yassuh', 'Yessuh', 'yassuh', 'yessuh'},\n",
       " \"[]->['L']\": {'priveleges'},\n",
       " \"['L', 'EH']->[]\": {'priveleges'},\n",
       " \"[]->['IH', 'Z']\": {'priveleges'},\n",
       " \"['AH', 'DH']->['UW', 'TH']\": {'nuther'},\n",
       " \"['M']->[]\": {\"suth'in\", \"suthi'n\"},\n",
       " \"['AO', 'L']->['AH']\": {\"A'mighty\"},\n",
       " \"['IH']->['AH']\": {'Virginny', 'Virginy', 'perceeded', 'sheruf'},\n",
       " \"['Y', 'AH']->['IY']\": {'Virginny', 'Virginy', \"dockymint'll\"},\n",
       " \"['R']->['ER']\": {'chidren'},\n",
       " \"['AH', 'G', 'EH']->['AA', 'JH', 'IY']\": {'agin', \"agin'\"},\n",
       " \"['AH', 'Z']->['UW', 'EH', 'S']\": {'doo-es'},\n",
       " \"['IY']->['EH']\": {'lenyent', \"readin'\"},\n",
       " \"['IY']->['Y']\": {'lenyent'},\n",
       " \"['EH']->['AH']\": {'onless'},\n",
       " \"['IH', 'R']->['ER']\": {'perserverance'},\n",
       " \"['EH', 'R']->['IY']\": {'Wheah'},\n",
       " \"['AH', 'L']->['UW']\": {\"keerfu'\"},\n",
       " \"['T', 'ER', 'Z']->['AH', 'S']\": {\"massa's\"},\n",
       " \"['AE', 'F']->['AA', 'R']\": {'arter'},\n",
       " \"['DH', 'AH', 'OW']->['D', 'AA']\": {\"d'only\"},\n",
       " \"['UH', 'R']->['UW', 'AA']\": {'shuah'},\n",
       " \"[]->['D']\": {'miled'},\n",
       " \"['EH', 'S']->['AA', 'Z']\": {'Yaas'},\n",
       " \"['UH']->['UW']\": {'tuk'},\n",
       " \"['OW']->['AW']\": {\"nowhah's\"},\n",
       " \"['IH', 'R']->['AA']\": {\"nowhah's\"},\n",
       " \"['R', 'IH']->[]\": {\"'ceipt\", \"'member\", \"'publicans\", \"spec'able\"},\n",
       " \"['M']->['N']\": {'Anfibulous'},\n",
       " \"['AO', 'R']->['AA']\": {\"fo'th\", \"hencefo'th\", 'hoss', \"mohnin'\", 'wah'},\n",
       " \"[]->['S']\": {'sint'},\n",
       " \"['Z', 'AH']->[]\": {'sint', \"wan't\"},\n",
       " \"['TH']->['P']\": {\"sumpin'\"},\n",
       " \"['EH', 'S']->['AE']\": {'Yassar', 'Yasser', 'yassar', 'yasser'},\n",
       " \"['OW', 'V']->['UW']\": {'ouertake'},\n",
       " \"['AH', 'V']->['AO', 'T']\": {'ot'},\n",
       " \"['S', 'ER']->['IH', 'R']\": {'Yassir', 'Yessir'},\n",
       " \"['R', 'IY']->['S']\": {\"'sponsible\"},\n",
       " \"['ER']->['UW']\": {'newspapuh'},\n",
       " \"['IH', 'G']->['EH']\": {'ezzactly'},\n",
       " \"[]->['AH']\": {'Atalanty', 'Youah', 'wanta'},\n",
       " \"['AH']->['IY']\": {'Atalanty', 'Tennysee'},\n",
       " \"['V', 'AH']->['B', 'IH']\": {'debbil', 'debil'},\n",
       " \"['G', 'EH']->['JH', 'IH']\": {\"gittin'\"},\n",
       " \"['F']->['T']\": {'pertect'},\n",
       " \"['EH', 'R']->['EY']\": {'skacely'},\n",
       " \"['AH']->['AW']\": {\"'nough\"},\n",
       " \"['AH', 'V']->['N']\": {\"out'n\"},\n",
       " \"['AH']->['AO']\": {\"'mongst\", 'a-haungry'},\n",
       " \"['AH', 'S']->[]\": {'ossifer'},\n",
       " \"['T', 'ER']->['AH']\": {'Massa', 'massa'},\n",
       " \"['OW', 'IH', 'NG']->['W', 'OY', 'N']\": {'a-gwoin'},\n",
       " \"['DH']->[]\": {\"'em\"},\n",
       " \"[]->['AY', 'AA']\": {'hyard'},\n",
       " \"['AY']->['EY']\": {'laik', \"w'y\"},\n",
       " \"['ER']->['UH']\": {'pusson'},\n",
       " \"['DH', 'AH']->['T']\": {\"T'other\", \"t'others\"},\n",
       " \"['AW']->['UW']\": {'roun', \"roun'\"},\n",
       " \"['AH']->['OW', 'L']\": {'olbiged'},\n",
       " \"['L', 'AY']->['IH']\": {'olbiged'},\n",
       " \"[]->['M', 'AH']\": {'gemmens'},\n",
       " \"['T', 'AH', 'L', 'M', 'IH', 'N']->['Z']\": {'gemmens'},\n",
       " \"[]->['B']\": {\"climbin'\"},\n",
       " \"['AO']->['AE']\": {'allus'},\n",
       " \"['W', 'EY', 'Z']->['AH', 'S']\": {'allus'},\n",
       " \"['AH', 'D']->['ER']\": {'canderate'},\n",
       " \"['DH', 'AH']->['T', 'IY', 'EY', 'CH']\": {\"th'\"},\n",
       " \"['W', 'EY']->['ER']\": {'allers'},\n",
       " \"['AH']->['EY']\": {'unsartain'},\n",
       " \"[]->['EY', 'V']\": {'neveh'},\n",
       " \"['V', 'ER']->[]\": {'neveh'},\n",
       " \"['T', 'AH']->[]\": {\"Cap'n\", \"cap'n\", \"li'l\"},\n",
       " \"['IH', 'NG']->['N']\": {\"sett'n\"},\n",
       " \"[]->['OW', 'N']\": {\"con'ess\"},\n",
       " \"['N', 'F', 'EH']->[]\": {\"con'ess\"},\n",
       " \"['IY', 'R']->['AY', 'AH']\": {'hyah'},\n",
       " \"['ER']->['IH', 'R']\": {'heeerd', 'heerd'},\n",
       " \"['IH', 'Z', 'AH']->['EH']\": {'ent'},\n",
       " \"['UW']->['AW', 'S']\": {'youse'},\n",
       " \"['AH', 'S', 'T']->['Z']\": {\"Forres'\"},\n",
       " \"['ER', 'EH']->['OW', 'HH', 'EY']\": {'foheveh'},\n",
       " \"['V']->['N']\": {'outen'},\n",
       " \"['EH', 'R', 'AH']->['AA']\": {\"Ca'lina\"},\n",
       " \"['AY']->['IY']\": {\"Ca'lina\"},\n",
       " \"['R', 'Z']->['S']\": {\"Deh's\"},\n",
       " \"['R', 'IY', 'Z']->['AH', 'S']\": {'pussented'},\n",
       " \"['S', 'S', 'ER']->['SH', 'UW']\": {'Yesshuh'},\n",
       " \"['OW', 'AH']->[]\": {\"G'long\"},\n",
       " \"['DH']->['F']\": {'wif'},\n",
       " \"['AH', 'G', 'EH']->['ER', 'JH', 'IH']\": {'ergin'},\n",
       " \"['S', 'T']->[]\": {'ergin'},\n",
       " \"['R', 'AH']->['IH']\": {'Mistiss', 'mistiss'},\n",
       " \"['AH', 'N']->['IH', 'NG']\": {'chicking'},\n",
       " \"['AA', 'R', 'K']->['AE', 'S']\": {'Scasly'},\n",
       " \"['EH', 'R']->['ER', 'Z']\": {'somewhars'},\n",
       " \"['F', 'AH']->[]\": {\"'cilities\"},\n",
       " \"['EH', 'V']->['IY', 'B']\": {'neber'},\n",
       " \"['ER', 'EH']->['AO', 'R']\": {'foreveh'},\n",
       " \"['AA', 'DH']->['EY', 'D']\": {'grandfader'},\n",
       " \"['AA']->['AE']\": {'drap', 'draps'},\n",
       " \"['ER', 'AY', 'V']->['AA', 'R', 'IY', 'B']\": {'arribe'},\n",
       " \"['UW', 'TH']->['AH', 'F']\": {'truf'},\n",
       " \"['AE', 'S']->['AA']\": {'aak'},\n",
       " \"['W', 'IH', 'TH']->['D']\": {\"'dout\"},\n",
       " \"['R', 'AH']->['AA']\": {\"f'ont\"},\n",
       " \"['T', 'AH', 'L']->[]\": {\"gen'man\"},\n",
       " \"['EH', 'V', 'ER']->[]\": {'ebry'},\n",
       " \"[]->['B', 'R', 'IY']\": {'ebry'},\n",
       " \"[]->['G']\": {\"'mongst\"},\n",
       " \"['OW', 'IH', 'NG']->['W', 'AY', 'N']\": {'gwine'},\n",
       " \"['DH', 'AE']->['AH']\": {\"more'n\"},\n",
       " \"['D']->['IY']\": {'chile'},\n",
       " \"['IY', 'R']->['AY', 'ER']\": {'hyar'},\n",
       " \"['HH', 'AE', 'V']->['ER']\": {'mighter'},\n",
       " \"['OY']->['AA']\": {\"bollin'\"},\n",
       " \"['SH']->['S']\": {'finansully'},\n",
       " \"['ER']->['Y', 'UW']\": {'buhd'},\n",
       " \"['SH', 'AH']->['CH', 'IY']\": {'Chee-cargo'},\n",
       " \"[]->['R']\": {'Chee-cargo'},\n",
       " \"['AH', 'DH']->['OW', 'D']\": {'oder'},\n",
       " \"['T', 'AH']->['ER']\": {'gotter', 'wanter'},\n",
       " \"['K']->['T']\": {'asts'},\n",
       " \"['AW', 'TH']->['OW', 'F']\": {'mouf'},\n",
       " \"['IH', 'G', 'Z']->['S']\": {\"'sackly\"},\n",
       " \"[]->['T']\": {'closter'},\n",
       " \"['UH']->['OW']\": {'Goed'},\n",
       " \"['AY']->['AW']\": {'mout'},\n",
       " \"['ER']->['T']\": {'outseffs'},\n",
       " \"['L', 'V', 'Z']->['F', 'S']\": {'outseffs'},\n",
       " \"['SH', 'UH', 'R']->['S', 'UW', 'AA']\": {'suah'},\n",
       " \"['S', 'ER']->['AH', 'B']\": {'Yessub'},\n",
       " \"['EH', 'S', 'T']->['IY', 'Z']\": {\"bes'\"},\n",
       " \"['AH', 'TH']->['OW', 'F']\": {\"nofin'\"},\n",
       " \"['CH']->['K']\": {'nacherally'},\n",
       " \"['D', 'IY']->['JH']\": {'Injuns'},\n",
       " \"['TH']->['T', 'F']\": {'nutfin'},\n",
       " \"['EH', 'Z']->['UW']\": {'frew'},\n",
       " \"[]->['Z']\": {'Dars'},\n",
       " \"['AE']->['AH']\": {'skinnenbones'},\n",
       " \"['R', 'IH']->['ER']\": {'perceeded'},\n",
       " \"['JH', 'AH']->['D', 'EH']\": {'des'},\n",
       " \"['UW', 'IH', 'NG']->['OY', 'N']\": {\"doin's\"},\n",
       " \"['AY']->['AO']\": {'mought'},\n",
       " \"['EH', 'R']->['ER']\": {'somewhar'},\n",
       " \"['AH', 'N']->['T']\": {'happed'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phonological_variations_LOC = {}\n",
    "\n",
    "# ------\n",
    "# get a dict of words by phonological variation: i.e., variation2words['man'][\"['NG']->['N']\"] = [\"comin'\", ...]\n",
    "# ------\n",
    "with open(f\"RQ3_ood_words/RQ3_ood_words.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "for non_canonical, corrections in annotations.items():\n",
    "    if len(corrections) == 1 and non_canonical not in manually_assigned:\n",
    "        canonical = corrections[0]\n",
    "\n",
    "        for from_, to_ in get_changes(re.findall(r\"\\w+\", arpaguess(canonical)[0]), re.findall(r\"\\w+\", arpaguess(non_canonical)[0])):\n",
    "            # capture instances\n",
    "            if f\"{from_}->{to_}\" not in phonological_variations_LOC:\n",
    "                phonological_variations_LOC[f\"{from_}->{to_}\"] = set()\n",
    "            phonological_variations_LOC[f\"{from_}->{to_}\"].add(non_canonical)\n",
    "\n",
    "phonological_variations_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53484ec-1521-46ea-9287-e543ae3c7240",
   "metadata": {},
   "source": [
    "## build quotations corrected of variations (targeting each variation separately)\n",
    "\n",
    "thus, e.g., if a quote has 2 targetted variations, then there'll be 2 instnaces of a corrected quotations, each noted against the variation in question\n",
    "\n",
    "Note: once built, chain chains...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30858781-ed32-4ac1-aa93-cb18779fd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_quotes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493a4ae-2249-4018-86df-1ab0b7d6132a",
   "metadata": {},
   "source": [
    "get corrected quotations wrt., dialect_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a9da118-cd3e-42bd-a320-a7346ed43101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That water is dealt, Dat's is good\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'because I say so!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"don't do it\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'and why do you think that?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I am going'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def word_swaps(words:list[str], replacements:dict[str,str]):\n",
    "    swaps = [(word, replacements[word]) for word in words if word in replacements.keys()]\n",
    "    return swaps\n",
    "\n",
    "def replace_strings(quote:str, replacements:dict[str,str]):\n",
    "\n",
    "    # identify what is to be swapped in order\n",
    "    words = get_words(quote)\n",
    "    swaps = deque(word_swaps(words, replacements)) # state\n",
    "\n",
    "    # for every word (in quote), associate it with its quote.find index\n",
    "    word_and_in_text_index = []\n",
    "    start = 0\n",
    "    for word in words:\n",
    "        quote_index = quote.find(word, start)\n",
    "        word_and_in_text_index.append((word, quote_index))\n",
    "        start = quote_index + len(word)\n",
    "    word_and_in_text_index = deque(word_and_in_text_index)\n",
    "\n",
    "    # build the new quote\n",
    "    new_quote:str = \"\"\n",
    "    start = 0\n",
    "    while len(swaps) > 0:\n",
    "        old_word, new_word = swaps.popleft()\n",
    "\n",
    "        while True:\n",
    "            word, quote_index = word_and_in_text_index.popleft()\n",
    "\n",
    "            if old_word == word:\n",
    "                new_quote += quote[start:quote_index] + new_word\n",
    "                start = quote_index + len(old_word)\n",
    "                break      \n",
    "\n",
    "    new_quote += quote[start:]\n",
    "                \n",
    "    return new_quote\n",
    "        \n",
    "display(replace_strings(\"Dat water is dealt, Dat's _is_ good\", {\"Dat\":\"That\", \"_is_\":\"is\"}))\n",
    "display(replace_strings(\"'cause I say so!\", {\"'cause\":\"because\"}))\n",
    "display(replace_strings(\"don' do it\", {\"don'\":\"don't\"}))\n",
    "display(replace_strings(\"and _why_ do you think that?\", {\"_why_\":\"why\"}))\n",
    "display(replace_strings(\"I'se going\", {\"I'se\":\"I am\"}))\n",
    "\n",
    "\n",
    "# replacements = {'de':'the'}\n",
    "# q = PG_df.loc[13811, \"quote\"][1:-1]\n",
    "# print(q)\n",
    "# print(replace_strings(q, replacements))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0187a8e2-1c97-4903-8650-b5fc28e5d691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get corrections for italicised words\n",
    "def is_italicised(word:str)->bool:\n",
    "    if len(word) > 2:\n",
    "        if word[0] == \"_\" and word[-1] == \"_\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "targets = set(italicised_words.keys())\n",
    "\n",
    "counter = 0\n",
    "for i, tuple_ in enumerate(tuples_news):\n",
    "    \n",
    "    quote = tuple_[1][0]\n",
    "    words = get_words(quote)\n",
    "\n",
    "    targets_present = set(words).intersection(targets)\n",
    "\n",
    "    if len(targets_present) > 0:\n",
    "        counter += 1\n",
    "\n",
    "        # get dict of all corrections to be made\n",
    "        replacements = {t:t[1:-1] for t in targets_present}\n",
    "\n",
    "        # make corrections\n",
    "        corrected_quote = replace_strings(quote, replacements)\n",
    "\n",
    "        # record\n",
    "        corrected_quotes.append([i, replacements, \"italicised\", corrected_quote])\n",
    "\n",
    "targets = set(italicised_words.keys())\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16a35b5d-f3e5-4895-9341-39afdd647ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = set(dialect_words1.keys())\n",
    "\n",
    "counter = 0\n",
    "for i, tuple_ in enumerate(tuples_news):\n",
    "    \n",
    "    quote = tuple_[1][0]\n",
    "    words = get_words(quote)\n",
    "\n",
    "    targets_present = set(words).intersection(targets)\n",
    "\n",
    "    if len(targets_present) > 0:\n",
    "        counter += 1\n",
    "\n",
    "        # get dict of all corrections to be made\n",
    "        replacements = {t:dialect_words1[t] for t in targets_present}\n",
    "\n",
    "        # make corrections\n",
    "        corrected_quote = replace_strings(quote, replacements)\n",
    "\n",
    "        # record\n",
    "        corrected_quotes.append([i, replacements, \"dialect_words1\", corrected_quote])\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e7c032-4a1d-4046-9a14-ad7f8cfcafe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = set(dialect_words2.keys())\n",
    "\n",
    "counter = 0\n",
    "for i, tuple_ in enumerate(tuples_news):\n",
    "    \n",
    "    quote = tuple_[1][0]\n",
    "    words = get_words(quote)\n",
    "\n",
    "    targets_present = set(words).intersection(targets)\n",
    "\n",
    "    if len(targets_present) > 0:\n",
    "        counter += 1\n",
    "\n",
    "        # get dict of all corrections to be made\n",
    "        replacements = {t:dialect_words2[t] for t in targets_present}\n",
    "\n",
    "        # make corrections\n",
    "        corrected_quote = replace_strings(quote, replacements)\n",
    "\n",
    "        # record\n",
    "        corrected_quotes.append([i, replacements, \"dialect_words2\", corrected_quote])\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67658d5-3d49-4e46-87d5-c1a9295630dd",
   "metadata": {},
   "source": [
    "get corrected quotations wrt., regularised irregular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b894d932-db77-47cf-81e5-f8f561203671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = set(regularised_irregular.keys())\n",
    "\n",
    "counter = 0\n",
    "for i, tuple_ in enumerate(tuples_news):\n",
    "    \n",
    "    quote = tuple_[1][0]\n",
    "    words = get_words(quote)\n",
    "\n",
    "    targets_present = set(words).intersection(targets)\n",
    "\n",
    "    if len(targets_present) > 0:\n",
    "        counter += 1\n",
    "\n",
    "        # get dict of all corrections to be made\n",
    "        replacements = {t:regularised_irregular[t] for t in targets_present}\n",
    "\n",
    "        # make corrections\n",
    "        corrected_quote = replace_strings(quote, replacements)\n",
    "\n",
    "        # record\n",
    "        corrected_quotes.append([i, replacements, \"regularised_irregular\", corrected_quote])\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68995070-1361-4b47-ae80-82f95e3eb939",
   "metadata": {},
   "source": [
    "get corrected quotations wrt., phonological variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3b50813-00e3-4bba-b3b4-0b20cd6c3ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 272/272 [00:03<00:00, 74.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2637"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"RQ3_ood_words/RQ3_ood_words.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "counter = 0\n",
    "for v in tqdm(phonological_variations_LOC):\n",
    "\n",
    "    targets:set = phonological_variations_LOC[v].intersection([k for k,v in annotations.items() if len(v) == 1])\n",
    "\n",
    "    for i, tuple_ in enumerate(tuples_news):\n",
    "        \n",
    "        quote = tuple_[1][0]\n",
    "        words = get_words(quote)\n",
    "    \n",
    "        targets_present = set(words).intersection(targets)\n",
    "    \n",
    "        if len(targets_present) > 0:\n",
    "            counter += 1\n",
    "    \n",
    "            # get dict of all corrections to be made\n",
    "            replacements = {t:annotations[t][0] for t in targets_present}\n",
    "    \n",
    "            # make corrections\n",
    "            corrected_quote = replace_strings(quote, replacements)\n",
    "    \n",
    "            # record\n",
    "            corrected_quotes.append([i, replacements, v, corrected_quote])\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a6811f7-43c7-4da3-930b-cb0989c7465e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[86,\n",
       "  {\"I's\": 'I am'},\n",
       "  'dialect_words1',\n",
       "  'Jist like a woman! always wants to visit in bad weather! And now I am got to sue de white folks of this train in the Federal Court for my damages and rights. I is going to do that very thing, if God spares me and I can git a lawyer!']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2750"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(corrected_quotes[:1])\n",
    "len(corrected_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c09824e-1f00-47f3-8808-4ffb512386b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gwine'}\n",
      "{\"I'se\", \"they's\", \"We'se\", \"I's\", \"dey's\", \"They's\", \"they'se\", \"i'se\", \"we's\", \"Dey's\"}\n"
     ]
    }
   ],
   "source": [
    "# out of interest: what dialect words do we see in LOC?\n",
    "print(set(chain(*[x[1].keys() for x in corrected_quotes if x[2] == \"dialect_words2\"])))\n",
    "print(set(chain(*[x[1].keys() for x in corrected_quotes if x[2] == \"dialect_words1\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c5a3f-3d95-4dc1-b960-1beb096a177f",
   "metadata": {},
   "source": [
    "save corrected quotes (commented out, so as not to overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16101a06-108f-4d92-a7c9-4b4154d2961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"RQ3_downstream/all_corrected.json\", \"w\") as f:\n",
    "#     json.dump(corrected_quotes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679e8d6-7d08-45f1-87a5-8f1d14a03e3b",
   "metadata": {},
   "source": [
    "# Load chains for calculating .... \n",
    "\n",
    "$\\bar{S}_{nword/lit}$ \n",
    "\n",
    "$\\bar{S}_{nword/lit, \\backslash v}$\n",
    "\n",
    "$\\bar{S}_{nword/news}$\n",
    "\n",
    "$\\bar{S}_{nword/news, \\backslash v}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e80b7-c132-427a-a948-5a80a1326b7a",
   "metadata": {},
   "source": [
    "load PG quotes and corresponding chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf2c85e9-92a1-4873-8eb0-1c7430bc8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../PG/extract_quotes_via_spaCy/quotes_blacklist.json', 'r') as f:\n",
    "    PG_blacklist = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b59ae0d4-769b-4c66-a449-d8df4246be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../PG/extract_quotes_via_spaCy/quotes_5Jul.json', 'r') as f:\n",
    "    PG_df = pd.DataFrame([t for i,t in enumerate(json.load(f))], columns = [\"id\", \"p\", \"quote\", \"manner\", \"speaker\"])\n",
    "PG_df['i'] = list(range(len(PG_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abdb7523-1ea3-44a8-b3a4-ef87a0b3c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore (as done for chains) ... those quotes in blacklist or not of speakers of interest\n",
    "speakers_of_interest = set(['man', 'woman', 'child', 'gentleman', 'lady', 'negro', 'Negro'])\n",
    "PG_df = PG_df.loc[(PG_df.loc[:,'i'].isin(PG_blacklist)==False)&(PG_df.loc[:,'speaker'].isin(speakers_of_interest)==True),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49d96db-a3a1-4871-8138-768ac32f9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset indices, reset 'i'\n",
    "PG_df.reset_index(drop=True, inplace=True)\n",
    "PG_df['i'] = list(range(len(PG_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "364c2c50-2c2a-4a10-b8d8-be9de04f6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  9.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26502"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains_dir = pathlib.Path('../../PG/Snellius/mwcgln/llama3.1_70B/')\n",
    "\n",
    "# get orders list of chains fps - they need to be re-assembled in this order, to correspond to \n",
    "ordered_chains_fps = sorted(list(chains_dir.glob('*.json')), key=lambda c: int(re.search(r\"_(\\d+)\\.json\", str(c)).groups()[0]))\n",
    "\n",
    "# re-assemble\n",
    "PG_chains = []\n",
    "for chain_fp in tqdm(ordered_chains_fps):\n",
    "    with open(chain_fp, 'r') as f:\n",
    "        PG_chains += json.load(f)\n",
    "len(PG_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1f5e6a5-7b53-4be7-9576-b2f520a9e1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[399,\n",
       " 1199,\n",
       " 3217,\n",
       " 3880,\n",
       " 3882,\n",
       " 3995,\n",
       " 3997,\n",
       " 4401,\n",
       " 5371,\n",
       " 5878,\n",
       " 5881,\n",
       " 5884,\n",
       " 5885,\n",
       " 6795,\n",
       " 7345,\n",
       " 7347,\n",
       " 7349,\n",
       " 7350,\n",
       " 7352,\n",
       " 7353,\n",
       " 7358,\n",
       " 7362,\n",
       " 7364,\n",
       " 7765,\n",
       " 8793,\n",
       " 8803,\n",
       " 9064,\n",
       " 10471,\n",
       " 12156,\n",
       " 12194,\n",
       " 12670,\n",
       " 13484,\n",
       " 13525,\n",
       " 14547,\n",
       " 15114,\n",
       " 16441,\n",
       " 16461,\n",
       " 17035,\n",
       " 17101,\n",
       " 17106,\n",
       " 18217,\n",
       " 18809,\n",
       " 19027,\n",
       " 19129,\n",
       " 19603,\n",
       " 20159,\n",
       " 20164,\n",
       " 20467,\n",
       " 21675,\n",
       " 21676,\n",
       " 22704,\n",
       " 22771,\n",
       " 23625,\n",
       " 24905,\n",
       " 25744,\n",
       " 25756,\n",
       " 25765,\n",
       " 26422]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'man': 39, 'lady': 6, 'negro': 6, 'woman': 5, 'gentleman': 2})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_encoding_i = []\n",
    "bad_encoding_speakers = Counter()\n",
    "for i, (q, speaker) in enumerate(zip(PG_df.loc[:,'quote'], PG_df.loc[:,'speaker'])):\n",
    "    try:\n",
    "        s = q[1:-1]\n",
    "        words = get_words(s)\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        words_indices = get_words_indices(words, tokens)\n",
    "\n",
    "        assert len(words) == len(words_indices)\n",
    "    except:\n",
    "        bad_encoding_i.append(i)\n",
    "        bad_encoding_speakers[speaker] += 1\n",
    "\n",
    "display(bad_encoding_i)\n",
    "display(bad_encoding_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35e7ae92-a4ae-4dd0-8c23-df9c0e4820b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop from quotes\n",
    "PG_df.drop(bad_encoding_i, inplace=True)\n",
    "PG_df.reset_index(drop=True, inplace=True)\n",
    "PG_df['i'] = list(range(len(PG_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7500293b-c59a-4c88-b50f-4bb782bf2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop from chains\n",
    "for i in sorted(bad_encoding_i, reverse=True):\n",
    "    del PG_chains[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cadb7d9f-225c-431d-890c-7526238dbb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26444"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "26444"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(PG_df))\n",
    "display(len(PG_chains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac993758-f416-4b48-adcd-0cb34cd541dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_surprisals(words, tokens, chain):\n",
    "\n",
    "    variation_surprisal = []\n",
    "    \n",
    "    for word, word_indices in zip(words, get_words_indices(words, tokens)):\n",
    "        \n",
    "        # are word_indices a decimal? then we need to apply a power to the probability, to split a token that spans words\n",
    "        powers, indices = zip(*[math.modf(i) for i in word_indices])\n",
    "        powers = np.array([1 if p == 0 else p for p in powers])\n",
    "        indices = [int(i) for i in indices]\n",
    "\n",
    "        # log(x^p) = p*log(x)\n",
    "        word_surprisal = (-np.log(chain[indices])*powers).sum()\n",
    "        variation_surprisal.append(word_surprisal)\n",
    "\n",
    "    return variation_surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5458c538-6690-4cec-aeae-be52012c9190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 26444/26444 [00:00<00:00, 111153.93it/s]\n"
     ]
    }
   ],
   "source": [
    "PG_df[\"words\"] = PG_df[\"quote\"].progress_apply(lambda x: get_words(x[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5debbff8-5d1f-4faa-91fd-66c758c7db78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 26444/26444 [00:02<00:00, 13106.07it/s]\n"
     ]
    }
   ],
   "source": [
    "PG_df['surprisals'] = [get_surprisals(PG_df.loc[i, 'words'], tokenizer.tokenize(PG_df.loc[i, 'quote'][1:-1]), np.array(PG_chains[i][1:])) for i in tqdm(range(len(PG_df)))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e598f4-9bca-4371-9e47-a97274f0a0a9",
   "metadata": {},
   "source": [
    "load PG corrected quotes and corresponding chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06642442-a4cb-4297-9fc9-a77725859f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8653"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"RQ1_downstream/all_corrected.json\", \"r\") as f:\n",
    "    corrected_quotes_PG = json.load(f)\n",
    "len(corrected_quotes_PG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45b71c91-9954-4f8f-8a81-9d560ee8b4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8653"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"RQ1_downstream/chains_llama3.1_70B_all_corrected.json\", \"r\") as f:\n",
    "    chains_corrected_PG = json.load(f)\n",
    "len(chains_corrected_PG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d28309-1959-40c7-96af-cfa86d12a011",
   "metadata": {},
   "source": [
    "load LOC quotes and corresponding chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7d47f7f-60dc-4b8d-b085-3c7de0e98d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../LOC/tuples_news.json\", \"r\") as f:\n",
    "    tuples_news = json.load(f)\n",
    "len(tuples_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "113c751d-809c-4550-8ff2-f886a310cb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../LOC/chains_llama3.1_news.json\", \"r\") as f:\n",
    "    chains_LOC = json.load(f)\n",
    "len(chains_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac76bc9-291f-44a4-a6ca-920997969100",
   "metadata": {},
   "source": [
    "load LOC corrected quotes and corresponding chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3707c35a-977f-4f48-930b-403bbc07d75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2750"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"RQ3_downstream/all_corrected.json\", \"r\") as f:\n",
    "    corrected_quotes_LOC = json.load(f)\n",
    "len(corrected_quotes_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4e21c93-3b93-48fc-974c-895a657104ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2750"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"RQ3_downstream/chains_llama3.1_70B_all_corrected.json\", \"r\") as f:\n",
    "    chains_corrected_LOC = json.load(f)\n",
    "len(chains_corrected_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b353ee-d0af-4d16-a0a0-fad6f89c7a19",
   "metadata": {},
   "source": [
    "## Deviation between observed mean LLM surprisal over words between n-word quotes of PG and LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b12f0-3fb1-4129-acbd-66399a17fc7c",
   "metadata": {},
   "source": [
    "calculate x_bar_PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c50d6d0-e11f-42f8-819f-5c288fa55150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.267566609622147"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bar_PG = np.array(list(chain(*3PG_df.loc[PG_df.loc[:,'speaker'].isin([\"Negro\", \"negro\"]), 'surprisals']))).mean()\n",
    "x_bar_PG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31abae-0d76-4f2a-a680-77dda5b22b97",
   "metadata": {},
   "source": [
    "### calculate x_bar_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "035bbe27-8611-4813-8bd3-ee378185ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add word surprisals to tuples_news\n",
    "surprisals = []\n",
    "for i, tuple_ in enumerate(tuples_news):\n",
    "    c = np.array(chains_LOC[i][1:])\n",
    "    q = tuple_[1][0]\n",
    "    words = get_words(q)\n",
    "    tokens = tokenizer.tokenize(sq)\n",
    "    tuple_[1] += [list(get_surprisals(words, tokens, c))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bca9fb7-4d32-44f9-81c2-505ed82c6beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.373269680636792"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bar_LOC = np.array(list(chain(*[tuple_[1][-1] for tuple_ in tuples_news]))).mean()\n",
    "x_bar_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5cc355-dfa4-4bc8-b0ca-8d78406a2bf0",
   "metadata": {},
   "source": [
    "calc. $(\\bar{S}_{nword, lit} - \\bar{S}_{nword, news}) / \\bar{S}_{nword, news} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bf63389-7be4-40d0-952e-cef4143aabd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.164800188903053"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(x_bar_PG - x_bar_LOC) / x_bar_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3c479-a84d-4f03-8fb9-d54a8a1efdbe",
   "metadata": {},
   "source": [
    "## Deviation between latent mean LLM surprisal over words between n-word quotes of PG and LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eeb61e23-21a7-4076-89f1-ffc30261dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdi(d: np.ndarray, hdi=0.89)->tuple[float]:\n",
    "    \"\"\" Return (lowerbound::float, upperbound::float) wrt.,\n",
    "        prescribed highest density interval\n",
    "    \"\"\"\n",
    "    lb = (1-hdi)/2\n",
    "    ub = hdi+lb\n",
    "    return (np.quantile(d, lb), np.quantile(d, ub))\n",
    "\n",
    "def bayesian_bootstrapping(data, a=3, target_accept=0.95):\n",
    "    with pm.Model() as model:\n",
    "\n",
    "        w = pm.Dirichlet(\"w\", a=np.ones(len(data))*a)\n",
    "        \n",
    "        mean = pm.Deterministic(\"mean\", pm.math.sum(w * data))\n",
    "\n",
    "        ## using NUTS sampler\n",
    "        trace = pm.sample(target_accept=target_accept)\n",
    "        \n",
    "    return model, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d6230-011a-4f81-8f1e-dc7da777446e",
   "metadata": {},
   "source": [
    "### estimate the $\\bar{Z}_{n-word, literature}$ wrt., PG & n-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "953a44ae-8a6a-40e2-81d9-093014de4648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15.167998199577706, 15.343414115553507)\n"
     ]
    }
   ],
   "source": [
    "s = \",\".join([\"Negro\", \"negro\"])\n",
    "trace_PG = az.from_netcdf(f'RQ1_population_samples/TRACE_{s}.nc')\n",
    "posterior_samples_PG = az.extract(trace_PG, var_names=['mean'], combined=True)\n",
    "print(get_hdi(posterior_samples_PG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f14b4e-5116-463e-98ff-71ec29979b7b",
   "metadata": {},
   "source": [
    "### estimate the $\\bar{Z}_{n-word, news}$ wrt., LOC & n-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "164d6bf5-6635-4d26-a4e2-8949867685ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample the data (commented out now data has been sampled)\n",
    "# data = np.array(list(chain(*[tuple_[1][-1] for tuple_ in tuples_news])))\n",
    "# sample_data = list(np.random.choice(data, size=10000))\n",
    "# with open(\"RQ3_population_samples/sample_10000.json\", \"w\") as f:\n",
    "#     json.dump(sample_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0df00f4f-c4fc-437e-a3a9-6ef5bbd1382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13.400298910739892, 13.541473622224425)\n"
     ]
    }
   ],
   "source": [
    "trace_LOC = az.from_netcdf('RQ3_population_samples/TRACE.nc')\n",
    "posterior_samples_LOC = az.extract(trace_LOC, var_names=['mean'], combined=True)\n",
    "print(get_hdi(posterior_samples_LOC))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68887fe-362d-471f-ab15-8f84f324ab1d",
   "metadata": {},
   "source": [
    "### estimate $(\\bar{Z}_{nword, lit} - \\bar{Z}_{nword, news}) / \\bar{Z}_{nword, news}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b92ccbd-04f7-4b33-97a5-0f8a3cdbcc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.823071442419211, 14.724926470554383)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = 100*(posterior_samples_PG - posterior_samples_LOC) / posterior_samples_LOC\n",
    "get_hdi(pc, hdi=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd77c23-a6fc-4b46-b012-ab719d4f50b3",
   "metadata": {},
   "source": [
    "# calculate $C(\\bar{S}_{\\text{nword}}, v) - C(\\bar{S}_{\\text{normative reference speaker}}, v)$\n",
    "\n",
    "consider all phonological variations from PG, from news, and manually specified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed2d58-82d9-4065-b7fc-c0fe41c492f9",
   "metadata": {},
   "source": [
    "### get phonological variations in PG n-word quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ea4555b-007d-4756-be5f-53b558ce7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonological_variations_PG = {}\n",
    "\n",
    "DH_D = {\n",
    "    \"de\":[\"the\"], \n",
    "    \"dey\":[\"they\"],\n",
    "    \"dey's\":[\"they's\"],\n",
    "    \"dey'll\":[\"they'll\"],\n",
    "    \"dey'd\":[\"they'd\"],\n",
    "    \"dis\":[\"this\"],\n",
    "    \"dis'll\":[\"this'll\"],\n",
    "    \"dis'd\":[\"this'd\"],\n",
    "    \"dat\":[\"that\"],\n",
    "    \"dat's\":[\"that\"],\n",
    "    \"dat'll\":[\"that'll\"],\n",
    "    \"dat'd\":[\"that'd\"],\n",
    "    \"De\":[\"The\"], \n",
    "    \"Dey\":[\"They\"],\n",
    "    \"Dey's\":[\"They's\"],\n",
    "    \"Dey'll\":[\"They'll\"],\n",
    "    \"Dey'd\":[\"They'd\"],\n",
    "    \"Dis\":[\"This\"],\n",
    "    \"Dis'll\":[\"This'll\"],\n",
    "    \"Dis'd\":[\"This'd\"],\n",
    "    \"Dat\":[\"That\"],\n",
    "    \"Dat's\":[\"That\"],\n",
    "    \"Dat'll\":[\"That'll\"],\n",
    "    \"Dat'd\":[\"That'd\"]\n",
    "}\n",
    "\n",
    "for speakers in [[\"Negro\", \"negro\"]]:\n",
    "\n",
    "    s = \",\".join(speakers)\n",
    "    phonological_variations_PG = {\"['DH']->['D']\":set(list(DH_D.keys()))}\n",
    "\n",
    "manually_assigned = set(list(DH_D.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa892cba-fa84-4cd6-a21a-7fcf10849c71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['DH']->['D']\": {'Brudder',\n",
       "  'Dat',\n",
       "  \"Dat'd\",\n",
       "  \"Dat'll\",\n",
       "  \"Dat's\",\n",
       "  'Dats',\n",
       "  'De',\n",
       "  'Dem',\n",
       "  'Dey',\n",
       "  \"Dey'd\",\n",
       "  \"Dey'll\",\n",
       "  \"Dey's\",\n",
       "  'Dis',\n",
       "  \"Dis'd\",\n",
       "  \"Dis'll\",\n",
       "  'anudder',\n",
       "  'brudder',\n",
       "  'dan',\n",
       "  'dass',\n",
       "  'dat',\n",
       "  \"dat'd\",\n",
       "  \"dat'll\",\n",
       "  \"dat's\",\n",
       "  'dat-what',\n",
       "  'de',\n",
       "  'dem',\n",
       "  'dese',\n",
       "  'dey',\n",
       "  \"dey'd\",\n",
       "  \"dey'll\",\n",
       "  \"dey's\",\n",
       "  \"dey'se\",\n",
       "  \"deyse'f\",\n",
       "  'deyselves',\n",
       "  'deze',\n",
       "  'dis',\n",
       "  \"dis'd\",\n",
       "  \"dis'll\",\n",
       "  'fedders',\n",
       "  'furder',\n",
       "  'nudder',\n",
       "  'togedder',\n",
       "  'wid'},\n",
       " \"['OW']->['AA']\": {\"don'\", 'fokses', 'naw', 'totin'},\n",
       " \"['T']->[]\": {\"'spec\",\n",
       "  \"'speck\",\n",
       "  \"Baptis'\",\n",
       "  \"Cap'en\",\n",
       "  \"Fus'\",\n",
       "  \"Jes'\",\n",
       "  'Marser',\n",
       "  \"Mas'r\",\n",
       "  'Masser',\n",
       "  \"ain'\",\n",
       "  \"cap'en\",\n",
       "  \"couldn'\",\n",
       "  'dass',\n",
       "  'des',\n",
       "  \"des'\",\n",
       "  'dess',\n",
       "  \"don'\",\n",
       "  \"don'e\",\n",
       "  \"expec'\",\n",
       "  'fack',\n",
       "  \"greates'\",\n",
       "  \"had'n\",\n",
       "  'inturrup',\n",
       "  'jes',\n",
       "  \"jes'\",\n",
       "  \"jus'\",\n",
       "  \"las'\",\n",
       "  'marser',\n",
       "  \"marser's\",\n",
       "  \"mas'r\",\n",
       "  'masser',\n",
       "  \"mos'\",\n",
       "  \"mus'\",\n",
       "  'nex',\n",
       "  \"nex'\",\n",
       "  'speckin',\n",
       "  'spek',\n",
       "  'widgable'},\n",
       " \"['AE', 'S', 'T', 'ER']->['AA', 'R', 'Z']\": {'mars', \"mars'\"},\n",
       " \"['AH', 'V']->['AA', 'B']\": {'Ob'},\n",
       " \"['AY', 'V']->['IH', 'B']\": {'lib'},\n",
       " \"['AO', 'R']->['ER']\": {'fer', 'ur'},\n",
       " \"['OW']->['ER']\": {'feller', 'fellers', 'medder', 'minnar'},\n",
       " \"['AH']->['ER']\": {'erbout',\n",
       "  'erway',\n",
       "  'gwineter',\n",
       "  'innercent',\n",
       "  'ter-morrer',\n",
       "  'ter-night'},\n",
       " \"['UW']->['ER']\": {'ter'},\n",
       " \"['EH', 'S', 'T']->['EY', 'Z']\": {\"res'\"},\n",
       " \"['D']->[]\": {\"'an\",\n",
       "  \"Hol'\",\n",
       "  'Lor',\n",
       "  \"Lor'\",\n",
       "  \"McDonal'\",\n",
       "  'behine',\n",
       "  'fren',\n",
       "  \"lan's\",\n",
       "  \"ol'\",\n",
       "  'ole',\n",
       "  'roun',\n",
       "  \"roun'\",\n",
       "  \"tol'\",\n",
       "  'un',\n",
       "  \"un'er\",\n",
       "  \"und'stan\",\n",
       "  \"und'stan'\",\n",
       "  \"won'erful\",\n",
       "  \"worl'\",\n",
       "  'wurrell'},\n",
       " \"['AO', 'L']->[]\": {\"mos'\"},\n",
       " \"['AO', 'R']->['OW']\": {\"Afo'\", \"afo'\", \"fo'\", 'foh'},\n",
       " \"['TH']->['F']\": {'bofe',\n",
       "  'def',\n",
       "  'forefwif',\n",
       "  'fousand',\n",
       "  'frew',\n",
       "  'froo',\n",
       "  'froos',\n",
       "  'fru',\n",
       "  'nuffin',\n",
       "  'sumfing',\n",
       "  'troof'},\n",
       " \"['AA']->['AO']\": {'Gawd', 'borry', 'ter-morrer'},\n",
       " \"['NG']->['N']\": {\"Evenin'\",\n",
       "  \"Mawnin'\",\n",
       "  \"Noffin'\",\n",
       "  \"Nothin'\",\n",
       "  \"a-burnin'\",\n",
       "  \"a-rushin'\",\n",
       "  \"a-wearin'\",\n",
       "  \"bad-lookin'\",\n",
       "  \"bitin'\",\n",
       "  \"breakin'\",\n",
       "  \"bumpin'\",\n",
       "  \"bustin'\",\n",
       "  \"camp-meetin'\",\n",
       "  'ceptin',\n",
       "  \"comin'\",\n",
       "  'creepin',\n",
       "  'cummin',\n",
       "  \"cussin'\",\n",
       "  \"doin'\",\n",
       "  \"drinkin'\",\n",
       "  'ebenin',\n",
       "  \"excusin'\",\n",
       "  \"fallin'\",\n",
       "  \"fishin'\",\n",
       "  \"gittin'\",\n",
       "  \"groanin'\",\n",
       "  'gwyn',\n",
       "  \"habin'\",\n",
       "  \"hittin'\",\n",
       "  \"holdin'\",\n",
       "  \"int'ruptin'\",\n",
       "  \"keepin'\",\n",
       "  \"knowin'\",\n",
       "  \"lightnin'\",\n",
       "  'lookin',\n",
       "  \"lookin'\",\n",
       "  \"losin'\",\n",
       "  \"makin'\",\n",
       "  \"marchin'\",\n",
       "  \"marryin'\",\n",
       "  \"mawnin'\",\n",
       "  'mornin',\n",
       "  \"mornin'\",\n",
       "  'noffin',\n",
       "  \"nothin'\",\n",
       "  'nuffin',\n",
       "  \"nussin'\",\n",
       "  \"nuthin'\",\n",
       "  'nuttin',\n",
       "  \"nuttin'\",\n",
       "  \"passin'\",\n",
       "  'perishin',\n",
       "  \"puttin'\",\n",
       "  \"ridin'\",\n",
       "  \"sayin'\",\n",
       "  \"scattarin'\",\n",
       "  \"shootin'\",\n",
       "  \"sinkin'\",\n",
       "  \"snappin'\",\n",
       "  'speckin',\n",
       "  \"standin'\",\n",
       "  \"steppin'\",\n",
       "  \"takin'\",\n",
       "  \"talkin'\",\n",
       "  \"tellin'\",\n",
       "  'totin',\n",
       "  \"tryin'\",\n",
       "  \"tyin'\",\n",
       "  \"waitin'\",\n",
       "  \"wakin'\",\n",
       "  'wavin',\n",
       "  \"weddin'\",\n",
       "  'whinin',\n",
       "  \"willin'\",\n",
       "  'wonderin',\n",
       "  \"worryin'\"},\n",
       " \"['V']->['W']\": {'conwicted', 'wase', 'werry', 'wessel', 'wishious'},\n",
       " \"['AO', 'R']->['AA']\": {'hoss', 'hossreddish', 'quatah', \"sho'tly\"},\n",
       " \"['ER']->['AH']\": {'Cunnel',\n",
       "  \"Fus'\",\n",
       "  'Fust',\n",
       "  'Kunnel',\n",
       "  \"Maje'\",\n",
       "  'Mistah',\n",
       "  'Sutny',\n",
       "  'Suttinly',\n",
       "  \"chu'ch-house\",\n",
       "  'cullud',\n",
       "  'fust',\n",
       "  'fust-rate',\n",
       "  \"nussin'\",\n",
       "  'quatah',\n",
       "  \"retu'n\",\n",
       "  'suh',\n",
       "  \"sut'n'y\",\n",
       "  'sutney',\n",
       "  'sutny',\n",
       "  'wusser'},\n",
       " \"['UH']->['UW']\": {'tuk'},\n",
       " \"['TH']->['T']\": {\"anyt'ing\",\n",
       "  'anyting',\n",
       "  'eberyting',\n",
       "  'fortwid',\n",
       "  'nuttin',\n",
       "  \"nuttin'\",\n",
       "  \"pant'er\",\n",
       "  \"pant'ers\",\n",
       "  'someting',\n",
       "  \"t'ing\",\n",
       "  \"t'ink\",\n",
       "  \"t'inks\",\n",
       "  \"t'rough\",\n",
       "  'tief',\n",
       "  'tink',\n",
       "  'tinks',\n",
       "  'troo'},\n",
       " \"['UW']->['AO', 'F']\": {\"t'rough\"},\n",
       " \"['EH']->[]\": {'kommissery', 'prepars', 'skuze'},\n",
       " \"['Y']->[]\": {\"'ee\", 'butifullest', 'skuze'},\n",
       " \"['IH', 'K']->[]\": {\"'cept\",\n",
       "  \"'spec\",\n",
       "  \"'speck\",\n",
       "  'scuse',\n",
       "  'speckin',\n",
       "  'speckted',\n",
       "  'spect',\n",
       "  'spected',\n",
       "  'spek'},\n",
       " \"['AH']->['IH']\": {\"'jected\",\n",
       "  'Sarvint',\n",
       "  \"cha'ges\",\n",
       "  'chawnces',\n",
       "  'hallibooloos',\n",
       "  'jestice',\n",
       "  'jist',\n",
       "  'kommissery',\n",
       "  'minnit',\n",
       "  'sability',\n",
       "  'sich',\n",
       "  'speckted',\n",
       "  'spected',\n",
       "  'stummic',\n",
       "  'stummik'},\n",
       " \"['IH']->['AY']\": {'childen'},\n",
       " \"['R']->[]\": {'Lawd',\n",
       "  \"Lawd's\",\n",
       "  'Lawd-a-massy',\n",
       "  \"Mawnin'\",\n",
       "  'apaht',\n",
       "  'bawn',\n",
       "  'childen',\n",
       "  'fah',\n",
       "  'heah',\n",
       "  \"mawnin'\",\n",
       "  'pahdon',\n",
       "  \"tyin'\"},\n",
       " \"['ER']->['AA', 'R']\": {'Sartainly',\n",
       "  'Sarvint',\n",
       "  \"l'arn\",\n",
       "  'larn',\n",
       "  'obsarve',\n",
       "  'sar',\n",
       "  'sartain'},\n",
       " \"['D']->['JH']\": {'tremenjous'},\n",
       " \"['EH']->['AA']\": {'Whar', 'Yassah', 'fotched', 'yassah'},\n",
       " \"['IH', 'R']->['AH']\": {'yassah', 'yassuh'},\n",
       " \"['AH']->['AA']\": {\"'oman\", \"Afo'\", \"afo'\", 'ontwel', 'togedder'},\n",
       " \"['IH']->['W', 'EH']\": {'ontwel', 'twell'},\n",
       " \"['AW', 'TH']->['OW', 'F']\": {'mouf'},\n",
       " \"['IH']->['EH']\": {\"'Pears\",\n",
       "  \"'pear\",\n",
       "  'Ef',\n",
       "  'deliber',\n",
       "  'deserb',\n",
       "  'ef',\n",
       "  \"expec'\",\n",
       "  'sence',\n",
       "  'sperrits'},\n",
       " \"['ER']->['AA']\": {'offah', 'ovah', 'sah'},\n",
       " \"['AE']->['EH']\": {'hed', 'hez', 'hossreddish', 'kerrige'},\n",
       " \"['IH', 'N']->[]\": {\"'Deed\", \"'case\", \"'deed\", \"'fernal\", \"'jected\"},\n",
       " \"['AH']->['EH']\": {'Jedge',\n",
       "  \"Jes'\",\n",
       "  'genermen',\n",
       "  'jedge',\n",
       "  'jedgment',\n",
       "  'jes',\n",
       "  \"jes'\",\n",
       "  'jestice',\n",
       "  'provedense'},\n",
       " \"[]->['IY']\": {'Jedge', 'jedge', 'uv'},\n",
       " \"['AH']->[]\": {\"'Merica\",\n",
       "  \"'Pears\",\n",
       "  \"'blige\",\n",
       "  \"'bout\",\n",
       "  \"'feered\",\n",
       "  \"'long\",\n",
       "  \"'pear\",\n",
       "  \"'pinion\",\n",
       "  \"'pon\",\n",
       "  \"'preciate\",\n",
       "  \"'specially\",\n",
       "  \"'tempt\",\n",
       "  \"'way\",\n",
       "  \"Cel'bratun\",\n",
       "  'Philadelphy',\n",
       "  \"S'pose\",\n",
       "  'Sutny',\n",
       "  \"couldn'\",\n",
       "  \"dar'll\",\n",
       "  \"greates'\",\n",
       "  'nudder',\n",
       "  'pon',\n",
       "  \"s'pose\",\n",
       "  'sassinated',\n",
       "  \"sut'n'y\",\n",
       "  'sutney',\n",
       "  'sutny',\n",
       "  \"wudn't\"},\n",
       " \"['OW', 'TH']->['AA', 'F']\": {'bof'},\n",
       " \"['IH', 'Z']->['EH', 'S']\": {\"'s\"},\n",
       " \"['V']->['B']\": {'beliebe',\n",
       "  'bery',\n",
       "  'deserb',\n",
       "  'ebbery',\n",
       "  'eber',\n",
       "  'ebery',\n",
       "  'forgib',\n",
       "  'hab',\n",
       "  \"habin'\",\n",
       "  'leab',\n",
       "  'libbing',\n",
       "  'lubliest',\n",
       "  'nebber',\n",
       "  'ober',\n",
       "  'oberkote',\n",
       "  'seben',\n",
       "  'sebenteen',\n",
       "  'trabeled'},\n",
       " \"['JH']->['G']\": {\"ge'mmen\", 'gemman', 'gemplan', \"gemplan's\", 'gemplans'},\n",
       " \"[]->['M', 'AH']\": {\"ge'mmen\"},\n",
       " \"['T', 'AH', 'L', 'M', 'IH', 'N']->[]\": {\"ge'mmen\"},\n",
       " \"['T', 'ER']->[]\": {\"Mahs'\", 'Marse'},\n",
       " \"['DH']->[]\": {\"'em\"},\n",
       " \"['V']->['N']\": {'outen', \"use'n\"},\n",
       " \"['Z']->['S']\": {'fousand', \"losin'\", 'proposishion'},\n",
       " \"['ER']->['EH']\": {'docteh', 'laws-a-messy', 'powehful', 'seh'},\n",
       " \"['IH']->['AA']\": {\"cl'ar\", 'clar'},\n",
       " \"['S', 'ER']->['AH']\": {'Yassah', 'Yessah'},\n",
       " \"['EH']->['IH']\": {'keer', 'skeer', 'skeered', 'skeert', 'yit'},\n",
       " \"['OY']->['AY']\": {'jined',\n",
       "  'pinted',\n",
       "  'pintedly',\n",
       "  \"pizen'd\",\n",
       "  'pizened',\n",
       "  \"sp'iled\"},\n",
       " \"['AH']->['OW']\": {'altogever', \"won'erful\"},\n",
       " \"['AY']->['IH']\": {'arrivs',\n",
       "  \"bitin'\",\n",
       "  'chile',\n",
       "  'gwineter',\n",
       "  'hilands',\n",
       "  'lilacks',\n",
       "  'minit',\n",
       "  \"ridin'\",\n",
       "  'whinin',\n",
       "  'whitaw'},\n",
       " \"['UW', 'IH', 'NG']->['OY', 'N']\": {'doins'},\n",
       " \"['AY']->['AH']\": {'clumb'},\n",
       " \"['Y', 'AH']->['ER']\": {'Feberwary'},\n",
       " \"['AY']->['AA']\": {'Mah'},\n",
       " \"['EH', 'R']->['UW']\": {'whuh'},\n",
       " \"['D', 'IY', 'AH']->['JH', 'AY']\": {'Injine'},\n",
       " \"['T', 'AH']->[]\": {\"Cap'n\", \"cap'n\", \"li'l\"},\n",
       " \"[]->['R']\": {'dorg', 'orter', 'ourt'},\n",
       " \"['AH']->['EY']\": {\"'Federate\", 'Sartainly', 'apaht', 'sartain'},\n",
       " \"['DH']->['T']\": {'tudder'},\n",
       " \"['AH', 'DH']->['D']\": {'tudder'},\n",
       " \"['EY', 'N']->['IH']\": {\"cha'ges\"},\n",
       " \"['IH']->['IY']\": {'leetle', \"retu'n\", 'rheumatiz'},\n",
       " \"['AH']->['AY']\": {\"interes'\"},\n",
       " \"['EH', 'S', 'T']->['Z']\": {\"interes'\"},\n",
       " \"['EH', 'Z']->['UW']\": {'frew'},\n",
       " \"['OY']->['AH']\": {'disappunted'},\n",
       " \"[]->['AH']\": {\"'specially\", 'gwineter'},\n",
       " \"['T', 'ER']->['AH']\": {'Massa', 'marsa', 'massa'},\n",
       " \"['JH', 'AH']->['D', 'EH']\": {'des', \"des'\", 'dess'},\n",
       " \"['ER']->['UH']\": {'pusson'},\n",
       " \"['AH']->['AE']\": {\"'an\", 'anudder', 'hallibooloos'},\n",
       " \"['N', 'T', 'AH', 'L']->[]\": {'gemman'},\n",
       " \"['EH', 'K']->[]\": {'ceptin'},\n",
       " \"['EH', 'L']->['IH']\": {'hersef', \"hisse'f\"},\n",
       " \"['ER']->[]\": {\"Mist'\",\n",
       "  \"cul'd\",\n",
       "  'nachully',\n",
       "  \"pow'ful\",\n",
       "  \"s'prised\",\n",
       "  \"slave'y\",\n",
       "  'sprises',\n",
       "  \"und'stan\",\n",
       "  \"und'stan'\"},\n",
       " \"['IH']->[]\": {\"'Taint\",\n",
       "  \"'Tis\",\n",
       "  \"'nough\",\n",
       "  \"'tain't\",\n",
       "  \"'taint\",\n",
       "  \"'tis\",\n",
       "  \"b'l'eve\",\n",
       "  \"b'lief\",\n",
       "  'nuff',\n",
       "  \"tain't\"},\n",
       " \"['AH']->['AW']\": {\"'nough\", 'crowner'},\n",
       " \"[]->['D']\": {'comed', 'gownds'},\n",
       " \"['EY']->['AE']\": {\"a'n't\", 'plantashun', 'tak', 'wavin'},\n",
       " \"['IH']->['AH']\": {'mockisun', 'wush', 'wushes'},\n",
       " \"['TH', 'IH', 'NG']->['AH', 'N']\": {\"some'n'\"},\n",
       " \"['HH']->[]\": {\"'er\", \"'im\"},\n",
       " \"['K', 'AH', 'N']->[]\": {\"'Federate\", \"'fusing\"},\n",
       " \"['R']->['ER']\": {'heer'},\n",
       " \"['DH', 'OW']->['D', 'AW']\": {'aldough'},\n",
       " \"['DH', 'EH']->['D', 'IY']\": {'deir', \"dere's\"},\n",
       " \"['HH', 'IY', 'Z']->['EH', 'S']\": {\"'e's\"},\n",
       " \"['AE', 'D']->[]\": {\"'miration\"},\n",
       " \"['ER']->['IH', 'R']\": {\"'miration\", 'heered'},\n",
       " \"['L']->['R']\": {'Bress', 'brack', 'bress', 'bressed'},\n",
       " \"['V']->['B', 'EH']\": {'sabed'},\n",
       " \"['AH', 'TH']->['OW', 'T']\": {\"not'ing\"},\n",
       " \"['IY', 'R']->['AY', 'AE', 'AH']\": {'hyeah'},\n",
       " \"['AO']->['AA']\": {'gon', 'good-fartune'},\n",
       " \"['AE']->['AA']\": {\"Mas'r\", \"mas'r\"},\n",
       " \"['EY']->['EH']\": {'Mebbe', 'sesso', 'tek'},\n",
       " \"['IY']->[]\": {'Mebbe'},\n",
       " \"['IY']->['EH']\": {\"Evenin'\", \"be'nt\"},\n",
       " \"[]->['IH']\": {\"Evenin'\"},\n",
       " \"['AW']->['UW']\": {'roun', \"roun'\", 'yus'},\n",
       " \"['AE', 'F']->['AA', 'R']\": {'arter', 'arternoon'},\n",
       " \"['AE', 'T']->['AA']\": {\"Tha's\", \"tha's\"},\n",
       " \"['AE']->['AA', 'R']\": {'Marse',\n",
       "  'Marser',\n",
       "  'Marster',\n",
       "  \"mars'r\",\n",
       "  'marsa',\n",
       "  'marser',\n",
       "  \"marser's\",\n",
       "  'marster'},\n",
       " \"['S']->['Z']\": {'escuse', 'huzzie', 'scuse', 'wase'},\n",
       " \"['D']->['IY']\": {'chile'},\n",
       " \"['EH', 'K', 'S']->[]\": {\"'pressly\"},\n",
       " \"['OW']->['IY']\": {'borry'},\n",
       " \"[]->['EY', 'G']\": {'agen'},\n",
       " \"['G', 'EH']->[]\": {'agen'},\n",
       " \"['AE', 'DH']->['UW', 'TH']\": {'ruther'},\n",
       " \"['TH', 'IH', 'NG']->['P', 'AH', 'N']\": {'sumpen'},\n",
       " \"['EH']->['EY']\": {'ainy', 'daid', \"haid's\", 'raikon'},\n",
       " \"['OY']->['AA', 'D', 'IY']\": {'noboddy'},\n",
       " \"['Y', 'AH']->[]\": {\"reg'lar\"},\n",
       " \"['AE']->['EY']\": {\"Cap'en\", \"cap'en\", 'kaint', 'nachully'},\n",
       " \"['HH']->['AH', 'M']\": {'Lawd-a-massy'},\n",
       " \"['V', 'M', 'ER']->[]\": {'Lawd-a-massy', 'Lorra-massy'},\n",
       " \"['D', 'HH']->['AH', 'M']\": {'Lorra-massy'},\n",
       " \"['AH']->['AO']\": {\"'portant\", \"W'at\", 'a-haungry', 'hongry', \"w'at\"},\n",
       " \"['D', 'EH', 'Z']->['S']\": {\"'sert\", \"'serter\"},\n",
       " \"['L']->[]\": {'Sutny',\n",
       "  \"deyse'f\",\n",
       "  \"he'p\",\n",
       "  'sef',\n",
       "  'sojers',\n",
       "  \"sut'n'y\",\n",
       "  'sutney',\n",
       "  'sutny',\n",
       "  'yersef'},\n",
       " \"['B', 'IH']->[]\": {\"'fore\", 'caze'},\n",
       " \"['IH', 'NG']->['AH', 'N']\": {\"Cel'bratun\",\n",
       "  \"actin'\",\n",
       "  \"expectin'\",\n",
       "  \"goin'\",\n",
       "  \"workin'\"},\n",
       " \"['AA', 'Z']->['AH', 'S']\": {'wus'},\n",
       " \"['IY', 'V', 'IH']->['EH', 'B', 'AH']\": {'eben'},\n",
       " \"['AO']->['AE']\": {\"alwa's\"},\n",
       " \"['EY']->['AH']\": {\"Cel'bratun\", \"alwa's\", 'comed'},\n",
       " \"['EH', 'R']->['ER']\": {'foreber',\n",
       "  'shurrufs',\n",
       "  \"skear'd\",\n",
       "  'somewhar',\n",
       "  'somewhur',\n",
       "  'sumwhar',\n",
       "  'turrible'},\n",
       " \"['AW', 'TH']->['UW']\": {'moufful'},\n",
       " \"['UH']->['AH']\": {'moufful', 'wud', \"wudn't\"},\n",
       " \"['AH', 'Z']->['ER', 'S']\": {'persesshun'},\n",
       " \"['AH', 'TH']->['AA', 'F']\": {\"Noffin'\", 'noffin'},\n",
       " \"['K', 'AE', 'R', 'IY']->['S', 'AY', 'AH']\": {'cyah'},\n",
       " \"['AH', 'G', 'EH']->['AA', 'JH', 'IY']\": {\"ag'in\", 'agin'},\n",
       " \"['IH', 'R']->['IY']\": {'ni'},\n",
       " \"['Y', 'UW']->['D', 'AA']\": {'da'},\n",
       " \"[]->['AE']\": {'raound'},\n",
       " \"[]->['M', 'P', 'L', 'AE']\": {'gemplan'},\n",
       " \"['T', 'AH', 'L', 'M', 'AH', 'N']->[]\": {'gemplan'},\n",
       " \"['TH', 'R']->['DH']\": {'thoo'},\n",
       " \"['Z', 'AH']->['S', 'IH']\": {'dussint'},\n",
       " \"['AH']->['IY']\": {\"Baptis'\", 'butifullest'},\n",
       " \"['DH', 'AH']->['T']\": {\"t'other\"},\n",
       " \"['UW']->['AW', 'S']\": {'youse'},\n",
       " \"['AE']->['AH']\": {\"Baptis'\", \"an'ef\"},\n",
       " \"['AH', 'Z', 'AH']->['AE', 'S']\": {\"dassn't\"},\n",
       " \"['OW']->[]\": {\"'possum\", 'ter-morrer'},\n",
       " \"['D']->['T']\": {'beyont', 'loant', 'skeert'},\n",
       " \"['AA']->['OW']\": {\"McDonal'\", 'sumbodv'},\n",
       " \"['IY']->['V']\": {'sumbodv'},\n",
       " \"['AO', 'L']->['AA']\": {\"a'most\"},\n",
       " \"['IY', 'V']->['EH', 'B', 'AH']\": {'ebenin'},\n",
       " \"['D', 'AH']->[]\": {\"ha'n't\"},\n",
       " \"['DH', 'AH']->['T', 'IY', 'EY', 'CH']\": {\"th'\"},\n",
       " \"['EH', 'S']->['AE']\": {'Yasser'},\n",
       " \"['ER']->['IH']\": {'dollehs', 'yestiddy'},\n",
       " \"['EY']->['IY']\": {\"'teak\", 'yestiddy'},\n",
       " \"['IH', 'M']->[]\": {\"'portant\"},\n",
       " \"['UW', 'R']->['OW']\": {'Poh', 'po', \"po'\"},\n",
       " \"['DH', 'EH']->['D', 'AA']\": {'dar', \"dar'll\", \"dar's\"},\n",
       " \"['D', 'IY', 'AH']->['JH', 'IH']\": {'Injin'},\n",
       " \"['UW']->['AH']\": {'tuh', 'yuh'},\n",
       " \"['ER', 'T']->[]\": {'comfabull'},\n",
       " \"['DH', 'AH']->['D', 'EH']\": {\"dem's\"},\n",
       " \"['G', 'EH']->['JH', 'IH']\": {\"gittin'\"},\n",
       " \"['AH', 'G', 'EH']->['ER', 'JH', 'IH']\": {\"erg'in\", 'ergin'},\n",
       " \"[]->['HH']\": {\"h'experience\", \"hain't\", 'haint', 'wha', 'whitaw', 'whut'},\n",
       " \"['OW']->['W']\": {'gwyn'},\n",
       " \"['V', 'AH']->['B', 'IH']\": {'debbil', 'debil'},\n",
       " \"['AY']->['AW']\": {'mout', 'mouter'},\n",
       " \"['HH', 'AE', 'V']->['ER']\": {'mouter'},\n",
       " \"['S']->[]\": {\"'teak\"},\n",
       " \"['R', 'W']->[]\": {'forrards'},\n",
       " \"['ER']->['R']\": {\"'round\", \"P'rhaps\", \"int'ruptin'\", \"sep'rate\"},\n",
       " \"['T', 'ER']->['R']\": {\"mars'r\"},\n",
       " \"['AA']->['AE']\": {'drap', 'drapping', \"fallin'\", 'yander'},\n",
       " \"['AE']->['AO']\": {'chawnces'},\n",
       " \"['EH']->['AE']\": {'Yassir', \"marryin'\", 'yass', 'yassuh'},\n",
       " \"['AO', 'R', 'S']->['OW', 'Z']\": {'cose'},\n",
       " \"['T', 'AH', 'L']->['ER']\": {'genermen'},\n",
       " \"['CH']->['T']\": {\"cretur's\", 'nateral', 'picter', 'questun', 'scriptur'},\n",
       " \"['ER']->['UW']\": {'wuth'},\n",
       " \"[]->['EH']\": {\"McDonal'\", \"las'\", \"n't\"},\n",
       " \"['AH', 'K']->['S', 'IY']\": {\"McDonal'\"},\n",
       " \"['ER']->['IY']\": {\"heah'd\"},\n",
       " \"['AW', 'ER']->['OW', 'R']\": {\"flow'rs\"},\n",
       " \"['DH', 'EH', 'R']->['TH', 'EY']\": {'thaih'},\n",
       " \"['N', 'AH']->['M', 'P']\": {'gemplans'},\n",
       " \"['M']->[]\": {\"gemplan's\", 'gemplans'},\n",
       " \"['AH', 'S', 'T']->['Z']\": {\"bigges'\"},\n",
       " \"['EY']->['AE', 'G']\": {'bagonets'},\n",
       " \"['S', 'ER']->['IH', 'R']\": {'Yassir', 'Yessir'},\n",
       " \"['AO']->['EY']\": {'caze'},\n",
       " \"['ER']->['AH', 'S', 'AH']\": {'wussest'},\n",
       " \"['IH', 'AA']->['IY', 'AO']\": {'beyont'},\n",
       " \"[]->['K', 'W', 'IH', 'R']\": {'quirin'},\n",
       " \"['K', 'W', 'AY', 'ER', 'IH', 'NG']->[]\": {'quirin'},\n",
       " \"[]->['IH', 'Z']\": {'fokses'},\n",
       " \"['V', 'R']->['B', 'ER']\": {'eberyting'},\n",
       " \"[]->['Z']\": {'froos'},\n",
       " \"['DH', 'EH']->['D', 'IH']\": {'dere'},\n",
       " \"['DH']->['V']\": {'altogever'},\n",
       " \"['EH', 'L']->['AH']\": {'mysef'},\n",
       " \"['AH', 'T', 'AE']->['AA']\": {\"whah'm\"},\n",
       " \"['DH', 'AH']->['IY']\": {\"'e\"},\n",
       " \"['IH', 'V']->['AY', 'B', 'EH']\": {'libed'},\n",
       " \"['IH']->['AE']\": {'axplaned'},\n",
       " \"['AH', 'L']->[]\": {\"gent'mun\"},\n",
       " \"[]->['W']\": {'gwo'},\n",
       " \"['IH', 'K']->['EH']\": {'escuse'},\n",
       " \"['R', 'AH']->['IH']\": {'Mistis', 'mistis'},\n",
       " \"[]->['G']\": {\"Mahg'ry\"},\n",
       " \"['JH', 'ER']->[]\": {\"Mahg'ry\"},\n",
       " \"['AA']->['AH']\": {'wuz'},\n",
       " \"['S', 'T', 'S']->['Z', 'IH', 'Z']\": {'ghoses'},\n",
       " \"[]->['S']\": {'sability'},\n",
       " \"['W']->[]\": {'backards'},\n",
       " \"['AA', 'R']->['AE']\": {'passley'},\n",
       " \"['UH', 'R']->['UW', 'AA']\": {'shuah'},\n",
       " \"['UH']->['AO']\": {'shorely'},\n",
       " \"['AH', 'M']->[]\": {'rheumatiz'},\n",
       " \"['AY']->['AE']\": {'lak'},\n",
       " \"['ER', 'G']->['AO', 'R', 'JH']\": {'forgib'},\n",
       " \"['V']->['B', 'IY']\": {'beliebes'},\n",
       " \"['AA']->[]\": {\"be'nt\", \"n't\"},\n",
       " \"['AO', 'Z']->['EY', 'S']\": {'bekase'},\n",
       " \"['UW']->['OW', 'D']\": {'growed'},\n",
       " \"['EH', 'S']->['AA', 'Z']\": {'Yaas'},\n",
       " \"['EH', 'R', 'AH']->['ER']\": {'shurff'},\n",
       " \"['N', 'D']->['NG', 'T']\": {'brungt'},\n",
       " \"['IY', 'R']->['AY', 'ER']\": {\"h'yar\", 'hyer'},\n",
       " \"['AO']->[]\": {'crowner'},\n",
       " \"['EH']->['AO']\": {\"w'ar\"},\n",
       " \"['IH']->['OW']\": {'gove'},\n",
       " \"['AH']->['Y', 'UW']\": {'yuthers'},\n",
       " \"['B', 'IH']->['AH']\": {'acause'},\n",
       " \"['AH', 'T']->['AA']\": {'wha'},\n",
       " \"['S', 'T']->[]\": {'ergin'},\n",
       " \"['ER', 'HH']->['R']\": {\"P'r'aps\"},\n",
       " \"['N']->['D']\": {'throwed'},\n",
       " \"['R', 'IH']->[]\": {\"'member\", \"'spectable\"},\n",
       " \"['IY']->['ER']\": {'inviderous'},\n",
       " \"['ER']->['AO', 'R', 'AH']\": {'wurrell'},\n",
       " \"[]->['Y']\": {'yearth'},\n",
       " \"['TH']->['D']\": {'Dere', 'fortwid', 'widdout', 'widout'},\n",
       " \"[]->['ER']\": {'sorrer'},\n",
       " \"['IY', 'ER']->[]\": {'sorrer'},\n",
       " \"['EH']->['IY']\": {\"we'en\"},\n",
       " \"['UH', 'R']->['UW']\": {'shue'},\n",
       " \"[]->['OW', 'T']\": {\"notin'\"},\n",
       " \"['TH', 'IH', 'NG']->['N']\": {\"notin'\"},\n",
       " \"['ER', 'G', 'EH']->['AA', 'JH', 'IH']\": {\"fo'git\"},\n",
       " \"['AH', 'N']->[]\": {'twell'},\n",
       " \"['OW']->['AH']\": {\"don'e\"},\n",
       " \"['EH', 'V']->['IY', 'B']\": {'neber'},\n",
       " \"[]->['AH', 'N']\": {'broughten'},\n",
       " \"[]->['B']\": {\"bain't\"},\n",
       " \"['AY']->['AO']\": {'moughty'},\n",
       " \"['AO']->['AH']\": {'alway'},\n",
       " \"['Z']->[]\": {'alway'},\n",
       " \"['ER', 'HH']->[]\": {\"P'a'ps\"},\n",
       " \"['V', 'ER']->['B', 'EH', 'R']\": {'eberry'},\n",
       " \"['HH', 'ER']->['Y', 'IH', 'R']\": {'yeared'},\n",
       " \"['W', 'UH']->['OW']\": {\"'oman\"},\n",
       " \"['IH', 'NG']->['IY', 'N']\": {'see-sawin'},\n",
       " \"['V', 'EH']->['W', 'IH', 'D']\": {'widgable'},\n",
       " \"['AE']->['EY', 'EH']\": {\"las'\"},\n",
       " \"['IH', 'V']->['IY', 'B']\": {'deliber'},\n",
       " \"[]->['AA', 'R', 'S', 'UW', 'IH']\": {'parsewered'},\n",
       " \"['S', 'AH', 'V', 'IH', 'R']->[]\": {'parsewered'},\n",
       " \"['OW', 'IH', 'NG']->['OY', 'N']\": {\"ergoin'\"},\n",
       " \"['AH', 'V']->['N']\": {\"out'n\"},\n",
       " \"['AH', 'B']->[]\": {\"'jectment\"},\n",
       " \"['T', 'AH']->['ER']\": {'orter'},\n",
       " \"['OW', 'TH']->['AO', 'F']\": {'boff'},\n",
       " \"['N', 'T', 'AH']->['M', 'P']\": {\"gemplan's\"},\n",
       " \"['UH', 'R']->['OW']\": {'sho', \"sho'\"},\n",
       " \"['EH', 'S', 'T']->['IY', 'Z']\": {\"bes'\"},\n",
       " \"['HH', 'AE']->['Y', 'AH']\": {'uv'},\n",
       " \"['AE', 'F']->['EY']\": {\"a'ter\"},\n",
       " \"['ER']->['EH', 'R']\": {'sare'},\n",
       " \"['DH', 'AE']->['D', 'IY', 'EH']\": {\"d'n\"},\n",
       " \"['DH', 'AE']->[]\": {\"less'n\"},\n",
       " \"['ER']->['AO']\": {'whitaw'},\n",
       " \"['AE', 'DH']->['EY', 'D']\": {'rader'},\n",
       " \"['R', 'AH']->['AA']\": {\"f'om\"},\n",
       " \"['D', 'IH']->['EH']\": {\"an'ef\"},\n",
       " \"['HH', 'IY']->['Y', 'AA']\": {'yar'},\n",
       " \"['EH', 'S']->['AE', 'Z']\": {'Yas', 'yas'},\n",
       " \"['W', 'EY']->['ER']\": {'allers'},\n",
       " \"['D', 'IH']->[]\": {'scovered'},\n",
       " \"['AH']->['IY', 'N']\": {'een'},\n",
       " \"['K']->[]\": {'espect'},\n",
       " \"['T']->['K']\": {'turcle'},\n",
       " \"['EH']->['OW']\": {'loant'},\n",
       " \"['HH', 'AE']->[]\": {\"haid's\"},\n",
       " \"[]->['IH', 'S']\": {'promisin'},\n",
       " \"['S', 'IH', 'NG']->['N']\": {'promisin'},\n",
       " \"['R', 'D', 'HH', 'AE', 'V']->['Z', 'AH']\": {'laws-a-messy'},\n",
       " \"['AH']->['UW']\": {'hallibooloos'},\n",
       " \"['DH', 'EH']->['TH', 'AA']\": {\"thar'\"}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------\n",
    "# get a dict of words by phonological variation: i.e., variation2words['man'][\"['NG']->['N']\"] = [\"comin'\", ...]\n",
    "# ------\n",
    "with open(f\"RQ1_ood_words/RQ1_ood_words_Negro,negro.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "for non_canonical, corrections in annotations.items():\n",
    "    if len(corrections) == 1 and non_canonical not in manually_assigned:\n",
    "        canonical = corrections[0]\n",
    "\n",
    "        for from_, to_ in get_changes(re.findall(r\"\\w+\", arpaguess(canonical)[0]), re.findall(r\"\\w+\", arpaguess(non_canonical)[0])):\n",
    "            # capture instances\n",
    "            if f\"{from_}->{to_}\" not in phonological_variations_PG:\n",
    "                phonological_variations_PG[f\"{from_}->{to_}\"] = set()\n",
    "            phonological_variations_PG[f\"{from_}->{to_}\"].add(non_canonical)\n",
    "\n",
    "phonological_variations_PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "585a3cc1-8269-4bfb-8314-97d22deb2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PG phonological variations as a base\n",
    "variations = copy.deepcopy(phonological_variations_PG)  # {\"man\":{\"italicised\":set of non canonical words}\n",
    "\n",
    "# add in phonological variations from LOC\n",
    "for v, wordset in phonological_variations_LOC.items():\n",
    "    if v not in variations:\n",
    "        variations[v] = set()\n",
    "    variations[v] = variations[v].union(wordset)   \n",
    "\n",
    "# add in ...\n",
    "variations[\"dialect_words1\"] = set(dialect_words1.keys())\n",
    "variations[\"dialect_words2\"] = set(dialect_words2.keys())\n",
    "variations[\"regularised_irregular\"] = set(regularised_irregular.keys())\n",
    "variations[\"italicised\"] = set(italicised_words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e0106-5608-43e8-8207-93cf5c8f2b66",
   "metadata": {},
   "source": [
    "# First, as a check, without considering the downstream effects ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6fbb3-1d6a-401f-b9ae-bdad4c461668",
   "metadata": {},
   "source": [
    "\n",
    "## get $C(\\bar{S_{nword}, lit}, v) = \\frac{\\bar{S_{i}} - \\bar{S}_{i,\\backslash v}}{\\bar{S_{i}}}$\n",
    "\n",
    "i.e., the proportional reduction in mean word LLM surprisal due to the ommission of words demonstrative of the variation\n",
    "\n",
    "doesn't consider downstream effects due to the upstream instances of variation in question!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0eeef8d-d058-4dec-aa76-995ed16d4cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 432/432 [00:00<00:00, 492.81it/s]\n"
     ]
    }
   ],
   "source": [
    "contributions_lit = {}\n",
    "\n",
    "# get mean of word LLM surprisal\n",
    "x_bar = np.array(list(chain(*PG_df.loc[PG_df.loc[:,\"speaker\"].isin([\"Negro\",\"negro\"]), \"surprisals\"]))).mean()\n",
    "\n",
    "# consider each variation in turn\n",
    "V = variations.keys()\n",
    "for v in tqdm(V):    \n",
    "\n",
    "    ss = 0\n",
    "    counter = 0\n",
    "\n",
    "    # surprisals for speaker, ignoring corrected quotes\n",
    "    surprisals = list(chain(*PG_df.loc[PG_df.loc[:,\"speaker\"].isin(speakers), \"surprisals\"]))\n",
    "    words = list(chain(*PG_df.loc[PG_df.loc[:,\"speaker\"].isin(speakers), \"words\"]))\n",
    "\n",
    "    for word, surprisal in zip(words, surprisals):\n",
    "        if word not in variations[v]:\n",
    "            ss += surprisal\n",
    "            counter += 1\n",
    "\n",
    "    contributions_lit[v] = 100*(x_bar - ss / counter) / x_bar   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "142fddb6-6b10-4c81-a833-cdbfd0008a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.267566609622147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.5571436397028469"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = \"['NG']->['N']\"\n",
    "display(x_bar_PG)\n",
    "display(contributions_lit[v])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3214709-49e5-46ae-b4c9-d7d30365eee8",
   "metadata": {},
   "source": [
    "\n",
    "## get $C(\\bar{S}_{nword, news}, v) = \\frac{\\bar{S_{i}} - \\bar{S}_{i,\\backslash v}}{\\bar{S_{i}}}$\n",
    "\n",
    "i.e., the proportional reduction in mean word LLM surprisal due to the ommission of words demonstrative of the variation\n",
    "\n",
    "doesn't consider downstream effects due to the upstream instances of variation in question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8c8bc41-1259-46d7-9d1c-2835cc9858c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 432/432 [00:01<00:00, 387.74it/s]\n"
     ]
    }
   ],
   "source": [
    "contributions_news = {}\n",
    "\n",
    "# get mean of word LLM surprisal\n",
    "words = list(chain(*[get_words(x[1][0]) for x in tuples_news]))\n",
    "surprisals = list(chain(*[x[1][-1] for x in tuples_news]))\n",
    "x_bar = np.array(surprisals).mean()\n",
    "\n",
    "# consider each variation in turn\n",
    "V = variations.keys()\n",
    "for v in tqdm(V):    \n",
    "\n",
    "    ss = 0\n",
    "    counter = 0\n",
    "\n",
    "    for word, surprisal in zip(words, surprisals):\n",
    "        if word not in variations[v]:\n",
    "            ss += surprisal\n",
    "            counter += 1\n",
    "\n",
    "    contributions_news[v] = 100*(x_bar - ss / counter) / x_bar   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "990c5605-9c25-4926-b6bf-e5f8087df2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.373269680636792"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.15873526998720544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = \"['NG']->['N']\"\n",
    "display(x_bar_LOC)\n",
    "display(contributions_news[v])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9d19b-b6d2-49d1-b4d0-d799335a9e01",
   "metadata": {},
   "source": [
    "## get $C(\\bar{S}_{\\text{nword}}, v) - C(\\bar{S}_{\\text{normative reference speaker}}, v)$\n",
    "\n",
    "i.e., let's look at the relative contributions of words demonstrative of each variation...\n",
    "\n",
    "again ... does not correct for downstream effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9decfbc6-2f42-4416-903c-4101ba866fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"['NG']->['N']\", 1.4, 11.4, 1.56, 0.16),\n",
      " ('dialect_words2', 0.46, 3.7, 0.45, -0.01),\n",
      " (\"['ER']->['AH']\", 0.42, 3.5, 0.48, 0.06),\n",
      " (\"['T']->[]\", 0.41, 3.3, 0.47, 0.07),\n",
      " (\"['AH']->[]\", 0.38, 3.1, 0.4, 0.01),\n",
      " (\"['TH']->['T']\", 0.25, 2.0, 0.24, -0.01),\n",
      " (\"['R']->[]\", 0.24, 2.0, 0.28, 0.04),\n",
      " (\"['AE']->['AA', 'R']\", 0.24, 2.0, 0.28, 0.04),\n",
      " (\"['TH']->['F']\", 0.22, 1.7, 0.21, -0.01),\n",
      " (\"['OW', 'IH', 'NG']->['W', 'AY', 'N']\", 0.22, 1.8, 0.21, -0.01),\n",
      " ('dialect_words1', 0.22, 1.8, 0.23, 0.01),\n",
      " (\"['T', 'ER']->[]\", 0.18, 1.5, 0.21, 0.03),\n",
      " (\"['L']->[]\", 0.16, 1.3, 0.16, 0.0),\n",
      " (\"['ER']->['AA', 'R']\", 0.14, 1.1, 0.17, 0.03),\n",
      " (\"['AH']->['IH']\", 0.13, 1.0, 0.15, 0.02)]\n"
     ]
    }
   ],
   "source": [
    "# biggest differences between ...\n",
    "xbar_lit = np.array(list(chain(*PG_df.loc[PG_df.loc[:,\"speaker\"].isin([\"Negro\",\"negro\"]), \"surprisals\"]))).mean()\n",
    "xbar_news = np.array(list(chain(*[x[1][-1] for x in tuples_news]))).mean()\n",
    "\n",
    "pp(\n",
    "    sorted(\n",
    "        [\n",
    "            (\n",
    "                # varatiation\n",
    "                v, \n",
    "                # difference in proportional contributions\n",
    "                round(contributions_lit[v] - contributions_news[v], 2), \n",
    "                # percentage reduction in difference in mean word LLM surprisal betweeen nword and normative reference speaker\n",
    "                round(100*(contributions_lit[v]*xbar_lit/100 - contributions_news[v]*xbar_news/100)/(xbar_lit - xbar_news), 1),\n",
    "                # proportional contribution of variation wrt., nword mean word LLM surprisal\n",
    "                round(contributions_lit[v], 2), \n",
    "                # proportional contribution of variation wrt., normative ref mean word LLM surprisal\n",
    "                round(contributions_news[v], 2)\n",
    "            ) \n",
    "            if v in contributions_news\n",
    "            else (\n",
    "                # varatiation\n",
    "                v, \n",
    "                # difference in proportional contributions\n",
    "                contributions_lit[v] - 0, \n",
    "                # reduction in difference in mean word LLM surprisal betweeen nword and normative reference speaker\n",
    "                100*(contributions_lit[v]*xbar_lit/100 - 0)/(xbar_lit - xbar_news),\n",
    "                # proportional contribution of variation wrt., nword mean word LLM surprisal\n",
    "                contributions_lit[v], \n",
    "                # proportional contribution of variation wrt., normative ref mean word LLM surprisal\n",
    "                0 \n",
    "            ) for v in contributions_lit.keys()\n",
    "        ], \n",
    "        key = lambda x: x[1], reverse=True\n",
    "    )[:15]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51862206-6142-451c-b919-f994b230aec5",
   "metadata": {},
   "source": [
    "# Next, considering the downstream effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9598a-3860-47ec-a3ea-3f37d94b070d",
   "metadata": {},
   "source": [
    "# get $C_{n\\text{-word},lit}(\\bar{S}_{i}, v) = \\frac{\\bar{S}_{i} - \\bar{S}_{i,\\backslash v}}{\\bar{S_{i}}}$\n",
    "\n",
    "i.e., contribution correcting for downstream effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "768031b1-2fdb-4826-aa72-bced9e5d0681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 432/432 [00:01<00:00, 351.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# consider all variations found in the corrected quotes\n",
    "V = variations.keys()\n",
    "\n",
    "contributions_lit_c = {}  # estimated contributions based on 'correction' of words demonstrative of a each variation\n",
    "contributions_lit_c[s] = {}\n",
    "\n",
    "x_bar =  np.array(list(chain(*PG_df.loc[PG_df.loc[:, 'speaker'].isin([\"Negro\",\"negro\"]), \"surprisals\"]))).mean() \n",
    "\n",
    "# consider each variation in turn\n",
    "for v in tqdm(V):\n",
    "\n",
    "    # get PG_df indices which contain words demonstrative of the variation\n",
    "    i2c = {i:c for c, (i, d, variation , _) in enumerate(corrected_quotes_PG) if v==variation}  \n",
    "    I = list(i2c.keys())\n",
    "\n",
    "    # surprisals for speaker, ignoring I ... we will add to these\n",
    "    surprisals = list(chain(*PG_df.loc[(~PG_df.loc[:,\"i\"].isin(I)) & (PG_df.loc[:,\"speaker\"].isin(speakers)), \"surprisals\"]))\n",
    "\n",
    "    # add surprisals for I\n",
    "    for i in PG_df.loc[(PG_df.loc[:,\"i\"].isin(I)) & (PG_df.loc[:,\"speaker\"].isin([\"Negro\", \"negro\"])), \"i\"]:\n",
    "\n",
    "        # get surprisals (by word) wrt., corrected quotations\n",
    "        quote_c = corrected_quotes_PG[i2c[i]][3]\n",
    "        chain_c = np.array(chains_corrected_PG[i2c[i]][1:])  # ignore standard beginning token given to all chains\n",
    "        words_c = get_words(quote_c)\n",
    "        tokens_c = tokenizer.tokenize(quote_c)\n",
    "        surprisals_c = get_surprisals(words_c, tokens_c, chain_c)\n",
    "\n",
    "        # sum the surprisals where corresponding words are unchanged between correction and original \n",
    "        matcher = difflib.SequenceMatcher(None, PG_df.loc[i, 'words'], words_c)\n",
    "        opcodes = matcher.get_opcodes()\n",
    "        for tag, i1, i2, j1, j2 in opcodes:  # where x1\n",
    "            if tag == \"equal\":\n",
    "                surprisals += surprisals_c[j1:j2]\n",
    "\n",
    "    # report\n",
    "    contributions_lit_c[v] = 100*(x_bar - np.array(surprisals).mean())/x_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dad8579e-34c9-40c2-b2af-b2b473563e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.267566609622147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.5571436397028469"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.5278906256067426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = \"['NG']->['N']\"\n",
    "display(xbar_lit)\n",
    "display(contributions_lit[v])\n",
    "display(contributions_lit_c[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfe522-d777-48d4-9b42-971c06b402d4",
   "metadata": {},
   "source": [
    "# get $C_{n\\text{-word},news}(\\bar{S}_{i}, v) = \\frac{\\bar{S}_{i} - \\bar{S}_{i,\\backslash v}}{\\bar{S_{i}}}$\n",
    "\n",
    "i.e., contribution correcting for downstream effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d153be8d-e10d-456e-9942-56b32530a316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86,\n",
       " {\"I's\": 'I am'},\n",
       " 'dialect_words1',\n",
       " 'Jist like a woman! always wants to visit in bad weather! And now I am got to sue de white folks of this train in the Federal Court for my damages and rights. I is going to do that very thing, if God spares me and I can git a lawyer!']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_quotes_LOC[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf22e3a3-8799-48f9-8ca1-5e2186aff42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nutfin'] ['nothing']\n",
      "['sich'] ['such']\n",
      "[\"wantin'\"] ['wanting']\n",
      "[\"Dancin'\"] ['Dancing']\n",
      "['bress'] ['bless']\n",
      "[\"doin'\"] ['doing']\n",
      "[\"readin'\"] ['reading']\n",
      "[\"cain't\"] [\"can't\"]\n",
      "['Heah'] ['Here']\n",
      "['heah'] ['hear']\n",
      "[\"nothin'\"] ['nothing']\n",
      "['aginst'] ['against']\n",
      "[\"roarin'\"] ['roaring']\n",
      "['Dancin'] ['Dancing']\n",
      "[\"Dancin'\"] ['Dancing']\n",
      "[\"'publicans\"] ['republicans']\n",
      "[\"'publicans\"] ['republicans']\n",
      "['sah'] ['sir']\n",
      "['mah'] ['my']\n",
      "[\"hopin'\"] ['hoping']\n",
      "[\"'cause\"] ['because']\n",
      "['perceeded'] ['preceded']\n",
      "[\"th'\"] ['the']\n",
      "[\"th'\"] ['the']\n",
      "[\"th'\"] ['the']\n",
      "[\"th'\"] ['the']\n",
      "[\"summin'\"] ['something']\n",
      "[\"sumpin'\"] ['something']\n",
      "[\"Dancin'\"] ['Dancing']\n",
      "[\"Dancin'\"] ['Dancing']\n",
      "[\"Dancin'\"] ['Dancing']\n",
      "[\"noffin'\"] ['nothing']\n",
      "['wuz'] ['was']\n",
      "['wuz'] ['was']\n",
      "['sint'] [\"isn't\"]\n",
      "[\"showin'\"] ['showing']\n",
      "['debbil'] ['devil']\n",
      "[\"Dancin'\"] ['Dancing']\n",
      "['nuffin'] ['nothing']\n",
      "['truf'] ['truth']\n",
      "[\"a-tryin'\"] ['a-trying']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"showin'\"] ['showing']\n",
      "[\"sett'n\"] ['setting']\n",
      "['somewhars'] ['somewhere']\n",
      "['debbil'] ['devil']\n",
      "[\"accomodatin'\"] ['accomodating']\n",
      "[\"a-tryin'\"] ['a-trying']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"sett'n\"] ['setting']\n",
      "[\"li'l\"] ['little']\n",
      "['foreveh'] ['forever']\n",
      "[\"nothin'\"] ['nothing']\n",
      "['foheveh'] ['forever']\n",
      "[\"mornin'\"] ['morning']\n",
      "['Goed'] ['Good']\n",
      "[\"a-tryin'\"] ['a-trying']\n",
      "[\"a'ter\"] ['after']\n",
      "['debbil'] ['devil']\n",
      "[\"sett'n\"] ['setting']\n",
      "[\"'round\"] ['around']\n",
      "['debbil'] ['devil']\n",
      "[\"showin'\"] ['showing']\n",
      "['evah'] ['ever']\n",
      "['wanter'] ['want', 'to']\n",
      "['wah'] ['war']\n",
      "['suh'] ['sir']\n",
      "[\"befo'\"] ['before']\n",
      "[\"heah's\"] [\"here's\"]\n",
      "['evah'] ['ever']\n",
      "['wanter'] ['want', 'to']\n",
      "[\"doin's\"] ['doings']\n",
      "[\"nothin'\"] ['nothing']\n",
      "[\"more'n\"] ['more', 'than']\n",
      "['wuz'] ['was']\n",
      "[\"tellin'\"] ['telling']\n",
      "['showin'] ['showing']\n",
      "['hoss'] ['horse']\n",
      "['mouf'] ['mouth']\n",
      "['somewhars'] ['somewhere']\n",
      "['nacherally'] ['naturally']\n",
      "['evah'] ['ever']\n",
      "[\"ridin'\"] ['riding']\n",
      "['laik'] ['like']\n",
      "[\"doin'\"] ['doing']\n",
      "['gotter'] ['got', 'to']\n",
      "[\"doin'\"] ['doing']\n",
      "['Atalanty'] ['Atlanta']\n",
      "[\"agin'\"] ['again']\n",
      "[\"sneakin'\"] ['sneaking']\n",
      "[\"a-tryin'\"] ['a-trying']\n",
      "['mawnin'] ['morning']\n",
      "['mawnin'] ['morning']\n",
      "['heeerd'] ['heard']\n",
      "['ef'] ['if']\n",
      "['foreveh'] ['forever']\n",
      "[\"th'\"] ['the']\n",
      "['mouf'] ['mouth']\n",
      "[\"tryin'\"] ['trying']\n",
      "[\"mawnin'\"] ['morning']\n",
      "['mawnin'] ['morning']\n",
      "[\"anythin'\"] ['anything']\n",
      "[\"tryin'\"] ['trying']\n",
      "[\"mawnin'\"] ['morning']\n",
      "['mawnin'] ['morning']\n",
      "['arter'] ['after']\n",
      "['gwine'] ['going']\n",
      "[\"roun'\"] ['round']\n",
      "[\"showin'\"] ['showing']\n",
      "['gwine'] ['going']\n",
      "['gwine'] ['going']\n",
      "[\"Nuffin'\"] ['Nothing']\n",
      "['somewhars'] ['somewhere']\n",
      "[\"th'\"] ['the']\n",
      "['foreveh'] ['forever']\n",
      "['debbil'] ['devil']\n",
      "['lak'] ['like']\n",
      "['evah'] ['ever']\n",
      "[\"livin'\"] ['living']\n",
      "['neveh'] ['never']\n",
      "['asts'] ['asks']\n",
      "['wah'] ['war']\n",
      "['wah'] ['war']\n",
      "['suh'] ['sir']\n",
      "['suh'] ['sir']\n",
      "['dar'] ['there']\n",
      "['des'] ['just']\n",
      "['des'] ['just']\n",
      "['gwine'] ['going']\n",
      "['des'] ['just']\n",
      "['Massa'] ['Master']\n",
      "['arter'] ['after']\n",
      "[\"showin'\"] ['showing']\n",
      "[\"th'\"] ['the']\n",
      "['suh'] ['sir']\n",
      "[\"showin'\"] ['showing']\n",
      "[\"a-tryin'\"] ['a-trying']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"peelin'\"] ['peeling']\n",
      "[\"spec'able\"] ['respectable']\n",
      "[\"savin'\"] ['saving']\n",
      "['gwine'] ['going']\n",
      "['somewhars'] ['somewhere']\n",
      "['nacherally'] ['naturally']\n",
      "['thouht'] ['though']\n",
      "[\"agin'\"] ['again']\n",
      "[\"sneakin'\"] ['sneaking']\n",
      "[\"nothin'\"] ['nothing']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"mawnin'\"] ['morning']\n",
      "[\"tryin'\"] ['trying']\n",
      "['debbil'] ['devil']\n",
      "['showin'] ['showing']\n",
      "[\"showin'\"] ['showing']\n",
      "['kaint'] [\"can't\"]\n",
      "[\"climbin'\"] ['climbing']\n",
      "[\"nothin'\"] ['nothing']\n",
      "['hearn'] ['heard']\n",
      "['ef'] ['if']\n",
      "['debbil'] ['devil']\n",
      "['wanter'] ['want', 'to']\n",
      "[\"savin'\"] ['saving']\n",
      "[\"yo'se'f\"] ['yourself']\n",
      "[\"nuffin'\"] ['nothing']\n",
      "[\"walkin'\"] ['walking']\n",
      "[\"showin'\"] ['showing']\n",
      "[\"gittin'\"] ['getting']\n",
      "['skinnenbones'] ['skin', 'and', 'bones']\n",
      "['debbil'] ['devil']\n",
      "['mought'] ['might']\n",
      "['debbil'] ['devil']\n",
      "[\"roun'\"] ['round']\n",
      "[\"sett'n\"] ['setting']\n",
      "[\"nothin'\"] ['nothing']\n",
      "[\"A'mighty\"] ['Almighty']\n",
      "['gwine'] ['going']\n",
      "[\"nofin'\"] ['nothing']\n",
      "[\"'em\"] ['them']\n"
     ]
    }
   ],
   "source": [
    "# consider all variations found in the corrected quotes\n",
    "V = variations.keys()\n",
    "\n",
    "contributions_news_c = {}  # estimated contributions based on 'correction' of words demonstrative of a each variation\n",
    "\n",
    "# get x_bar of the original collection\n",
    "surprisals = []\n",
    "for x in tuples_news:\n",
    "    surprisals += x[1][-1]\n",
    "x_bar =  np.array(surprisals).mean()\n",
    "\n",
    "# consider each variation in turn\n",
    "for v in [\"['NG']->['N']\"]: #tqdm(V):\n",
    "\n",
    "    I = [x[0] for x in corrected_quotes_LOC if x[2] == v]  # tuples_news indices corrected wrt., current variation\n",
    "    i2c = {x[0]:c for c, x in enumerate(corrected_quotes_LOC) if x[0] in I}\n",
    "\n",
    "    # surprisals for speaker, ignoring I ... we will add to these\n",
    "    surprisals = []\n",
    "    for i, x in enumerate(tuples_news):\n",
    "        if i not in I:\n",
    "            surprisals += x[1][-1]\n",
    "\n",
    "    # add surprisals for I\n",
    "    for i in I:\n",
    "\n",
    "        quote = tuples_news[i][1][0]\n",
    "        words = get_words(quote)\n",
    "\n",
    "        # get surprisals (by word) wrt., corrected quotations\n",
    "        quote_c = corrected_quotes_LOC[i2c[i]][3]\n",
    "        chain_c = np.array(chains_corrected_LOC[i2c[i]][1:])  # ignore standard beginning token given to all chains\n",
    "        words_c = get_words(quote_c)\n",
    "        tokens_c = tokenizer.tokenize(quote_c)\n",
    "        surprisals_c = get_surprisals(words_c, tokens_c, chain_c)\n",
    "\n",
    "        # sum the surprisals where corresponding words are unchanged between correction and original \n",
    "        matcher = difflib.SequenceMatcher(None, words, words_c)\n",
    "        opcodes = matcher.get_opcodes()\n",
    "        for tag, i1, i2, j1, j2 in opcodes:  # where x1\n",
    "            if tag == \"equal\":\n",
    "                surprisals += surprisals_c[j1:j2]\n",
    "            else:\n",
    "                if v == \"['NG']->['N']\":\n",
    "                    print(words[i1:i2], words_c[j1:j2])\n",
    "\n",
    "    # report\n",
    "    contributions_news_c[v] = 100*(x_bar - np.array(surprisals).mean())/x_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "675e4035-dba3-4bc2-8c41-75393ee3b0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.267566609622147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.15873526998720544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.7015255528268064"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = \"['NG']->['N']\"\n",
    "display(xbar_lit)\n",
    "display(contributions_news[v])\n",
    "display(contributions_news_c[v])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61024c2b-95b7-4d1e-9703-28ceee156c58",
   "metadata": {},
   "source": [
    "## get $C(\\bar{S}_{n-\\text{word},lit }, v) - C(\\bar{S}_{n-\\text{word},news}, v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b66cda0e-e78f-432d-ba7a-3a803bd493d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"['NG']->['N']\", 2.23, 17.3, 1.53, -0.7),\n",
      " ('dialect_words2',\n",
      "  0.4955245887615404,\n",
      "  3.993806118703225,\n",
      "  0.4955245887615404,\n",
      "  0),\n",
      " (\"['T']->[]\", 0.44716408583037237, 3.6040323781250168, 0.44716408583037237, 0),\n",
      " (\"['ER']->['AH']\",\n",
      "  0.42954214584137784,\n",
      "  3.4620038827735264,\n",
      "  0.42954214584137784,\n",
      "  0),\n",
      " (\"['AH']->[]\", 0.42877018058885635, 3.455782033002858, 0.42877018058885635, 0),\n",
      " (\"['R']->[]\", 0.3085463740836171, 2.4868077683060195, 0.3085463740836171, 0),\n",
      " ('dialect_words1',\n",
      "  0.2802483746226149,\n",
      "  2.258732863533135,\n",
      "  0.2802483746226149,\n",
      "  0),\n",
      " (\"['AE']->['AA', 'R']\",\n",
      "  0.26734129680233776,\n",
      "  2.15470499581009,\n",
      "  0.26734129680233776,\n",
      "  0),\n",
      " ('italicised',\n",
      "  0.26483787154652916,\n",
      "  2.1345280049379456,\n",
      "  0.26483787154652916,\n",
      "  0),\n",
      " (\"['TH']->['F']\",\n",
      "  0.24593656345737672,\n",
      "  1.9821881178566234,\n",
      "  0.24593656345737672,\n",
      "  0),\n",
      " (\"['TH']->['T']\",\n",
      "  0.23445056999205938,\n",
      "  1.8896138399669649,\n",
      "  0.23445056999205938,\n",
      "  0),\n",
      " (\"['T', 'ER']->[]\",\n",
      "  0.198993420783924,\n",
      "  1.6038379518053985,\n",
      "  0.198993420783924,\n",
      "  0),\n",
      " (\"['ER']->['AA', 'R']\",\n",
      "  0.1857073132181273,\n",
      "  1.4967551977030324,\n",
      "  0.1857073132181273,\n",
      "  0),\n",
      " (\"['V']->['B']\",\n",
      "  0.16798411140304742,\n",
      "  1.3539105569779666,\n",
      "  0.16798411140304742,\n",
      "  0),\n",
      " (\"['DH']->[]\", 0.16533378786396344, 1.332549602129305, 0.16533378786396344, 0)]\n"
     ]
    }
   ],
   "source": [
    "# biggest differences between ...\n",
    "xbar_lit = np.array(list(chain(*PG_df.loc[PG_df.loc[:,\"speaker\"].isin([\"Negro\",\"negro\"]), \"surprisals\"]))).mean()\n",
    "\n",
    "# get x_bar of the original collection\n",
    "surprisals = []\n",
    "for x in tuples_news:\n",
    "    surprisals += x[1][-1]\n",
    "xbar_news =  np.array(surprisals).mean()\n",
    "\n",
    "pp(\n",
    "    sorted(\n",
    "        [\n",
    "            (\n",
    "                # varatiation\n",
    "                v, \n",
    "                # difference in proportional contributions\n",
    "                round(contributions_lit_c[v] - contributions_news_c[v], 2), \n",
    "                # percentage reduction in difference in mean word LLM surprisal betweeen nword and normative reference speaker\n",
    "                round(100*(contributions_lit_c[v]*xbar_lit/100 - contributions_news_c[v]*xbar_news/100)/(xbar_lit - xbar_news), 1),\n",
    "                # proportional contribution of variation wrt., nword mean word LLM surprisal\n",
    "                round(contributions_lit_c[v], 2), \n",
    "                # proportional contribution of variation wrt., normative ref mean word LLM surprisal\n",
    "                round(contributions_news_c[v], 2)\n",
    "            ) \n",
    "            if v in contributions_news_c\n",
    "            else (\n",
    "                # varatiation\n",
    "                v, \n",
    "                # difference in proportional contributions\n",
    "                contributions_lit_c[v] - 0, \n",
    "                # reduction in difference in mean word LLM surprisal betweeen nword and normative reference speaker\n",
    "                100*(contributions_lit_c[v]*xbar_lit/100 - 0)/(xbar_lit - xbar_news),\n",
    "                # proportional contribution of variation wrt., nword mean word LLM surprisal\n",
    "                contributions_lit_c[v], \n",
    "                # proportional contribution of variation wrt., normative ref mean word LLM surprisal\n",
    "                0 \n",
    "            ) for v in contributions_lit.keys()\n",
    "        ], \n",
    "        key = lambda x: x[1], reverse=True\n",
    "    )[:15]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3fd09c39-0b3c-4f76-ae19-12b9c4f3ff13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"T'anks\",\n",
       " \"anyt'ing\",\n",
       " 'anyting',\n",
       " \"eberyt'ing\",\n",
       " 'eberyting',\n",
       " 'fortwid',\n",
       " 'nuttin',\n",
       " \"nuttin'\",\n",
       " \"pant'er\",\n",
       " \"pant'ers\",\n",
       " 'someting',\n",
       " \"t'ing\",\n",
       " \"t'ings\",\n",
       " \"t'ink\",\n",
       " \"t'inks\",\n",
       " \"t'rough\",\n",
       " 'tief',\n",
       " 'tink',\n",
       " 'tinks',\n",
       " 'troo'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variations[\"['TH']->['T']\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7e44e-2140-4b0b-bd9a-9482a0e78477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
