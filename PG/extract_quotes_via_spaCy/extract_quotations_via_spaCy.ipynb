{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U 'spacy[apple]'\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import typing\n",
    "from pprint import pprint as pp\n",
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61c0a5",
   "metadata": {},
   "source": [
    "load the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881dbcca",
   "metadata": {},
   "source": [
    "# target word: negro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f2043",
   "metadata": {},
   "source": [
    "## Identify all books in PG American Literature corpus that contain 'negro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be60b8",
   "metadata": {},
   "source": [
    "the American literate corpus, corresponds to \"en,PS,fiction\", which can be found in ~/surfdrive/Data/PG_en_PS_fiction_050204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925075d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_dir = pathlib.Path('~/surfdrive/Data/PG_en_PS_fiction_050204').expanduser().resolve()\n",
    "books_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac85fb0",
   "metadata": {},
   "source": [
    "first sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_books = []\n",
    "\n",
    "for fp in tqdm(books_dir.glob('*')):\n",
    "    \n",
    "    with open(fp, 'r', encoding='latin1') as f:\n",
    "        book_txt = f.read()\n",
    "        \n",
    "    if ' negro' in book_txt:\n",
    "        target_books.append(fp)\n",
    "        \n",
    "display(len(target_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf47b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(target_books[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce102fb3",
   "metadata": {},
   "source": [
    "## yield (file id, quote, manner, speaker) via spaCy dep parse and pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def between(x,t):\n",
    "    lower, upper = t\n",
    "    if x >= lower and x <= upper:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "print(between(3, (1,5)), between(7, (1,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf918fef-c6a1-46f7-b783-788fb23673c6",
   "metadata": {},
   "source": [
    "let's keep it simple ...\n",
    "\n",
    "    * replace quotations with e.g., 'oh' ... to remove parser complexity\n",
    "    * assume that each consecutive pair of quote marks, subtend a quotation\n",
    "    * where subtending quote marks share a VERB head, that VERB is assumed the to be the manner of speaking the quote\n",
    "    * where one of the above VERBs is modified by dobj or nsubj, then we assume the speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c61da6-22a4-4abd-b873-6dbf8a56c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_quotes(chunk):\n",
    "    return chunk.count('\"') + chunk.count('“') + chunk.count('”')\n",
    "\n",
    "def yield_type1(text:str, replacement='\"#\"', graph = False):\n",
    "\n",
    "    # chunk into quotes and text-in-between-quotes\n",
    "    chunks = [chunk for chunk in re.split(r'(\".*?\")|(“.*?”)', text) if chunk]\n",
    "    # print('chunks', chunks)\n",
    "\n",
    "    # get the quotes only\n",
    "    quotes = [mo.group(0) for mo in re.finditer(r'(\".*?\")|(“.*?”)', text)]\n",
    "    # print('quotes', quotes)\n",
    "\n",
    "    # get counts of quotes in each chunk\n",
    "    quote_counts = [count_quotes(chunk) for chunk in chunks]\n",
    "    # print('quote counts in chunks', quote_counts)\n",
    "\n",
    "    # ensure where a quotations mark occurs, it does so in pairs\n",
    "    if all([True if (count == 0 or count == 2) else False for count in quote_counts]):\n",
    "\n",
    "        # build a chunks version with the quotes replaced with replacement\n",
    "        if replacement == '':\n",
    "            chunks_ = chunks\n",
    "        else:\n",
    "            chunks_ = [chunk if quote_count == 0 else replacement for chunk, quote_count in zip(chunks, quote_counts)]\n",
    "\n",
    "        # build a text version with quotes replaced\n",
    "        text_ = \"\".join(chunks_)\n",
    "        # print('text_', text_)\n",
    "\n",
    "        # text as a list of tokens and their properties\n",
    "        doc = nlp(text_)\n",
    "        if graph:\n",
    "            displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "        \n",
    "        tokens = {token.i: {'text':token.text, 'modifiers':list([m.i for m in token.children]), 'pos':token.pos_, 'head':token.head.i, 'dep':token.dep_, 'idx':token.idx} for token in doc}\n",
    "        # print('tokens', tokens)\n",
    "        \n",
    "                # list of indices which are opening quotation marks\n",
    "        marks_i = [i for i, d in tokens.items() if d['text']=='\"' or d['text']=='“' or d['text']=='”']\n",
    "        # print('marks', marks_i)\n",
    "        opening_marks_i = [marks_i[x] for x in range(0,len(marks_i),2)]\n",
    "        # print('opening quotation marks', opening_marks_i)\n",
    "        \n",
    "        # list of indices which are verbs\n",
    "        verbs_i = [i for i, d in tokens.items() if d['pos']==\"VERB\"]\n",
    "        # print('verbs', verbs_i)\n",
    "\n",
    "        # build a list of (quote, manner, speaker)\n",
    "        extracts = []\n",
    "        for r, o in enumerate(opening_marks_i):\n",
    "            \n",
    "            o_matched = False\n",
    "            \n",
    "            # find a verb which is modified by the opening quotation ...\n",
    "            for v in verbs_i:\n",
    "                if o in tokens[v]['modifiers']:\n",
    "                    # for that verb, find a corresponding speaker, i.e., subj or dobj which modifies the verb\n",
    "                    for m in tokens[v]['modifiers']:\n",
    "                        if tokens[m]['dep']==\"dobj\" or tokens[m]['dep']==\"nsubj\":\n",
    "                            extracts.append((quotes[r], tokens[v]['text'], tokens[m]['text']))\n",
    "                            o_matched = True\n",
    "\n",
    "            if o_matched == False:\n",
    "                extracts.append((quotes[r], None, None))\n",
    "\n",
    "        # yield\n",
    "        for t in extracts:\n",
    "            yield t\n",
    "\n",
    "input_ = '\"today seems nice\", said Tom'\n",
    "list(yield_type1(input_))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc17f60-07cf-4bb5-84cc-f8b16a73c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-line dialogue ... \n",
    "input_ = \"\"\"\n",
    "He says:\n",
    "\n",
    "“What you doin’ with this gun?”\n",
    "\n",
    "I judged he didn’t know nothing about what he had been doing, so I says:\n",
    "\n",
    "“Somebody tried to get in, so I was laying for him.”\n",
    "\n",
    "“Why didn’t you roust me out?”\n",
    "\n",
    "“Well, I tried to, but I couldn’t; I couldn’t budge you.” \n",
    "\"\"\"\n",
    "list(yield_type1(input_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: doesn't pick up on conjuction wrt., multiple speakers ... this is fine ... we aren't targetting this\n",
    "input_ = '\"oh,\" said Tom and Dave'\n",
    "list(yield_type1(input_, graph=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4881b58",
   "metadata": {},
   "source": [
    "## test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    # direct dialogue\n",
    "    (\n",
    "        yield_type1, \n",
    "        '\"blah,\" said the Mr. Tom Jones to Dave', \n",
    "        [('\"blah,\"', \"said\", \"Jones\")]\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        '\"blah,\" Prof. James E. Jones said to Dave',\n",
    "        [('\"blah,\"', \"said\", \"Jones\")],\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        '\"blah,\" said Tom Liam Smith to Dave',\n",
    "        [('\"blah,\"', \"said\", \"Smith\")],\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        '\"blah,\" said J.F.K. about Dave',\n",
    "        [('\"blah,\"', \"said\",  \"J.F.K.\")],\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        '\"blah,\" said the nurse to Dave', \n",
    "        [('\"blah,\"', \"said\", \"nurse\")]\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        'Tom Smith shouts \"blah\", over the state of the Union',\n",
    "        [('\"blah\"', \"shouts\", \"Smith\")],\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        'Tom S. Smith said to Dave, \"blah\"',\n",
    "        [('\"blah\"', \"said\", \"Smith\")],\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        'the nurse said \"blah\" to Dave', \n",
    "        [('\"blah\"', \"said\", \"nurse\")],\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        'the nurse said excitedly, \"blah\"',\n",
    "        [('\"blah\"', \"said\", \"nurse\")],\n",
    "    ),\n",
    "    # split dialogue\n",
    "    (\n",
    "        yield_type1, \n",
    "        '\"blah,\" said the very able nurse about Tom, \"I don\\'t like him\"',\n",
    "        [('\"blah,\"', \"said\", \"nurse\"), ('\"I don\\'t like him\"', \"said\", \"nurse\")],\n",
    "    ),\n",
    "    (\n",
    "        yield_type1, \n",
    "        '\"if we are not quick\", she replied, \"we will be late\"',\n",
    "        [('\"if we are not quick\"', \"replied\", \"she\"), ('\"we will be late\"', \"replied\", \"she\")],\n",
    "    ),\n",
    "    # \n",
    "]\n",
    "\n",
    "# run test cases on import\n",
    "print(\"\\trun test cases\")\n",
    "for i, (f, input_, expected) in enumerate(test_cases, start=1):\n",
    "    \n",
    "    out = list(f(input_))\n",
    "   \n",
    "    for e in expected:\n",
    "        assert e in out, f\"test case {i}: expected {e}, found {out}\"\n",
    "        \n",
    "print(\"\\ttests successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371998e3",
   "metadata": {},
   "source": [
    "## get the quotatations\n",
    "\n",
    "A large disparity hints at an oversight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_fp = pathlib.Path(\"../dictionaries/english.txt\")\n",
    "with open(dictionary_fp, 'r') as f:\n",
    "    dictionary = set([line.strip('\\n') for line in f.readlines()])\n",
    "# display(str(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_paragraphs(fp: pathlib.Path, *, dictionary: set[str]) -> typing.Generator:\n",
    "    \"\"\"Return a generator of paragraph strings for book at fp.\n",
    "\n",
    "    Note: dictionary is used to help resolve hyphenatic split words due to formatting\n",
    "    Note: paragraphs assumed as separated by '\\n\\n'\n",
    "    Note: paragraphs cleaned up, removing \\n is a way sentitive to hyphens\n",
    "    \"\"\"\n",
    "\n",
    "    # open the doc\n",
    "    # As per https://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html,\n",
    "    # latin-1 encoding is an acceptable best approach if in doubt of encoding, and is close to the python3 permissible model.\n",
    "    with open(fp, \"r\", encoding=\"latin-1\") as f:\n",
    "        doc = f.read()\n",
    "\n",
    "    # ignore the extraneous PG text, take only the book\n",
    "    match = re.search(\n",
    "        r\"\\*\\*\\*\\s*START OF.+?\\*\\*\\*(.+)\\*\\*\\*\\s*END OF\",\n",
    "        doc,\n",
    "        flags=re.DOTALL,\n",
    "    )\n",
    "\n",
    "    if match:\n",
    "\n",
    "        # true book text\n",
    "        doc = match.groups()[0]\n",
    "\n",
    "        # split into presumed paragraphs\n",
    "        paragraphs = re.split(\"\\n\\n\\n*\", doc)\n",
    "\n",
    "        # remove empty paragraphs\n",
    "        paragraphs = [p for p in paragraphs if len(p) != 0]\n",
    "\n",
    "        # remove newlines (sensitively)\n",
    "        pattern_split = re.compile(r\"([a-zA-Z']+)-\\s*\\n\\s*([a-zA-Z']+)\")\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "\n",
    "            # remove newlines adjacent to hyphenated words\n",
    "            for x, y in set(re.findall(pattern_split, paragraph)):\n",
    "\n",
    "                try:\n",
    "                    if x + y in dictionary:\n",
    "                        paragraph = re.sub(rf\"{x}-\\s*\\n\\s*{y}\", f\"{x}{y}\", paragraph)\n",
    "                    else:\n",
    "                        paragraph = re.sub(rf\"{x}-\\s*\\n\\s*{y}\", f\"{x}-{y}\", paragraph)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # remove other newline cases\n",
    "            paragraph = re.sub(r\"\\s*\\n\\s*\", r\" \", paragraph)\n",
    "\n",
    "            # strip start and end whitespace\n",
    "            paragraph = paragraph.strip()\n",
    "\n",
    "            # re-add amended to paragraph container\n",
    "            paragraphs[i] = paragraph\n",
    "\n",
    "        # yield\n",
    "        for paragraph in paragraphs:\n",
    "            yield paragraph\n",
    "\n",
    "    else:\n",
    "\n",
    "        return\n",
    "        yield\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802e3b3",
   "metadata": {},
   "source": [
    "get list of (fp, quote_count, patterns[:-1] match count ) for all books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16811f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get paragraphs for book_id\n",
    "extracts = []\n",
    "\n",
    "for fp in tqdm(target_books):\n",
    "# for fp in tqdm([pathlib.Path('~/surfdrive/Data/PG_en_PS_fiction_050204/15603.txt').expanduser()]):  \n",
    "    \n",
    "    paragraphs = list(gen_paragraphs(fp, dictionary=dictionary))\n",
    "    \n",
    "    # count extracted quotations\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        \n",
    "        if '\"' in paragraph or '”' in paragraph:\n",
    "            for t in yield_type1(paragraph):\n",
    "                if len(t) > 0:\n",
    "                    extracts.append([str(fp.stem), i, t[0], t[1], t[2]])\n",
    "                    \n",
    "    with open(\"quotes.json\", \"w\") as f:\n",
    "        json.dump(extracts, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4282d3",
   "metadata": {},
   "source": [
    "## Quick overview of extracted quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfab1f",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('quotes_5Jul/quotes_5Jul.json', 'r') as f:\n",
    "    quotes = np.array(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc236b3",
   "metadata": {},
   "source": [
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do the quotes look like?\n",
    "quotes[110000:110010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d90bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many books in the quotations set?\n",
    "len(set([id_ for id_, _, _, _, _ in quotes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128cd2f-9507-42db-af42-0ad969e6d670",
   "metadata": {},
   "source": [
    "## Identify problematic spans (i.e., > 1024 tokens wrt., gpt-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075f1ad-01ef-4f4b-9161-90b3eb774023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b822f-a06e-4473-9924-75fa73af5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bfeba-44d4-4b20-8725-9863f7363aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_lengths = np.array([len(tokenizer.tokenize(quote.strip('\"“”'))) for id_, p, quote, manner, speaker in tqdm(quotes)])\n",
    "(spans_lengths>1024).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28038c4c-1aa8-4d6f-8dc9-4747001205b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = np.array([i for i,(id_, p, quote, manner, speaker) in enumerate(quotes)])[spans_lengths>1024]\n",
    "blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f5606-c144-463c-b20b-351721ed1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"quotes_blacklist.json\", 'w') as f:\n",
    "    json.dump([x.item() for x in blacklist], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836a042-da0c-4e5c-a3e6-1c7356a5029f",
   "metadata": {},
   "source": [
    "# Examine the Quotes (removed of blacklisted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98617035-4125-4b45-9da8-9df91c53814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# who are the named speakers?\n",
    "speaker_counts = Counter()\n",
    "for i, (id_, p, quote, manner, speaker) in tqdm(enumerate(quotes)):\n",
    "    if i not in blacklist:\n",
    "        speaker_counts[speaker] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c8e78-863f-4272-ab2c-f56ef3ff4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker in ['negro', 'Negro', 'nigger', 'Nigger', 'chinaman', 'Chinaman', 'Oriental', 'oriental', 'coloured', 'Coloured', 'mulatto', 'quadroon', 'black', 'Black', 'jew', 'Jew', 'yid', 'Yid']:\n",
    "    print(speaker, speaker_counts[speaker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98675fdb-5aaf-4ec9-845e-35a7a5147ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many books containing quotations attributed to the speaker, 'negro'?\n",
    "ids_with_negro_descriptors = set([id_ for id_, p, quote, manner, speaker in quotes if (speaker == 'negro' or speaker =='Negro')])\n",
    "print(len(ids_with_negro_descriptors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
